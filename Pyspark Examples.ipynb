{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3300e416-be2f-4e85-bfda-db80a2f139fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. How to import PySpark and check the version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1d4222-6694-4b5d-a605-d312139a01a2",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">3.1.2\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">3.1.2\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Pyspark Concepts\").getOrCreate()\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09b14a7-2990-43c8-b0c9-7bb204a993b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. How to convert the index of a PySpark DataFrame into a column?\n",
    "- Hint: The PySpark DataFrame doesn’t have an explicit concept of an index like Pandas DataFrame. However, if you have a DataFrame and you’d like to add a new column that is basically a row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84f1e9b-0be4-4176-bf7d-13564569043d",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Value</th></tr></thead><tbody><tr><td>Alice</td><td>1</td></tr><tr><td>Bob</td><td>2</td></tr><tr><td>Charlie</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         1
        ],
        [
         "Bob",
         2
        ],
        [
         "Charlie",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Value",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf56973-cc5d-486c-9358-4a3540d3c0a1",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Value</th><th>Index</th></tr></thead><tbody><tr><td>Alice</td><td>1</td><td>0</td></tr><tr><td>Bob</td><td>2</td><td>1</td></tr><tr><td>Charlie</td><td>3</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         1,
         0
        ],
        [
         "Bob",
         2,
         1
        ],
        [
         "Charlie",
         3,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Index",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number,lit,monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "w= Window().orderBy(lit('Name'))\n",
    "#or\n",
    "w= Window().orderBy(monotonically_increasing_id())\n",
    "df_index = df.withColumn('Index',row_number().over(w) -1)\n",
    "\n",
    "df_index.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38017991-a754-40f3-92d6-b253667b087b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. How to combine many lists to form a PySpark DataFrame?\n",
    "- Create a PySpark DataFrame from list1 and list2\n",
    "\n",
    "- Hint: For Creating DataFrame from multiple lists, first create an RDD (Resilient Distributed Dataset) from those lists and then convert the RDD to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b90692-f110-4d26-b683-acca5ce9d479",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#combine many lists\n",
    "# Define your lists\n",
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab4fce1-0978-40c9-ae2e-2395283b37fc",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>column1</th><th>column2</th></tr></thead><tbody><tr><td>a</td><td>1</td></tr><tr><td>b</td><td>2</td></tr><tr><td>c</td><td>3</td></tr><tr><td>d</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "a",
         1
        ],
        [
         "b",
         2
        ],
        [
         "c",
         3
        ],
        [
         "d",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "column1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "column2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "rdd_ser = spark.sparkContext.parallelize(list(zip(list1,list2)))\n",
    "df = rdd_ser.toDF([\"column1\",\"column2\"])\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ba968f-87d9-48b0-ac48-938e8ce79809",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;d&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[&#39;d&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sort a list\n",
    "list1.sort(reverse=True)\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85fde8a-82be-4de2-afe7-545df1dc2e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. How to get the items of list A not present in list B?\n",
    "- Get the items of list_A not present in list_B in PySpark, you can use the subtract operation on RDDs (Resilient Distributed Datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95375bff-1858-4b81-b509-49ab94330558",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f170721-9fbd-4664-8a1a-87340cc17b2f",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[1, 2, 3]\n",
       "[1, 2, 3]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[1, 2, 3]\n[1, 2, 3]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the items of list A not present in list B\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "set_C = set(list_A) - set(list_B)\n",
    "list_C = [*set_C]\n",
    "print(list_C)\n",
    "\n",
    "rdd_A = sc.parallelize(list_A)\n",
    "rdd_B = sc.parallelize(list_B)\n",
    "result = rdd_A.subtract(rdd_B).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7f1f09-b6c7-4c54-b842-5bc58ea37c95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#5. How to get the items not common to both list A and list B?\n",
    "- Get all items of list_A and list_B not common to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29daf01-507a-4add-8443-457444ed43d8",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd7e635-fdb5-4ab6-aa26-089fc6f4eb84",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[1, 2, 3, 6, 7, 8]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[1, 2, 3, 6, 7, 8]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###items not common to both list\n",
    "list_C = list(set(list_A) ^ set(list_B))\n",
    "print(list_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffe8c79-c60c-491d-9b32-66ca68aa62c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?\n",
    "- Compute the minimum, 25th percentile, median, 75th, and maximum of column Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811c5951-82d3-4c1b-b0c8-47afc8931e58",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+---+\n",
       "Name|Age|\n",
       "+----+---+\n",
       "   A| 10|\n",
       "   B| 20|\n",
       "   C| 30|\n",
       "   D| 40|\n",
       "   E| 50|\n",
       "   F| 15|\n",
       "   G| 28|\n",
       "   H| 54|\n",
       "   I| 41|\n",
       "   J| 86|\n",
       "+----+---+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+---+\n|Name|Age|\n+----+---+\n|   A| 10|\n|   B| 20|\n|   C| 30|\n|   D| 40|\n|   E| 50|\n|   F| 15|\n|   G| 28|\n|   H| 54|\n|   I| 41|\n|   J| 86|\n+----+---+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6b1e2c-2a29-413e-9567-634ccdd92668",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Minimum</th><th>Percentile_25</th><th>Average</th><th>Median</th><th>Percentile_75</th><th>Maximum</th></tr></thead><tbody><tr><td>10</td><td>20</td><td>37.4</td><td>30</td><td>50</td><td>86</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         20,
         37.4,
         30,
         50,
         86
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Minimum",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Percentile_25",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Average",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Median",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Percentile_75",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Maximum",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###minimum, 25th percentile, median, 75th, and max\n",
    "from pyspark.sql.functions import min,max,avg,percentile_approx\n",
    "df_min = df.select(min('Age').alias('Minimum'),\n",
    "                   percentile_approx('Age',0.25).alias('Percentile_25'),\n",
    "                   avg('Age').alias('Average'),\n",
    "                   percentile_approx('Age',0.50).alias('Median'),\n",
    "                   percentile_approx('Age',0.75).alias('Percentile_75'),\n",
    "                   max('Age').alias('Maximum'))\n",
    "\n",
    "df_min.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e4fc72-530c-48c4-86bf-92e50ec7e2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#7. How to get frequency counts of unique items of a column?\n",
    "- Calculte the frequency counts of each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2619fcd4-bdcd-4a51-8279-3f7afcc9959b",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+---------+\n",
       "name|      job|\n",
       "+----+---------+\n",
       "John| Engineer|\n",
       "John| Engineer|\n",
       "Mary|Scientist|\n",
       " Bob| Engineer|\n",
       " Bob| Engineer|\n",
       " Bob|Scientist|\n",
       " Sam|   Doctor|\n",
       "+----+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+---------+\n|name|      job|\n+----+---------+\n|John| Engineer|\n|John| Engineer|\n|Mary|Scientist|\n| Bob| Engineer|\n| Bob| Engineer|\n| Bob|Scientist|\n| Sam|   Doctor|\n+----+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd2f85b-19da-458d-8895-4a35c546356e",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job</th><th>count</th></tr></thead><tbody><tr><td>Engineer</td><td>4</td></tr><tr><td>Scientist</td><td>2</td></tr><tr><td>Doctor</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Engineer",
         4
        ],
        [
         "Scientist",
         2
        ],
        [
         "Doctor",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###frequency counts\n",
    "df_freq = df.groupBy('job').count()\n",
    "\n",
    "df_freq.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dcc86df-89df-4c29-aa9e-ad04f90e5d60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e27d03-ba75-4439-a321-5ff7f6af041c",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+---------+\n",
       "name|      job|\n",
       "+----+---------+\n",
       "John| Engineer|\n",
       "John| Engineer|\n",
       "Mary|Scientist|\n",
       " Bob| Engineer|\n",
       " Bob| Engineer|\n",
       " Bob|Scientist|\n",
       " Sam|   Doctor|\n",
       "+----+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+---------+\n|name|      job|\n+----+---------+\n|John| Engineer|\n|John| Engineer|\n|Mary|Scientist|\n| Bob| Engineer|\n| Bob| Engineer|\n| Bob|Scientist|\n| Sam|   Doctor|\n+----+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a9fb6f-3153-497f-af53-a7a97acc76c1",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job</th><th>name</th><th>new</th></tr></thead><tbody><tr><td>Engineer</td><td>John</td><td>Engineer</td></tr><tr><td>Engineer</td><td>John</td><td>Engineer</td></tr><tr><td>Scientist</td><td>Mary</td><td>Scientist</td></tr><tr><td>Engineer</td><td>Bob</td><td>Engineer</td></tr><tr><td>Engineer</td><td>Bob</td><td>Engineer</td></tr><tr><td>Scientist</td><td>Bob</td><td>Scientist</td></tr><tr><td>Doctor</td><td>Sam</td><td>Other</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Engineer",
         "John",
         "Engineer"
        ],
        [
         "Engineer",
         "John",
         "Engineer"
        ],
        [
         "Scientist",
         "Mary",
         "Scientist"
        ],
        [
         "Engineer",
         "Bob",
         "Engineer"
        ],
        [
         "Engineer",
         "Bob",
         "Engineer"
        ],
        [
         "Scientist",
         "Bob",
         "Scientist"
        ],
        [
         "Doctor",
         "Sam",
         "Other"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "new",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###2 most frequent values as it is and replace everything else as ‘Other’\n",
    "from pyspark.sql.functions import desc,col,when,lit\n",
    "\n",
    "df_tmp = df.groupBy('job').count().alias('count')\n",
    "df_tmp = df_tmp.orderBy(col('count').desc()).limit(2)\n",
    "df_tmp = df.join(df_tmp,on='job',how='left')\n",
    "df_tmp = df_tmp.withColumn('new',when(col('count')> 0,col('job')).otherwise(lit('Other'))).drop('count')\n",
    "\n",
    "\n",
    "df_tmp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed86afe4-0f7b-4fc4-b40b-ad7e6f6f8cc0",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+---------+\n",
       "name|      job|\n",
       "+----+---------+\n",
       "John| Engineer|\n",
       "John| Engineer|\n",
       "Mary|Scientist|\n",
       " Bob| Engineer|\n",
       " Bob| Engineer|\n",
       " Bob|Scientist|\n",
       " Sam|    Other|\n",
       "+----+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+---------+\n|name|      job|\n+----+---------+\n|John| Engineer|\n|John| Engineer|\n|Mary|Scientist|\n| Bob| Engineer|\n| Bob| Engineer|\n| Bob|Scientist|\n| Sam|    Other|\n+----+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Get the top 2 most frequent jobs\n",
    "top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Replace all but the top 2 most frequent jobs with 'Other'\n",
    "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf221b6-18e4-4a2e-a87d-7a6f194c0dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#9. How to Drop rows with NA values specific to a particular column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07dfc0c-a936-45a0-ac07-f5ab2446b8f0",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+-----+----+\n",
       "Name|Value|  id|\n",
       "+----+-----+----+\n",
       "   A|    1|null|\n",
       "   B| null| 123|\n",
       "   B|    3| 456|\n",
       "   D| null|null|\n",
       "+----+-----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+-----+----+\n|Name|Value|  id|\n+----+-----+----+\n|   A|    1|null|\n|   B| null| 123|\n|   B|    3| 456|\n|   D| null|null|\n+----+-----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242a69e7-9266-45b0-a640-67f000403ea2",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Value</th><th>id</th></tr></thead><tbody><tr><td>A</td><td>1</td><td>null</td></tr><tr><td>B</td><td>3</td><td>456</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         1,
         null
        ],
        [
         "B",
         3,
         "456"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Drop rows with NA values\n",
    "df_result = df.filter(col('Value').isNotNull())\n",
    "#OR\n",
    "df_result = df.dropna(subset=['Value'])\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0bc1a0-7d2f-4d5d-b54b-73168591e1f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3badc2-c731-4c45-bad2-cb10c1daa12d",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+----+\n",
       "col1|col2|col3|\n",
       "+----+----+----+\n",
       "   1|   2|   3|\n",
       "   4|   5|   6|\n",
       "+----+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   1|   2|   3|\n|   4|   5|   6|\n+----+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4faf5c3-8126-4e13-a562-dc4b1b81c406",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>new_col1</th><th>new_col2</th><th>new_col3</th></tr></thead><tbody><tr><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2,
         3
        ],
        [
         4,
         5,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "new_col1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "new_col2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "new_col3",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "###rename columns of a PySpark DataFrame using two lists\n",
    "df_renamed = df.select(*[col(old).alias(new) for old,new in zip(old_names,new_names)])\n",
    "df_renamed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a523797-3697-4868-8f50-f2653bcccb32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#11. How to bin a numeric list to 10 groups of equal size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef14999-4154-4921-83e1-42b6b6e17faa",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------------+\n",
       "             values|\n",
       "+-------------------+\n",
       "  0.619189370225301|\n",
       " 0.5096018842446481|\n",
       " 0.8325259388871524|\n",
       "0.26322809041172357|\n",
       " 0.6702867696264135|\n",
       "+-------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------------------+\n|             values|\n+-------------------+\n|  0.619189370225301|\n| 0.5096018842446481|\n| 0.8325259388871524|\n|0.26322809041172357|\n| 0.6702867696264135|\n+-------------------+\nonly showing top 5 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a214d6-5547-420e-908b-f5d0343b1fd2",
     "showTitle": true,
     "title": "Solution XXX"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>binned_values</th><th>count</th></tr></thead><tbody><tr><td>8.0</td><td>10</td></tr><tr><td>0.0</td><td>9</td></tr><tr><td>7.0</td><td>10</td></tr><tr><td>4.0</td><td>10</td></tr><tr><td>3.0</td><td>10</td></tr><tr><td>2.0</td><td>10</td></tr><tr><td>6.0</td><td>10</td></tr><tr><td>5.0</td><td>10</td></tr><tr><td>9.0</td><td>11</td></tr><tr><td>1.0</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8.0,
         10
        ],
        [
         0.0,
         9
        ],
        [
         7.0,
         10
        ],
        [
         4.0,
         10
        ],
        [
         3.0,
         10
        ],
        [
         2.0,
         10
        ],
        [
         6.0,
         10
        ],
        [
         5.0,
         10
        ],
        [
         9.0,
         11
        ],
        [
         1.0,
         10
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\":{\"ord\":true,\"vals\":[\"0.0031434569487398534, 0.1353439163856851\",\"0.1353439163856851, 0.2422600602366628\",\"0.2422600602366628, 0.3205165688068011\",\"0.3205165688068011, 0.4509378549789149\",\"0.4509378549789149, 0.5874807147974267\",\"0.5874807147974267, 0.6703304054828096\",\"0.6703304054828096, 0.7463770360606422\",\"0.7463770360606422, 0.8184715436384177\",\"0.8184715436384177, 0.8630976835817983\",\"0.8630976835817983, 0.9991441647585968\"],\"type\":\"nominal\"}}",
         "name": "binned_values",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###numeric list to 10 groups of equal size\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "num_buckets = 10\n",
    "quantiles = df.stat.approxQuantile(\"values\", [i/num_buckets for i in range(num_buckets+1)], 0.00)\n",
    "\n",
    "#splits = [-float('inf'), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, float('inf')]\n",
    "\n",
    "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"binned_values\")\n",
    "\n",
    "df_binned = bucketizer.transform(df)\n",
    "df_result = df_binned.groupBy('binned_values').count()\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58482867-d20f-40c1-aa8b-8108c4c41efc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#12. How to create contigency table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a8a4f9-4a26-4462-a329-86428acbec1c",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+---------+\n",
       "category1|category2|\n",
       "+---------+---------+\n",
       "        A|        X|\n",
       "        A|        Y|\n",
       "        A|        X|\n",
       "        B|        Y|\n",
       "        B|        X|\n",
       "        C|        X|\n",
       "        C|        X|\n",
       "        C|        Y|\n",
       "+---------+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+---------+\n|category1|category2|\n+---------+---------+\n|        A|        X|\n|        A|        Y|\n|        A|        X|\n|        B|        Y|\n|        B|        X|\n|        C|        X|\n|        C|        X|\n|        C|        Y|\n+---------+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33954df1-da46-417e-b6a4-94d27c64cce7",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category1</th><th>count</th></tr></thead><tbody><tr><td>null</td><td>8</td></tr><tr><td>A</td><td>3</td></tr><tr><td>B</td><td>2</td></tr><tr><td>C</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         8
        ],
        [
         "A",
         3
        ],
        [
         "B",
         2
        ],
        [
         "C",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category1_category2</th><th>X</th><th>Y</th></tr></thead><tbody><tr><td>A</td><td>2</td><td>1</td></tr><tr><td>C</td><td>2</td><td>1</td></tr><tr><td>B</td><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         2,
         1
        ],
        [
         "C",
         2,
         1
        ],
        [
         "B",
         1,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category1_category2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "X",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Y",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### contigency table\n",
    "#Frequency:\n",
    "df_result = df.cube('category1').count()\n",
    "df_result.display()\n",
    "#Contigency:\n",
    "df_result = df.crosstab('category1','category2')\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ecef10d-0753-430b-90ef-4f6edc660bec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#13. How to find the numbers that are multiples of 3 from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021c6fa6-51a4-4bb4-a161-ce6b77f6c434",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+------+\n",
       " id|random|\n",
       "+---+------+\n",
       "  0|     7|\n",
       "  1|     9|\n",
       "  2|     8|\n",
       "  3|     8|\n",
       "  4|     3|\n",
       "  5|     1|\n",
       "  6|     7|\n",
       "  7|     4|\n",
       "  8|     5|\n",
       "  9|     1|\n",
       "+---+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+------+\n| id|random|\n+---+------+\n|  0|     7|\n|  1|     9|\n|  2|     8|\n|  3|     8|\n|  4|     3|\n|  5|     1|\n|  6|     7|\n|  7|     4|\n|  8|     5|\n|  9|     1|\n+---+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efc6a85-86d9-4ec0-b8f8-6a1fc7770994",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>random</th><th>mult_3</th></tr></thead><tbody><tr><td>0</td><td>7</td><td>no</td></tr><tr><td>1</td><td>9</td><td>yes</td></tr><tr><td>2</td><td>8</td><td>no</td></tr><tr><td>3</td><td>8</td><td>no</td></tr><tr><td>4</td><td>3</td><td>yes</td></tr><tr><td>5</td><td>1</td><td>no</td></tr><tr><td>6</td><td>7</td><td>no</td></tr><tr><td>7</td><td>4</td><td>no</td></tr><tr><td>8</td><td>5</td><td>no</td></tr><tr><td>9</td><td>1</td><td>no</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         7,
         "no"
        ],
        [
         1,
         9,
         "yes"
        ],
        [
         2,
         8,
         "no"
        ],
        [
         3,
         8,
         "no"
        ],
        [
         4,
         3,
         "yes"
        ],
        [
         5,
         1,
         "no"
        ],
        [
         6,
         7,
         "no"
        ],
        [
         7,
         4,
         "no"
        ],
        [
         8,
         5,
         "no"
        ],
        [
         9,
         1,
         "no"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "random",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "mult_3",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### find the numbers that are multiples of 3\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_result = df.withColumn('mult_3', when((col('random') % 3 == 0), 'yes').otherwise('no'))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12ce602-b21b-43e1-9884-86f39ed416bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#14. How to extract items at given positions from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e8ac92-5663-4f7d-a799-222ec6e80af3",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+------+\n",
       " id|random|\n",
       "+---+------+\n",
       "  0|     7|\n",
       "  1|     9|\n",
       "  2|     8|\n",
       "  3|     8|\n",
       "  4|     3|\n",
       "  5|     1|\n",
       "  6|     7|\n",
       "  7|     4|\n",
       "  8|     5|\n",
       "  9|     1|\n",
       "+---+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+------+\n| id|random|\n+---+------+\n|  0|     7|\n|  1|     9|\n|  2|     8|\n|  3|     8|\n|  4|     3|\n|  5|     1|\n|  6|     7|\n|  7|     4|\n|  8|     5|\n|  9|     1|\n+---+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "pos = [0, 4, 8, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1852393b-0e51-405f-9a37-3ed43cb3a42a",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>random</th><th>Row</th></tr></thead><tbody><tr><td>0</td><td>7</td><td>0</td></tr><tr><td>4</td><td>3</td><td>4</td></tr><tr><td>5</td><td>1</td><td>5</td></tr><tr><td>8</td><td>5</td><td>8</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         7,
         0
        ],
        [
         4,
         3,
         4
        ],
        [
         5,
         1,
         5
        ],
        [
         8,
         5,
         8
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "random",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Row",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### extract items at given positions from a column\n",
    "pos = [0, 4, 8, 5]\n",
    "from pyspark.sql.functions import row_number,monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df_tmp = df.withColumn('Row',row_number().over(w) -1)\n",
    "df_result = df_tmp.filter(col('Row').isin(pos))\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e630645f-154d-4a58-858e-543d0f2e8368",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#15. How to stack two DataFrames vertically ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0291ca85-54a2-4ec6-8a40-88491075369d",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+-----+-----+\n",
       "  Name|Col_1|Col_2|\n",
       "+------+-----+-----+\n",
       " apple|    3|    5|\n",
       "banana|    1|   10|\n",
       "orange|    2|    8|\n",
       "+------+-----+-----+\n",
       "\n",
       "+------+-----+-----+\n",
       "  Name|Col_1|Col_3|\n",
       "+------+-----+-----+\n",
       " apple|    3|    5|\n",
       "banana|    1|   15|\n",
       " grape|    4|    6|\n",
       "+------+-----+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+-----+-----+\n|  Name|Col_1|Col_2|\n+------+-----+-----+\n| apple|    3|    5|\n|banana|    1|   10|\n|orange|    2|    8|\n+------+-----+-----+\n\n+------+-----+-----+\n|  Name|Col_1|Col_3|\n+------+-----+-----+\n| apple|    3|    5|\n|banana|    1|   15|\n| grape|    4|    6|\n+------+-----+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1747986-1a84-43e6-9701-fa6b4c512ee4",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Col_1</th><th>Col_2</th><th>Col_3</th></tr></thead><tbody><tr><td>apple</td><td>3</td><td>5</td><td>null</td></tr><tr><td>banana</td><td>1</td><td>10</td><td>null</td></tr><tr><td>orange</td><td>2</td><td>8</td><td>null</td></tr><tr><td>apple</td><td>3</td><td>null</td><td>5</td></tr><tr><td>banana</td><td>1</td><td>null</td><td>15</td></tr><tr><td>grape</td><td>4</td><td>null</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "apple",
         3,
         5,
         null
        ],
        [
         "banana",
         1,
         10,
         null
        ],
        [
         "orange",
         2,
         8,
         null
        ],
        [
         "apple",
         3,
         null,
         5
        ],
        [
         "banana",
         1,
         null,
         15
        ],
        [
         "grape",
         4,
         null,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Col_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Col_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Col_3",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Col_1</th><th>Col_2</th></tr></thead><tbody><tr><td>apple</td><td>3</td><td>5</td></tr><tr><td>banana</td><td>1</td><td>10</td></tr><tr><td>orange</td><td>2</td><td>8</td></tr><tr><td>apple</td><td>3</td><td>5</td></tr><tr><td>banana</td><td>1</td><td>15</td></tr><tr><td>grape</td><td>4</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "apple",
         3,
         5
        ],
        [
         "banana",
         1,
         10
        ],
        [
         "orange",
         2,
         8
        ],
        [
         "apple",
         3,
         5
        ],
        [
         "banana",
         1,
         15
        ],
        [
         "grape",
         4,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Col_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Col_2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### stack two DataFrames vertically\n",
    "df_result = df_A.unionByName(df_B,allowMissingColumns=True)\n",
    "df_result.display()\n",
    "\n",
    "df_result = df_A.union(df_B)\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ac6654-f2c5-4e8c-9500-ab334dfb12e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#16. How to compute the mean squared error on a truth and predicted columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d24b630-00a6-416a-bae1-0d51b31cbc29",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+---------+\n",
       "actual|predicted|\n",
       "+------+---------+\n",
       "     1|        1|\n",
       "     2|        4|\n",
       "     3|        9|\n",
       "     4|       16|\n",
       "     5|       25|\n",
       "+------+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+---------+\n|actual|predicted|\n+------+---------+\n|     1|        1|\n|     2|        4|\n|     3|        9|\n|     4|       16|\n|     5|       25|\n+------+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume you have a DataFrame df with two columns \"actual\" and \"predicted\"\n",
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e56d9a-6c40-43f2-abb5-083e4930120b",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_square_err</th></tr></thead><tbody><tr><td>116.8</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         116.8
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "mean_square_err",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### compute the mean squared error on a truth and predicted columns\n",
    "from pyspark.sql.functions import col, pow, avg\n",
    "\n",
    "df_tmp = df.withColumn('square',pow(col('actual') -col('predicted'),2))\n",
    "df_result = df_tmp.select(avg(col('square')).alias('mean_square_err'))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f08764-3414-4376-aae8-c447fdd23041",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#17. How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6716dd88-1c65-45f7-b11b-984dfc91bd4c",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+\n",
       " name|\n",
       "+-----+\n",
       " john|\n",
       "alice|\n",
       "  bob|\n",
       "+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+\n| name|\n+-----+\n| john|\n|alice|\n|  bob|\n+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a660043-f367-4b4e-bc78-fd900adcd38c",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>new</th></tr></thead><tbody><tr><td>john</td><td>John</td></tr><tr><td>alice</td><td>Alice</td></tr><tr><td>bob</td><td>Bob</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "john",
         "John"
        ],
        [
         "alice",
         "Alice"
        ],
        [
         "bob",
         "Bob"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "new",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### first character of each element in a series to uppercase\n",
    "from pyspark.sql.functions import initcap, col,udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df_result = df.withColumn(\"Name_new\",initcap(col(\"name\")))\n",
    "#df_result.display()\n",
    "\n",
    "def cap_string(_string):\n",
    "    return \" \".join(word.capitalize() for word in _string.split())\n",
    "cap_string_udf=udf(cap_string,StringType())\n",
    "\n",
    "df_result=df.withColumn('new',cap_string_udf('name'))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40af98f5-6047-4599-ab36-b059777bfedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#18. How to compute summary statistics for all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da657e5d-349d-47ab-b98f-d61cb5f6c471",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+---+------+\n",
       "   name|age|salary|\n",
       "+-------+---+------+\n",
       "  James| 34| 55000|\n",
       "Michael| 30| 70000|\n",
       " Robert| 37| 60000|\n",
       "  Maria| 29| 80000|\n",
       "    Jen| 32| 65000|\n",
       "+-------+---+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+---+------+\n|   name|age|salary|\n+-------+---+------+\n|  James| 34| 55000|\n|Michael| 30| 70000|\n| Robert| 37| 60000|\n|  Maria| 29| 80000|\n|    Jen| 32| 65000|\n+-------+---+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b0a83c-dfa4-4d0f-b3d2-f96a072fec38",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>name</th><th>age</th><th>salary</th></tr></thead><tbody><tr><td>mean</td><td>null</td><td>32.4</td><td>66000.0</td></tr><tr><td>max</td><td>Robert</td><td>37</td><td>80000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "mean",
         null,
         "32.4",
         "66000.0"
        ],
        [
         "max",
         "Robert",
         "37",
         "80000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###compute summary statistics for all columns\n",
    "from pyspark.sql.functions import *\n",
    "df_result = df.summary()\n",
    "df_result = df.summary('mean','max')\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449bc8c2-6c09-4399-acf3-947f1b5ddc5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#19. How to calculate the number of characters in each word in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a659c22-53d8-4e58-aacd-36a8aa2cd30f",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+\n",
       " name|\n",
       "+-----+\n",
       " john|\n",
       "alice|\n",
       "  bob|\n",
       "+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+\n| name|\n+-----+\n| john|\n|alice|\n|  bob|\n+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2d241a-532e-477b-9ac6-77fbad126087",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>n_char</th></tr></thead><tbody><tr><td>john</td><td>4</td></tr><tr><td>alice</td><td>5</td></tr><tr><td>bob</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "john",
         4
        ],
        [
         "alice",
         5
        ],
        [
         "bob",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_char",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###calculate the number of characters\n",
    "#from pyspark.sql.functions import char_length,length\n",
    "df_result = df.withColumn(\"n_char\",length(\"name\"))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0552f24-1a15-44f8-bb37-e2871061eb3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#20 How to compute difference of differences between consecutive numbers of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6d5f05-6517-4d62-aed6-91dfdf06b07f",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+---+------+\n",
       "   name|age|salary|\n",
       "+-------+---+------+\n",
       "  James| 34| 55000|\n",
       "Michael| 30| 70000|\n",
       " Robert| 37| 60000|\n",
       "  Maria| 29| 80000|\n",
       "    Jen| 32| 65000|\n",
       "+-------+---+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+---+------+\n|   name|age|salary|\n+-------+---+------+\n|  James| 34| 55000|\n|Michael| 30| 70000|\n| Robert| 37| 60000|\n|  Maria| 29| 80000|\n|    Jen| 32| 65000|\n+-------+---+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0378d89e-004b-4df9-a62c-9e8383f747bc",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>salary</th><th>id</th><th>prev_salary</th><th>diff</th></tr></thead><tbody><tr><td>James</td><td>34</td><td>55000</td><td>8589934592</td><td>null</td><td>0</td></tr><tr><td>Michael</td><td>30</td><td>70000</td><td>25769803776</td><td>55000</td><td>15000</td></tr><tr><td>Robert</td><td>37</td><td>60000</td><td>34359738368</td><td>70000</td><td>-10000</td></tr><tr><td>Maria</td><td>29</td><td>80000</td><td>51539607552</td><td>60000</td><td>20000</td></tr><tr><td>Jen</td><td>32</td><td>65000</td><td>60129542144</td><td>80000</td><td>-15000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "James",
         34,
         55000,
         8589934592,
         null,
         0
        ],
        [
         "Michael",
         30,
         70000,
         25769803776,
         55000,
         15000
        ],
        [
         "Robert",
         37,
         60000,
         34359738368,
         70000,
         -10000
        ],
        [
         "Maria",
         29,
         80000,
         51539607552,
         60000,
         20000
        ],
        [
         "Jen",
         32,
         65000,
         60129542144,
         80000,
         -15000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "prev_salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "diff",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### compute difference of differences between consecutive numbers of a column\n",
    "from pyspark.sql.functions import monotonically_increasing_id, lag,col,when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df_tmp = df.withColumn(\"id\",monotonically_increasing_id())\n",
    "w = Window.orderBy(\"id\")\n",
    "df_tmp = df_tmp.withColumn(\"prev_salary\", lag(\"salary\").over(w))\n",
    "df_tmp = df_tmp.withColumn(\"tmp_diff\",col(\"salary\") - col(\"prev_salary\"))\n",
    "df_tmp = df_tmp.withColumn(\"diff\",when(col(\"tmp_diff\").isNull(),0).otherwise(col(\"tmp_diff\")))\n",
    "df_result = df_tmp.drop(\"tmp_diff\")\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59de8d87-45d8-4f27-b511-7210c1ca780a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#21. How to get the day of month, week number, day of year and day of week from a date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9e76d7-a94a-45fc-9735-ab5a7c150217",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----------+\n",
       "date_str_1| date_str_2|\n",
       "+----------+-----------+\n",
       "2023-05-18|01 Jan 2010|\n",
       "2023-12-31|01 Jan 2010|\n",
       "+----------+-----------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----------+\n|date_str_1| date_str_2|\n+----------+-----------+\n|2023-05-18|01 Jan 2010|\n|2023-12-31|01 Jan 2010|\n+----------+-----------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example data\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae403994-8f10-4b41-a2cf-401c398f54c1",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date_str_1</th><th>date_str_2</th><th>date_1</th><th>date_2</th><th>day_of_month</th><th>week_number</th><th>day_of_year</th><th>day_of_week</th></tr></thead><tbody><tr><td>2023-05-18</td><td>01 Jan 2010</td><td>2023-05-18</td><td>2010-01-01</td><td>18</td><td>20</td><td>138</td><td>5</td></tr><tr><td>2023-12-31</td><td>01 Jan 2010</td><td>2023-12-31</td><td>2010-01-01</td><td>31</td><td>52</td><td>365</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-05-18",
         "01 Jan 2010",
         "2023-05-18",
         "2010-01-01",
         18,
         20,
         138,
         5
        ],
        [
         "2023-12-31",
         "01 Jan 2010",
         "2023-12-31",
         "2010-01-01",
         31,
         52,
         365,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date_str_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date_str_2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date_1",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "date_2",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "day_of_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "week_number",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_of_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### day of month, week number, day of year and day of week from a date strings\n",
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "\n",
    "# Convert date string to date format\n",
    "df = df.withColumn(\"date_1\", to_date(df.date_str_1, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date(df.date_str_2, 'dd MMM yyyy'))\n",
    "\n",
    "df_result = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1410ce79-0872-4a5e-919d-fe9bd04cff2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#22. How to convert year-month string to dates corresponding to the 4th day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25535e4c-281a-40c1-ac0a-9d2ca67c739a",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+\n",
       "MonthYear|\n",
       "+---------+\n",
       " Jan 2010|\n",
       " Feb 2011|\n",
       " Mar 2012|\n",
       "+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+\n|MonthYear|\n+---------+\n| Jan 2010|\n| Feb 2011|\n| Mar 2012|\n+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example dataframe\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609d157e-4d91-426a-938b-222590441e38",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>MonthYear</th><th>Date</th></tr></thead><tbody><tr><td>Jan 2010</td><td>2010-01-04</td></tr><tr><td>Feb 2011</td><td>2011-02-04</td></tr><tr><td>Mar 2012</td><td>2012-03-04</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jan 2010",
         "2010-01-04"
        ],
        [
         "Feb 2011",
         "2011-02-04"
        ],
        [
         "Mar 2012",
         "2012-03-04"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "MonthYear",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### convert year-month string to dates corresponding to the 4th day of the month\n",
    "from pyspark.sql.functions import concat,col,lit\n",
    "df_result = df.withColumn(\"Date\", to_date(col(\"MonthYear\"),\"MMM yyyy\") +3)\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f98759-cb33-4b19-98f0-e408740767ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#23 How to filter words that contain atleast 2 vowels from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067e6b2b-6147-48f9-8637-9d0b9523d798",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+\n",
       "  Word|\n",
       "+------+\n",
       " Apple|\n",
       "Orange|\n",
       "  Plan|\n",
       "Python|\n",
       " Money|\n",
       "+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+\n|  Word|\n+------+\n| Apple|\n|Orange|\n|  Plan|\n|Python|\n| Money|\n+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example dataframe\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64b512f-3be7-466e-918a-2e1cd242954c",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-3406428054828030&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### filter words that contain atleast 2 vowels from a series</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> expr<span class=\"ansi-blue-fg\">,</span> regexp_count\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> df_result <span class=\"ansi-blue-fg\">=</span> df<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;regexp_count(upper(Word),&#39;[AEIOU]&#39;)&#34;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "\n",
       "<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;regexp_count&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3406428054828030&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### filter words that contain atleast 2 vowels from a series</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> expr<span class=\"ansi-blue-fg\">,</span> regexp_count\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> df_result <span class=\"ansi-blue-fg\">=</span> df<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;regexp_count(upper(Word),&#39;[AEIOU]&#39;)&#34;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n\n<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;regexp_count&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;regexp_count&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### filter words that contain atleast 2 vowels from a series\n",
    "from pyspark.sql.functions import expr, regexp_count\n",
    "from pyspark.sql.functions import *\n",
    "df_result = df.filter(expr(\"regexp_count(upper(Word),'[AEIOU]')\") >=2)\n",
    "\n",
    "# Define a custom function to check if a word has at least 2 vowels\n",
    "def has_at_least_two_vowels(word):\n",
    "    return expr(\"regexp_count(upper({}), '[AEIOU]') >= 2\".format(word))\n",
    "\n",
    "# Filter words with at least 2 vowels\n",
    "df_result = df.filter(has_at_least_two_vowels(\"Word\"))\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363ef4e0-eece-4342-afda-5e2e06440585",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#24. How to filter valid emails from a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1f47df-b2f9-4346-bf71-1aa4ad2edace",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------------+\n",
       "value                     |\n",
       "+--------------------------+\n",
       "buying books at amazom.com|\n",
       "rameses@egypt.com         |\n",
       "matt@t.co                 |\n",
       "narendra@modi.com         |\n",
       "+--------------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------------------------+\n|value                     |\n+--------------------------+\n|buying books at amazom.com|\n|rameses@egypt.com         |\n|matt@t.co                 |\n|narendra@modi.com         |\n+--------------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a list\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298ff71d-a5ab-4d4c-8347-14eba60c02d2",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>rameses@egypt.com</td></tr><tr><td>narendra@modi.com</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "rameses@egypt.com"
        ],
        [
         "narendra@modi.com"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>rameses@egypt.com</td></tr><tr><td>matt@t.co</td></tr><tr><td>narendra@modi.com</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "rameses@egypt.com"
        ],
        [
         "matt@t.co"
        ],
        [
         "narendra@modi.com"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### filter valid emails from a list\n",
    "df_result = df.filter(col(\"value\").rlike(\".+@.+.com\"))\n",
    "df_result.display()\n",
    "\n",
    "pattern = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "\n",
    "df_result = df.filter(col(\"value\").rlike(pattern))\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54525074-6f27-44c5-bf1a-5e7fc61f15d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#25. How to Pivot PySpark DataFrame?\n",
    "- Convert region categories to Columns and sum the revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ddcd03-8ab1-4394-9464-c8d9ec337860",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+-------+------+-------+\n",
       "year|quarter|region|revenue|\n",
       "+----+-------+------+-------+\n",
       "2021|      1|    US|   5000|\n",
       "2021|      1|    EU|   4000|\n",
       "2021|      2|    US|   5500|\n",
       "2021|      2|    EU|   4500|\n",
       "2021|      3|    US|   6000|\n",
       "2021|      3|    EU|   5000|\n",
       "2021|      4|    US|   7000|\n",
       "2021|      4|    EU|   6000|\n",
       "+----+-------+------+-------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+-------+------+-------+\n|year|quarter|region|revenue|\n+----+-------+------+-------+\n|2021|      1|    US|   5000|\n|2021|      1|    EU|   4000|\n|2021|      2|    US|   5500|\n|2021|      2|    EU|   4500|\n|2021|      3|    US|   6000|\n|2021|      3|    EU|   5000|\n|2021|      4|    US|   7000|\n|2021|      4|    EU|   6000|\n+----+-------+------+-------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671eb5c7-a2d1-4405-a300-76a10a13c79d",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>year</th><th>quarter</th><th>EU</th><th>US</th></tr></thead><tbody><tr><td>2021</td><td>2</td><td>4500</td><td>5500</td></tr><tr><td>2021</td><td>1</td><td>4000</td><td>5000</td></tr><tr><td>2021</td><td>3</td><td>5000</td><td>6000</td></tr><tr><td>2021</td><td>4</td><td>6000</td><td>7000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2021,
         2,
         4500,
         5500
        ],
        [
         2021,
         1,
         4000,
         5000
        ],
        [
         2021,
         3,
         5000,
         6000
        ],
        [
         2021,
         4,
         6000,
         7000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "EU",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "US",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Pivot PySpark DataFrame\n",
    "df_result = df.groupBy(\"year\",\"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e58d093-5d8a-4ca7-b8f0-db2e294cd495",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#26. How to get the mean of a variable grouped by another variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94b653c-c268-4b27-ba35-10415ecf7a82",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+----------+-----+\n",
       "OrderID|   Product|Price|\n",
       "+-------+----------+-----+\n",
       "   1001|    Laptop| 1000|\n",
       "   1002|     Mouse|   50|\n",
       "   1003|    Laptop| 1200|\n",
       "   1004|     Mouse|   30|\n",
       "   1005|Smartphone|  700|\n",
       "+-------+----------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+----------+-----+\n|OrderID|   Product|Price|\n+-------+----------+-----+\n|   1001|    Laptop| 1000|\n|   1002|     Mouse|   50|\n|   1003|    Laptop| 1200|\n|   1004|     Mouse|   30|\n|   1005|Smartphone|  700|\n+-------+----------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d493db8-fbae-4c0e-81bd-1578ce9fb621",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Product</th><th>avg(Price)</th></tr></thead><tbody><tr><td>Laptop</td><td>1100.0</td></tr><tr><td>Mouse</td><td>40.0</td></tr><tr><td>Smartphone</td><td>700.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Laptop",
         1100.0
        ],
        [
         "Mouse",
         40.0
        ],
        [
         "Smartphone",
         700.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg(Price)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Product</th><th>Avg_Price</th></tr></thead><tbody><tr><td>Laptop</td><td>1100.0</td></tr><tr><td>Mouse</td><td>40.0</td></tr><tr><td>Smartphone</td><td>700.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Laptop",
         1100.0
        ],
        [
         "Mouse",
         40.0
        ],
        [
         "Smartphone",
         700.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Avg_Price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### mean of a variable grouped by another variable\n",
    "from pyspark.sql.functions import mean\n",
    "df_result = df.groupBy(\"Product\").mean(\"Price\").alias(\"Avg_Price\")\n",
    "df_result.display()\n",
    "df_result = df.groupBy(\"Product\").agg(mean(\"Price\").alias(\"Avg_Price\"))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9256ca63-f5ac-4740-b211-fa10b597e713",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#27. How to compute the euclidean distance between two columns?\n",
    "-Compute the euclidean distance between series (points) p and q, without using a packaged formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479aff09-dc3f-4ce1-a0bc-e6b342a2d13f",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+-------+\n",
       "series1|series2|\n",
       "+-------+-------+\n",
       "      1|     10|\n",
       "      2|      9|\n",
       "      3|      8|\n",
       "      4|      7|\n",
       "      5|      6|\n",
       "      6|      5|\n",
       "      7|      4|\n",
       "      8|      3|\n",
       "      9|      2|\n",
       "     10|      1|\n",
       "+-------+-------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+-------+\n|series1|series2|\n+-------+-------+\n|      1|     10|\n|      2|      9|\n|      3|      8|\n|      4|      7|\n|      5|      6|\n|      6|      5|\n|      7|      4|\n|      8|      3|\n|      9|      2|\n|     10|      1|\n+-------+-------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define your series\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3be05a4-0e66-40d0-a151-41cf301cc748",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>series1</th><th>series2</th><th>vectors</th><th>squared_diff</th></tr></thead><tbody><tr><td>1</td><td>10</td><td>Map(vectorType -> dense, length -> 2, values -> List(1.0, 10.0))</td><td>81.0</td></tr><tr><td>2</td><td>9</td><td>Map(vectorType -> dense, length -> 2, values -> List(2.0, 9.0))</td><td>49.0</td></tr><tr><td>3</td><td>8</td><td>Map(vectorType -> dense, length -> 2, values -> List(3.0, 8.0))</td><td>25.0</td></tr><tr><td>4</td><td>7</td><td>Map(vectorType -> dense, length -> 2, values -> List(4.0, 7.0))</td><td>9.0</td></tr><tr><td>5</td><td>6</td><td>Map(vectorType -> dense, length -> 2, values -> List(5.0, 6.0))</td><td>1.0</td></tr><tr><td>6</td><td>5</td><td>Map(vectorType -> dense, length -> 2, values -> List(6.0, 5.0))</td><td>1.0</td></tr><tr><td>7</td><td>4</td><td>Map(vectorType -> dense, length -> 2, values -> List(7.0, 4.0))</td><td>9.0</td></tr><tr><td>8</td><td>3</td><td>Map(vectorType -> dense, length -> 2, values -> List(8.0, 3.0))</td><td>25.0</td></tr><tr><td>9</td><td>2</td><td>Map(vectorType -> dense, length -> 2, values -> List(9.0, 2.0))</td><td>49.0</td></tr><tr><td>10</td><td>1</td><td>Map(vectorType -> dense, length -> 2, values -> List(10.0, 1.0))</td><td>81.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         10,
         {
          "length": 2,
          "values": [
           1.0,
           10.0
          ],
          "vectorType": "dense"
         },
         81.0
        ],
        [
         2,
         9,
         {
          "length": 2,
          "values": [
           2.0,
           9.0
          ],
          "vectorType": "dense"
         },
         49.0
        ],
        [
         3,
         8,
         {
          "length": 2,
          "values": [
           3.0,
           8.0
          ],
          "vectorType": "dense"
         },
         25.0
        ],
        [
         4,
         7,
         {
          "length": 2,
          "values": [
           4.0,
           7.0
          ],
          "vectorType": "dense"
         },
         9.0
        ],
        [
         5,
         6,
         {
          "length": 2,
          "values": [
           5.0,
           6.0
          ],
          "vectorType": "dense"
         },
         1.0
        ],
        [
         6,
         5,
         {
          "length": 2,
          "values": [
           6.0,
           5.0
          ],
          "vectorType": "dense"
         },
         1.0
        ],
        [
         7,
         4,
         {
          "length": 2,
          "values": [
           7.0,
           4.0
          ],
          "vectorType": "dense"
         },
         9.0
        ],
        [
         8,
         3,
         {
          "length": 2,
          "values": [
           8.0,
           3.0
          ],
          "vectorType": "dense"
         },
         25.0
        ],
        [
         9,
         2,
         {
          "length": 2,
          "values": [
           9.0,
           2.0
          ],
          "vectorType": "dense"
         },
         49.0
        ],
        [
         10,
         1,
         {
          "length": 2,
          "values": [
           10.0,
           1.0
          ],
          "vectorType": "dense"
         },
         81.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "series1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "series2",
         "type": "\"long\""
        },
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"series1\"},{\"idx\":1,\"name\":\"series2\"}]},\"num_attrs\":2}}",
         "name": "vectors",
         "type": "{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}"
        },
        {
         "metadata": "{}",
         "name": "squared_diff",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>euclidean_distance</th></tr></thead><tbody><tr><td>18.16590212458495</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         18.16590212458495
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "euclidean_distance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### uclidean distance between two columns\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Convert series to vectors\n",
    "vecAssembler = VectorAssembler(inputCols=[\"series1\", \"series2\"], outputCol=\"vectors\")\n",
    "df_tmp = vecAssembler.transform(df)\n",
    "\n",
    "\n",
    "# Calculate squared differences\n",
    "df_tmp = df_tmp.withColumn(\"squared_diff\", expr(\"POW(series1 - series2, 2)\"))\n",
    "df_tmp.display()\n",
    "\n",
    "# Sum squared differences and take square root\n",
    "df_result = df_tmp.agg(expr(\"SQRT(SUM(squared_diff))\").alias(\"euclidean_distance\"))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3306da54-1f8e-4064-b9ca-99d2ff99ca4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#28. How to replace missing spaces in a string with the least frequent character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de17b2f-a573-4f3d-85ba-b14aa34600d1",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----------------+\n",
       "           string|\n",
       "+-----------------+\n",
       "dbc deb abed gade|\n",
       "+-----------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----------------+\n|           string|\n+-----------------+\n|dbc deb abed gade|\n+-----------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sample DataFrame\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37441b8f-81ef-41bd-8e1c-5f8c251d4a2d",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>string</th><th>tmp1</th></tr></thead><tbody><tr><td>dbc deb abed gade</td><td>dbcdebabedgade</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbc deb abed gade",
         "dbcdebabedgade"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "string",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tmp1",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### replace missing spaces in a string with the least frequent character\n",
    "from pyspark.sql.functions import regexp_replace,col,explode,split,count\n",
    "\n",
    "df_tmp = df.withColumn(\"tmp1\",regexp_replace(col(\"string\"),'\\\\s+',''))\n",
    "df_tmp = df_tmp.withColumn(\"tmp2\",explode(split(col(\"tmp1\"),'')))\n",
    "df_tmp = df_tmp.groupBy(\"tmp2\").agg(count(\"tmp2\").alias(\"cnt\"))\n",
    "\n",
    "char_select = df_tmp.orderBy(\"cnt\",\"tmp2\").collect()[0][0]\n",
    "print(char_select)\n",
    "\n",
    "df_result = df.withColumn(\"tmp1\",regexp_replace(col(\"string\"),'\\\\s+',char_select))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9136bac6-d73f-4813-8948-4862dbd7e97d",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----------------+-----------------+\n",
       "           string|  modified_string|\n",
       "+-----------------+-----------------+\n",
       "dbc deb abed gade|dbcddebdabeddgade|\n",
       "+-----------------+-----------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----------------+-----------------+\n|           string|  modified_string|\n+-----------------+-----------------+\n|dbc deb abed gade|dbcddebdabeddgade|\n+-----------------+-----------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "def least_freq_char_replace_spaces(s):\n",
    "    counter = Counter(s.replace(\" \", \"\"))\n",
    "    least_freq_char = statistics.mode(list(counter.elements()))\n",
    "    return s.replace(' ', least_freq_char)\n",
    "\n",
    "udf_least_freq_char_replace_spaces = udf(least_freq_char_replace_spaces, StringType())\n",
    "\n",
    "df = spark.createDataFrame([('dbc deb abed gade',)], [\"string\"])\n",
    "df.withColumn('modified_string', udf_least_freq_char_replace_spaces(df['string'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960fae3f-e81b-4da5-aadf-2970e7849d49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#29. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e393093-09af-4a42-911b-7a992b024826",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-366566501270890&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays)</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sequence<span class=\"ansi-blue-fg\">,</span>to_date<span class=\"ansi-blue-fg\">,</span>dayofweek<span class=\"ansi-blue-fg\">,</span>monotonically_increasing_id<span class=\"ansi-blue-fg\">,</span>randn<span class=\"ansi-blue-fg\">,</span>cast\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> IntegerType\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql <span class=\"ansi-green-fg\">import</span> Window\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> start_date <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#39;2000-01-01&#39;</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;cast&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-366566501270890&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sequence<span class=\"ansi-blue-fg\">,</span>to_date<span class=\"ansi-blue-fg\">,</span>dayofweek<span class=\"ansi-blue-fg\">,</span>monotonically_increasing_id<span class=\"ansi-blue-fg\">,</span>randn<span class=\"ansi-blue-fg\">,</span>cast\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> IntegerType\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql <span class=\"ansi-green-fg\">import</span> Window\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> start_date <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#39;2000-01-01&#39;</span>\n\n<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;cast&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;cast&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays)\n",
    "from pyspark.sql.functions import sequence,to_date,dayofweek,monotonically_increasing_id,randn,cast\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import Window\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "df = spark.sql(f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date\")\n",
    "df_tmp = df.withColumn(\"Dates\",explode('date')).drop('date')\n",
    "df_tmp = df_tmp.withColumn('DayWeek',dayofweek(\"Dates\"))\n",
    "df_tmp = df_tmp.filter(df_tmp[\"DayWeek\"]==7)\n",
    "df_tmp = df_tmp.withColumn(\"R\",monotonically_increasing_id() +1)\n",
    "df_tmp = df_tmp.filter(df_tmp[\"R\"]<=10)\n",
    "\n",
    "random_column = (randn(42) * 10 + 1).cast(IntegerType())\n",
    "\n",
    "df_result = df_tmp.withColumn(\"Random\",random_column).drop('DayWeek','R')\n",
    "\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4cce13-96ad-41da-b158-2a6af8d01dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#30. How to get the nrows, ncolumns, datatype of a dataframe?\n",
    "-Get the number of rows, columns, datatype and summary statistics of each column of the Churn_Modelling dataset. Also get the numpy array and list equivalent of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448a5d49-43bc-4aea-80ef-abe0b360930e",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-366566501270896&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> spark<span class=\"ansi-blue-fg\">.</span>sparkContext<span class=\"ansi-blue-fg\">.</span>addFile<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>SparkFiles<span class=\"ansi-blue-fg\">.</span>get<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Churn_Modelling.csv&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> <span class=\"ansi-red-fg\">#df = spark.read.csv(&#34;C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling.csv&#34;, header=True, inferSchema=True)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    762</span>             path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>path<span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    763</span>         <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> list<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 764</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    765</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    766</span>             <span class=\"ansi-green-fg\">def</span> func<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: dbfs:/local_disk0/spark-719d73cc-2fd8-4de5-83a6-9bf822f9cef8/userFiles-27d14bd7-c017-4361-b8c7-7cedbcbaca9c/Churn_Modelling.csv</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-366566501270896&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> spark<span class=\"ansi-blue-fg\">.</span>sparkContext<span class=\"ansi-blue-fg\">.</span>addFile<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>SparkFiles<span class=\"ansi-blue-fg\">.</span>get<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Churn_Modelling.csv&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> <span class=\"ansi-red-fg\">#df = spark.read.csv(&#34;C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling.csv&#34;, header=True, inferSchema=True)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    762</span>             path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>path<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    763</span>         <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> list<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 764</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    765</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    766</span>             <span class=\"ansi-green-fg\">def</span> func<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: dbfs:/local_disk0/spark-719d73cc-2fd8-4de5-83a6-9bf822f9cef8/userFiles-27d14bd7-c017-4361-b8c7-7cedbcbaca9c/Churn_Modelling.csv</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: dbfs:/local_disk0/spark-719d73cc-2fd8-4de5-83a6-9bf822f9cef8/userFiles-27d14bd7-c017-4361-b8c7-7cedbcbaca9c/Churn_Modelling.csv",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "#df = spark.read.csv(\"C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1a5017-dd3e-42a5-9b72-27c20a8a7696",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Number of Rows:  1\n",
       "Number of Columns:  1\n",
       "Data types:  [(&#39;string&#39;, &#39;string&#39;)]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Number of Rows:  1\nNumber of Columns:  1\nData types:  [(&#39;string&#39;, &#39;string&#39;)]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### the nrows, ncolumns, datatype of a dataframe\n",
    "# For number of rows\n",
    "nrows = df.count()\n",
    "print(\"Number of Rows: \", nrows)\n",
    "\n",
    "# For number of columns\n",
    "ncols = len(df.columns)\n",
    "print(\"Number of Columns: \", ncols)\n",
    "\n",
    "# For data types of each column\n",
    "datatypes = df.dtypes\n",
    "print(\"Data types: \", datatypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff67a30-1fc2-4ca7-93f3-94bb62d68877",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#31. How to rename a specific columns in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c18a63-766b-4878-af29-5cc5272daf18",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+---+---+\n",
       " name|age|qty|\n",
       "+-----+---+---+\n",
       "Alice|  1| 30|\n",
       "  Bob|  2| 35|\n",
       "+-----+---+---+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+---+---+\n| name|age|qty|\n+-----+---+---+\n|Alice|  1| 30|\n|  Bob|  2| 35|\n+-----+---+---+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3a9422-a6c7-466d-bd79-992c5f779fd8",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>user_age</th><th>qty</th></tr></thead><tbody><tr><td>Alice</td><td>1</td><td>30</td></tr><tr><td>Bob</td><td>2</td><td>35</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         1,
         30
        ],
        [
         "Bob",
         2,
         35
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "qty",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### rename a specific columns\n",
    "# Rename lists for specific columns\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "\n",
    "for old,new in zip(old_names,new_names):\n",
    "    df_result = df.withColumnRenamed(old,new)\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95610dc3-fe7e-414e-bbd8-8727e844ed6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#32. How to check if a dataframe has any missing values and count of missing values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a146fff-0ac9-492f-9da5-34c5c977ef83",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+-----+----+\n",
       "Name|Value|  id|\n",
       "+----+-----+----+\n",
       "   A|    1|null|\n",
       "   B| null| 123|\n",
       "   B|    3| 456|\n",
       "   D| null|null|\n",
       "+----+-----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+-----+----+\n|Name|Value|  id|\n+----+-----+----+\n|   A|    1|null|\n|   B| null| 123|\n|   B|    3| 456|\n|   D| null|null|\n+----+-----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4b3045-14c1-49a3-a2e6-854fec76f5b0",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Value</th><th>id</th></tr></thead><tbody><tr><td>0</td><td>2</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         2,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### dataframe has any missing values and count of missing values\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Calculate the count of missing values for each column\n",
    "df_result = df.select([count(when(isnan(c) | col(c).isNull() | (col(c) == \"\"), c)).alias(c) for c in df.columns])\n",
    "df_result.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4da31db-4418-480d-96b5-360bf3808184",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#33 How to replace missing values of multiple numeric columns with the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387d6105-0631-47b2-bb17-a64cb13ab2b6",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+----+\n",
       "Name|var1|var2|\n",
       "+----+----+----+\n",
       "   A|   1|null|\n",
       "   B|null| 123|\n",
       "   B|   3| 456|\n",
       "   D|   6|null|\n",
       "+----+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+----+\n|Name|var1|var2|\n+----+----+----+\n|   A|   1|null|\n|   B|null| 123|\n|   B|   3| 456|\n|   D|   6|null|\n+----+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab99a80-6a93-4e04-9896-fdcf0287878f",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>var1</th><th>var2</th></tr></thead><tbody><tr><td>A</td><td>1.0</td><td>289.5</td></tr><tr><td>B</td><td>3.3333333333333335</td><td>123.0</td></tr><tr><td>B</td><td>3.0</td><td>456.0</td></tr><tr><td>D</td><td>6.0</td><td>289.5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         1.0,
         289.5
        ],
        [
         "B",
         3.3333333333333335,
         123.0
        ],
        [
         "B",
         3.0,
         456.0
        ],
        [
         "D",
         6.0,
         289.5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "var1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "var2",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### replace missing values of multiple numeric columns\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_tmp = df.withColumn('var1',when(col('var1').isNull(),df.groupBy().mean('var1').first()[0]).otherwise(col('var1')))\\\n",
    "    .withColumn('var2',when(col('var2').isNull(),df.groupBy().mean('var2').first()[0]).otherwise(col('var2')))\n",
    "df_tmp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9290f291-8976-4588-98ba-5da5c6b6efc9",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>var1</th><th>var2</th></tr></thead><tbody><tr><td>A</td><td>1</td><td>289</td></tr><tr><td>B</td><td>3</td><td>123</td></tr><tr><td>B</td><td>3</td><td>456</td></tr><tr><td>D</td><td>6</td><td>289</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         1,
         289
        ],
        [
         "B",
         3,
         123
        ],
        [
         "B",
         3,
         456
        ],
        [
         "D",
         6,
         289
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "var1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "var2",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### OR ML\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "column_names = [\"var1\", \"var2\"]\n",
    "\n",
    "# Initialize the Imputer\n",
    "imputer = Imputer(inputCols= column_names, outputCols= column_names, strategy=\"mean\")\n",
    "\n",
    "# Fit the Imputer\n",
    "model = imputer.fit(df)\n",
    "\n",
    "#Transform the dataset\n",
    "df_result = model.transform(df)\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5520da99-c88b-44ad-896f-f4211c9c9f49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#34. How to change the order of columns of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee33157-b6ae-4e8a-8915-8592ef0b952f",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+---------+---+\n",
       "First_Name|Last_Name|Age|\n",
       "+----------+---------+---+\n",
       "      John|      Doe| 30|\n",
       "      Jane|      Doe| 25|\n",
       "     Alice|    Smith| 22|\n",
       "+----------+---------+---+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+---------+---+\n|First_Name|Last_Name|Age|\n+----------+---------+---+\n|      John|      Doe| 30|\n|      Jane|      Doe| 25|\n|     Alice|    Smith| 22|\n+----------+---------+---+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "\n",
    "# Create DataFrame from the data\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64ffa89-9919-4c2a-825a-376df59f1281",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Age</th><th>First_Name</th><th>Last_Name</th></tr></thead><tbody><tr><td>30</td><td>John</td><td>Doe</td></tr><tr><td>25</td><td>Jane</td><td>Doe</td></tr><tr><td>22</td><td>Alice</td><td>Smith</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         30,
         "John",
         "Doe"
        ],
        [
         25,
         "Jane",
         "Doe"
        ],
        [
         22,
         "Alice",
         "Smith"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "First_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Last_Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### change the order of columns\n",
    "col_seq = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
    "df_result = df.select([*col_seq])\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51913fd2-c69d-4fec-bdcc-8c1296d115b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#35. How to format or suppress scientific notations in a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5319ed-9ef9-4562-bc44-92b1b9cfe809",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+-----------+\n",
       " id|your_column|\n",
       "+---+-----------+\n",
       "  1|    1.23E-7|\n",
       "  2|  2.3456E-5|\n",
       "  3| 3.45678E-4|\n",
       "+---+-----------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+-----------+\n| id|your_column|\n+---+-----------+\n|  1|    1.23E-7|\n|  2|  2.3456E-5|\n|  3| 3.45678E-4|\n+---+-----------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have a DataFrame df and the column you want to format is 'your_column'\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156580d2-ab13-4b41-a5ac-7610c5f9737b",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>your_column</th></tr></thead><tbody><tr><td>1</td><td>0.0000001230</td></tr><tr><td>2</td><td>0.0000234560</td></tr><tr><td>3</td><td>0.0003456780</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "0.0000001230"
        ],
        [
         2,
         "0.0000234560"
        ],
        [
         3,
         "0.0003456780"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "your_column",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### format or suppress scientific notations\n",
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "# Determine the number of decimal places you want\n",
    "decimal_places = 10\n",
    "\n",
    "df_result = df.withColumn(\"your_column\", format_number(\"your_column\", decimal_places))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721d37b7-d102-43b0-9d1b-530e79d69de4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#36. How to format all the values in a dataframe as percentages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacd16df-951f-4a55-85b9-da9b84d77aa8",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+---------+\n",
       "numbers_1|numbers_2|\n",
       "+---------+---------+\n",
       "      0.1|     0.08|\n",
       "      0.2|     0.06|\n",
       "     0.33|     0.02|\n",
       "+---------+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+---------+\n|numbers_1|numbers_2|\n+---------+---------+\n|      0.1|     0.08|\n|      0.2|     0.06|\n|     0.33|     0.02|\n+---------+---------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8134cb-e3bf-469e-8c8f-e2264a0fa2b8",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4363358011361189&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-green-fg\">return</span> datafr\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> \n",
       "<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span>df_result <span class=\"ansi-blue-fg\">=</span> format_columns<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">,</span>df<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> df_result<span class=\"ansi-blue-fg\">.</span>display<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">&lt;command-4363358011361189&gt;</span> in <span class=\"ansi-cyan-fg\">format_columns</span><span class=\"ansi-blue-fg\">(column_names, datafr)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">def</span> format_columns<span class=\"ansi-blue-fg\">(</span>column_names<span class=\"ansi-blue-fg\">,</span>datafr<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">-&gt;</span> DataFrame<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">for</span> colName <span class=\"ansi-green-fg\">in</span> col_names<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>         datafr <span class=\"ansi-blue-fg\">=</span> datafr<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span>concat<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">100</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span>DecimalType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">18</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>lit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;%&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-green-fg\">return</span> datafr\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;col_names&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4363358011361189&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-green-fg\">return</span> datafr\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> \n<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span>df_result <span class=\"ansi-blue-fg\">=</span> format_columns<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">,</span>df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> df_result<span class=\"ansi-blue-fg\">.</span>display<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-4363358011361189&gt;</span> in <span class=\"ansi-cyan-fg\">format_columns</span><span class=\"ansi-blue-fg\">(column_names, datafr)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">def</span> format_columns<span class=\"ansi-blue-fg\">(</span>column_names<span class=\"ansi-blue-fg\">,</span>datafr<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">-&gt;</span> DataFrame<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">for</span> colName <span class=\"ansi-green-fg\">in</span> col_names<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>         datafr <span class=\"ansi-blue-fg\">=</span> datafr<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span>concat<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">100</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span>DecimalType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">18</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>lit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;%&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-green-fg\">return</span> datafr\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;col_names&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;col_names&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### all the values in a dataframe as percentages\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def format_columns(column_names,datafr)-> DataFrame:\n",
    "    for colName in col_names:\n",
    "        datafr = datafr.withColumn(colName,concat((col(colName) * 100).cast(DecimalType(18,2)),lit('%')))\n",
    "    return datafr\n",
    "\n",
    "df_result = format_columns(df.columns,df)\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c56b27-b842-47c6-9b14-61488e7c60b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#37. How to filter every nth row in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c64fc8-bf6a-4aa8-bf57-8887723f6e04",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+------+\n",
       "   Name|Number|\n",
       "+-------+------+\n",
       "  Alice|     1|\n",
       "    Bob|     2|\n",
       "Charlie|     3|\n",
       "   Dave|     4|\n",
       "    Eve|     5|\n",
       "  Frank|     6|\n",
       "  Grace|     7|\n",
       " Hannah|     8|\n",
       "   Igor|     9|\n",
       "   Jack|    10|\n",
       "+-------+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+------+\n|   Name|Number|\n+-------+------+\n|  Alice|     1|\n|    Bob|     2|\n|Charlie|     3|\n|   Dave|     4|\n|    Eve|     5|\n|  Frank|     6|\n|  Grace|     7|\n| Hannah|     8|\n|   Igor|     9|\n|   Jack|    10|\n+-------+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3be919-c8ed-4219-9156-34e2688a1b8e",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### filter every nth row in a dataframe\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "def _filter_nth(_position)-> DataFrame:\n",
    "    w = Window().orderBy(monotonically_increasing_id())\n",
    "    df_tmp = df.withColumn('nth',row_number().over(w))\n",
    "    df_tmp = df_tmp.filter((col('nth') % lit(_position))==0)\n",
    "    return df_tmp\n",
    "\n",
    "df_result = _filter_nth(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e458172-7175-4ec8-991c-fcc55c075db6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#38 How to get the row number of the nth largest value in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326780c9-12dd-4013-80ab-ec54e52f1900",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+-------+\n",
       " id|column1|\n",
       "+---+-------+\n",
       "  1|      5|\n",
       "  2|      8|\n",
       "  3|     12|\n",
       "  4|      1|\n",
       "  5|     15|\n",
       "  6|      7|\n",
       "+---+-------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+-------+\n| id|column1|\n+---+-------+\n|  1|      5|\n|  2|      8|\n|  3|     12|\n|  4|      1|\n|  5|     15|\n|  6|      7|\n+---+-------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "Row(id=1, column1=5),\n",
    "Row(id=2, column1=8),\n",
    "Row(id=3, column1=12),\n",
    "Row(id=4, column1=1),\n",
    "Row(id=5, column1=15),\n",
    "Row(id=6, column1=7),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad20254-6fbd-4416-8238-15465ae3b19e",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Row number: 3\n",
       "Column value: 8\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Row number: 3\nColumn value: 8\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number\n",
    "\n",
    "window = Window.orderBy(desc(\"column1\"))\n",
    "df = df.withColumn(\"row_number\", row_number().over(window))\n",
    "\n",
    "n = 3 # We're interested in the 3rd largest value.\n",
    "row = df.filter(df.row_number == n).first()\n",
    "\n",
    "if row:\n",
    "    print(\"Row number:\", row.row_number)\n",
    "    print(\"Column value:\", row.column1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad17112-e85b-4916-97ce-16603ec17183",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>column1</th><th>row_number</th><th>position</th></tr></thead><tbody><tr><td>3</td><td>12</td><td>2</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         12,
         2,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "column1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "row_number",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "position",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### get the row number of the nth largest value\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.orderBy('id')\n",
    "df_tmp = df.withColumn('position',row_number().over(w))\n",
    "#df_tmp.display()\n",
    "position = 3\n",
    "\n",
    "df_tmp = df_tmp.filter(col('position') <= position)\n",
    "df_result = df_tmp.filter(col('column1') == df_tmp.select(max(col('column1'))).collect()[0][0])\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c14dee-26f4-48ec-8ff2-0c5e6261d557",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#39. How to get the last n rows of a dataframe with row sum > 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8993c102-16ea-48da-98f3-59e0a86d87af",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+----+\n",
       "col1|col2|col3|\n",
       "+----+----+----+\n",
       "  10|  25|  70|\n",
       "  40|   5|  20|\n",
       "  70|  80| 100|\n",
       "  10|   2|  60|\n",
       "  40|  50|  20|\n",
       "+----+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|  10|  25|  70|\n|  40|   5|  20|\n|  70|  80| 100|\n|  10|   2|  60|\n|  40|  50|  20|\n+----+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Display original DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a47d5e-051d-46da-a200-3e00da6ffc40",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1</th><th>col2</th><th>col3</th><th>summ</th></tr></thead><tbody><tr><td>10</td><td>25</td><td>70</td><td>105</td></tr><tr><td>40</td><td>5</td><td>20</td><td>65</td></tr><tr><td>70</td><td>80</td><td>100</td><td>250</td></tr><tr><td>10</td><td>2</td><td>60</td><td>72</td></tr><tr><td>40</td><td>50</td><td>20</td><td>110</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         25,
         70,
         105
        ],
        [
         40,
         5,
         20,
         65
        ],
        [
         70,
         80,
         100,
         250
        ],
        [
         10,
         2,
         60,
         72
        ],
        [
         40,
         50,
         20,
         110
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "summ",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1</th><th>col2</th><th>col3</th><th>summ</th></tr></thead><tbody><tr><td>10</td><td>25</td><td>70</td><td>105</td></tr><tr><td>70</td><td>80</td><td>100</td><td>250</td></tr><tr><td>40</td><td>50</td><td>20</td><td>110</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         25,
         70,
         105
        ],
        [
         70,
         80,
         100,
         250
        ],
        [
         40,
         50,
         20,
         110
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "summ",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### last n rows of a dataframe with row sum > 100\n",
    "df_tmp = df.withColumn('summ',col('col1')+col('col2')+col('col3'))\n",
    "df_tmp.display()\n",
    "n=3\n",
    "df_result_tmp = df_tmp.filter(col('summ') >= 100).tail(n)\n",
    "df_result = spark.createDataFrame(df_result_tmp)\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d34bf68-8db7-4143-abae-4ad74f6f8bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#40. How to create a column containing the minimum by maximum of each row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef25b4af-ae1a-43a7-a1ae-353e2cc86209",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+----+\n",
       "col1|col2|col3|\n",
       "+----+----+----+\n",
       "   1|   2|   3|\n",
       "   4|   5|   6|\n",
       "   7|   8|   9|\n",
       "  10|  11|  12|\n",
       "+----+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   1|   2|   3|\n|   4|   5|   6|\n|   7|   8|   9|\n|  10|  11|  12|\n+----+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample Data\n",
    "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb09f3c-ca0b-44e4-aac6-570b74269649",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1</th><th>col2</th><th>col3</th><th>min_value</th><th>max_value</th><th>min_max_ratio</th></tr></thead><tbody><tr><td>1</td><td>2</td><td>3</td><td>1</td><td>3</td><td>0.3333333333333333</td></tr><tr><td>4</td><td>5</td><td>6</td><td>4</td><td>6</td><td>0.6666666666666666</td></tr><tr><td>7</td><td>8</td><td>9</td><td>7</td><td>9</td><td>0.7777777777777778</td></tr><tr><td>10</td><td>11</td><td>12</td><td>10</td><td>12</td><td>0.8333333333333334</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2,
         3,
         1,
         3,
         0.3333333333333333
        ],
        [
         4,
         5,
         6,
         4,
         6,
         0.6666666666666666
        ],
        [
         7,
         8,
         9,
         7,
         9,
         0.7777777777777778
        ],
        [
         10,
         11,
         12,
         10,
         12,
         0.8333333333333334
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "min_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "max_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "min_max_ratio",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### minimum by maximum of each row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_tmp = df.withColumn(\"min_value\", least(col(\"col1\"),col(\"col2\"),col(\"col3\")))\n",
    "df_tmp = df_tmp.withColumn(\"max_value\", greatest(col(\"col1\"),col(\"col2\"),col(\"col3\")))\n",
    "\n",
    "# Create a new column with the ratio of min_value to max_value\n",
    "df_result = df_tmp.withColumn(\"min_max_ratio\", col(\"min_value\") / col(\"max_value\"))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9ddcb7-d799-45b2-baee-ccd6cbd9628e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#41. How to create a column that contains the penultimate value in each row?\n",
    "- Create a new column ‘penultimate’ which has the second largest value of each row of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8602cb78-4be3-42e5-9846-17a7dc68058a",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+-------+-------+\n",
       "Column1|Column2|Column3|\n",
       "+-------+-------+-------+\n",
       "     10|     20|     30|\n",
       "     40|     60|     50|\n",
       "     80|     70|     90|\n",
       "+-------+-------+-------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+-------+-------+\n|Column1|Column2|Column3|\n+-------+-------+-------+\n|     10|     20|     30|\n|     40|     60|     50|\n|     80|     70|     90|\n+-------+-------+-------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c614c2-8717-41f5-841a-e02c0c8a0b31",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Column1</th><th>Column2</th><th>Column3</th><th>penultimate</th></tr></thead><tbody><tr><td>10</td><td>20</td><td>30</td><td>20</td></tr><tr><td>40</td><td>60</td><td>50</td><td>50</td></tr><tr><td>80</td><td>70</td><td>90</td><td>80</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         20,
         30,
         20
        ],
        [
         40,
         60,
         50,
         50
        ],
        [
         80,
         70,
         90,
         80
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Column1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Column2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Column3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "penultimate",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tmp = df.withColumn('my_array',sort_array(array(df.columns)))\n",
    "df_result = df_tmp.withColumn('penultimate',col('my_array').getItem(1)).drop('my_array')\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf68c54-7982-4e8a-9123-96481a58d279",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#42. How to normalize all columns in a dataframe?\n",
    "- Normalize all columns of df by subtracting the column mean and divide by standard deviation.\n",
    "- Range all columns of df such that the minimum value in each column is 0 and max is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5e2a45-1275-49e6-872f-30625df27e40",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+----+\n",
       "Col1|Col2|Col3|\n",
       "+----+----+----+\n",
       "   1|   2|   3|\n",
       "   2|   3|   4|\n",
       "   3|   4|   5|\n",
       "   4|   5|   6|\n",
       "+----+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+----+\n|Col1|Col2|Col3|\n+----+----+----+\n|   1|   2|   3|\n|   2|   3|   4|\n|   3|   4|   5|\n|   4|   5|   6|\n+----+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a sample dataframe\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(3, 4, 5),\n",
    "(4, 5, 6)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Col1\", \"Col2\", \"Col3\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34593b96-2fb5-47e4-adfe-0e53538640f3",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Col1</th><th>Col2</th><th>Col3</th></tr></thead><tbody><tr><td>-1.161895003862225</td><td>-1.161895003862225</td><td>-1.161895003862225</td></tr><tr><td>-0.3872983346207417</td><td>-0.3872983346207417</td><td>-0.3872983346207417</td></tr><tr><td>0.3872983346207417</td><td>0.3872983346207417</td><td>0.3872983346207417</td></tr><tr><td>1.161895003862225</td><td>1.161895003862225</td><td>1.161895003862225</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         -1.161895003862225,
         -1.161895003862225,
         -1.161895003862225
        ],
        [
         -0.3872983346207417,
         -0.3872983346207417,
         -0.3872983346207417
        ],
        [
         0.3872983346207417,
         0.3872983346207417,
         0.3872983346207417
        ],
        [
         1.161895003862225,
         1.161895003862225,
         1.161895003862225
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Col1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Col2",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Col3",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### normalize all columns\n",
    "df_src=df\n",
    "\n",
    "df_tmp = df.select(*[(mean(col(c)).alias(f'{c}_mean')) for c in df_src.columns],\n",
    "                  *[(stddev(col(c)).alias(f'{c}_stddev')) for c in df_src.columns])\n",
    "\n",
    "# Normalize columns using Z-score (standardization)\n",
    "for c in df_src.columns:\n",
    "    df_src = df_src.withColumn(c, (col(c) - df_tmp.first()[f\"{c}_mean\"]) / df_tmp.first()[f\"{c}_stddev\"])\n",
    "\n",
    "# Min-max scaling\n",
    "#for c in df.columns:\n",
    "#    min_val = df_src.selectExpr(f\"min({c})\").first()[0]\n",
    "#    max_val = df_src.selectExpr(f\"max({c})\").first()[0]\n",
    "#    df_src = df_src.withColumn(c, (col(c) - min_val) / (max_val - min_val))\n",
    "\n",
    "df_src.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328b9cf1-299a-4f86-97a2-9bdc6890f7ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#43. How to get the positions where values of two columns match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef34d03f-af3b-403e-9149-002a74b58d7d",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+-----+\n",
       "Name1|Name2|\n",
       "+-----+-----+\n",
       " John| John|\n",
       " Lily| Lucy|\n",
       "  Sam|  Sam|\n",
       " Lucy| Lily|\n",
       "+-----+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+-----+\n|Name1|Name2|\n+-----+-----+\n| John| John|\n| Lily| Lucy|\n|  Sam|  Sam|\n| Lucy| Lily|\n+-----+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create sample DataFrame\n",
    "data = [(\"John\", \"John\"), (\"Lily\", \"Lucy\"), (\"Sam\", \"Sam\"), (\"Lucy\", \"Lily\")]\n",
    "df = spark.createDataFrame(data, [\"Name1\", \"Name2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07aead15-3c06-4e2b-b5f1-7080f2763b3c",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name1</th><th>Name2</th><th>Match</th></tr></thead><tbody><tr><td>John</td><td>John</td><td>true</td></tr><tr><td>Lily</td><td>Lucy</td><td>false</td></tr><tr><td>Sam</td><td>Sam</td><td>true</td></tr><tr><td>Lucy</td><td>Lily</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         "John",
         true
        ],
        [
         "Lily",
         "Lucy",
         false
        ],
        [
         "Sam",
         "Sam",
         true
        ],
        [
         "Lucy",
         "Lily",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Name2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Match",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = df.withColumn('Match',when(col('Name1')==col('Name2'),True).otherwise(False))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2237d4da-a6c4-4980-a436-ed8f8cd47c13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#44. How to create lags and leads of a column by group in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be324afc-59b1-437c-83ff-61141986cb29",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+------+-----+\n",
       "      Date| Store|Sales|\n",
       "+----------+------+-----+\n",
       "2023-01-01|Store1|  100|\n",
       "2023-01-02|Store1|  150|\n",
       "2023-01-03|Store1|  200|\n",
       "2023-01-04|Store1|  250|\n",
       "2023-01-05|Store1|  300|\n",
       "2023-01-01|Store2|   50|\n",
       "2023-01-02|Store2|   60|\n",
       "2023-01-03|Store2|   80|\n",
       "2023-01-04|Store2|   90|\n",
       "2023-01-05|Store2|  120|\n",
       "+----------+------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+------+-----+\n|      Date| Store|Sales|\n+----------+------+-----+\n|2023-01-01|Store1|  100|\n|2023-01-02|Store1|  150|\n|2023-01-03|Store1|  200|\n|2023-01-04|Store1|  250|\n|2023-01-05|Store1|  300|\n|2023-01-01|Store2|   50|\n|2023-01-02|Store2|   60|\n|2023-01-03|Store2|   80|\n|2023-01-04|Store2|   90|\n|2023-01-05|Store2|  120|\n+----------+------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752340cc-5a93-4b46-8b1e-c45a3ba02e8b",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Store</th><th>Sales</th><th>Lag_Sales</th><th>Lead_Sales</th></tr></thead><tbody><tr><td>2023-01-01</td><td>Store1</td><td>100</td><td>null</td><td>150</td></tr><tr><td>2023-01-02</td><td>Store1</td><td>150</td><td>100</td><td>200</td></tr><tr><td>2023-01-03</td><td>Store1</td><td>200</td><td>150</td><td>250</td></tr><tr><td>2023-01-04</td><td>Store1</td><td>250</td><td>200</td><td>300</td></tr><tr><td>2023-01-05</td><td>Store1</td><td>300</td><td>250</td><td>null</td></tr><tr><td>2023-01-01</td><td>Store2</td><td>50</td><td>null</td><td>60</td></tr><tr><td>2023-01-02</td><td>Store2</td><td>60</td><td>50</td><td>80</td></tr><tr><td>2023-01-03</td><td>Store2</td><td>80</td><td>60</td><td>90</td></tr><tr><td>2023-01-04</td><td>Store2</td><td>90</td><td>80</td><td>120</td></tr><tr><td>2023-01-05</td><td>Store2</td><td>120</td><td>90</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2023-01-01",
         "Store1",
         100,
         null,
         150
        ],
        [
         "2023-01-02",
         "Store1",
         150,
         100,
         200
        ],
        [
         "2023-01-03",
         "Store1",
         200,
         150,
         250
        ],
        [
         "2023-01-04",
         "Store1",
         250,
         200,
         300
        ],
        [
         "2023-01-05",
         "Store1",
         300,
         250,
         null
        ],
        [
         "2023-01-01",
         "Store2",
         50,
         null,
         60
        ],
        [
         "2023-01-02",
         "Store2",
         60,
         50,
         80
        ],
        [
         "2023-01-03",
         "Store2",
         80,
         60,
         90
        ],
        [
         "2023-01-04",
         "Store2",
         90,
         80,
         120
        ],
        [
         "2023-01-05",
         "Store2",
         120,
         90,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Store",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Lag_Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Lead_Sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### lags and leads of a column by group\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert the date from string to date type\n",
    "df_tmp = df.withColumn(\"Date\", to_date(df.Date, 'yyyy-MM-dd'))\n",
    "\n",
    "# Create a Window partitioned by Store, ordered by Date\n",
    "windowSpec = Window.partitionBy(\"Store\").orderBy(\"Date\")\n",
    "\n",
    "# Create lag and lead variables\n",
    "df_tmp = df_tmp.withColumn(\"Lag_Sales\", lag(df_tmp[\"Sales\"]).over(windowSpec))\n",
    "df_result = df_tmp.withColumn(\"Lead_Sales\", lead(df_tmp[\"Sales\"]).over(windowSpec))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ef441b-3dfe-4ba1-91a9-671999da6e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#45. How to get the frequency of unique values in the entire dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee12600-9f13-4527-8d82-fb7b46777624",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------+-------+-------+\n",
       "Column1|Column2|Column3|\n",
       "+-------+-------+-------+\n",
       "      1|      2|      3|\n",
       "      2|      3|      4|\n",
       "      1|      2|      3|\n",
       "      4|      5|      6|\n",
       "      2|      3|      4|\n",
       "+-------+-------+-------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-------+-------+-------+\n|Column1|Column2|Column3|\n+-------+-------+-------+\n|      1|      2|      3|\n|      2|      3|      4|\n|      1|      2|      3|\n|      4|      5|      6|\n|      2|      3|      4|\n+-------+-------+-------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f807651-3b4b-4680-99cd-ebc3ffaff37c",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th><th>freq</th></tr></thead><tbody><tr><td>1</td><td>2</td></tr><tr><td>2</td><td>4</td></tr><tr><td>3</td><td>4</td></tr><tr><td>4</td><td>3</td></tr><tr><td>5</td><td>1</td></tr><tr><td>6</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         2
        ],
        [
         2,
         4
        ],
        [
         3,
         4
        ],
        [
         4,
         3
        ],
        [
         5,
         1
        ],
        [
         6,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "freq",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### frequency of unique values in the entire dataframe\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_result = None\n",
    "\n",
    "for c in df.columns:\n",
    "    if df_result is None:\n",
    "        df_result = df.select(col(c).alias('value'))\n",
    "    else:\n",
    "        df_result = df_result.union(df.select(col(c).alias('value')))\n",
    "\n",
    "df_result = df_result.groupBy(col('value')).agg(count(col('value')).alias('freq')).orderBy(col('value'))\n",
    "df_result.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3215428-953e-4ab3-94b5-6d17fb3db250",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#46. How to replace both the diagonals of dataframe with 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb6229c-0df5-4481-b347-466d854312f5",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+-----+-----+-----+\n",
       "col_1|col_2|col_3|col_4|\n",
       "+-----+-----+-----+-----+\n",
       "    1|    2|    3|    4|\n",
       "    2|    3|    4|    5|\n",
       "    1|    2|    3|    4|\n",
       "    4|    5|    6|    7|\n",
       "+-----+-----+-----+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+-----+-----+-----+\n|col_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n|    1|    2|    3|    4|\n|    2|    3|    4|    5|\n|    1|    2|    3|    4|\n|    4|    5|    6|    7|\n+-----+-----+-----+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526186d3-a8e3-488f-bb84-8cf878f80b44",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+-----+-----+-----+\n",
       "col_1|col_2|col_3|col_4|\n",
       "+-----+-----+-----+-----+\n",
       "    0|    2|    3|    0|\n",
       "    2|    0|    0|    5|\n",
       "    1|    0|    0|    4|\n",
       "    0|    5|    6|    0|\n",
       "+-----+-----+-----+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+-----+-----+-----+\n|col_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n|    0|    2|    3|    0|\n|    2|    0|    0|    5|\n|    1|    0|    0|    4|\n|    0|    5|    6|    0|\n+-----+-----+-----+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "\n",
    "df = df.select([when(col(\"id\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "# Create a reverse id column\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df = df.withColumn(\"id_2\", df.count() - 1 - df[\"id\"])\n",
    "\n",
    "df_with_diag_zero = df.select([when(col(\"id_2\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "df_with_diag_zero.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7110e0-dd40-4b40-a7df-7df85de12785",
     "showTitle": true,
     "title": "Solution 2 - tbd"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-1459838748039776&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> ncols<span class=\"ansi-blue-fg\">=</span>len<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> nlins<span class=\"ansi-blue-fg\">=</span>df<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 8</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> i <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span>ncls<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>     df_tmp <span class=\"ansi-blue-fg\">=</span> df_tmp<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> \n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;ncls&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1459838748039776&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> ncols<span class=\"ansi-blue-fg\">=</span>len<span class=\"ansi-blue-fg\">(</span>df<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> nlins<span class=\"ansi-blue-fg\">=</span>df<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 8</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> i <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span>ncls<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>     df_tmp <span class=\"ansi-blue-fg\">=</span> df_tmp<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> \n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;ncls&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;ncls&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df_tmp = df.withColumn('nrow',row_number().over(w))\n",
    "ncols=len(df.columns)\n",
    "nlins=df.count()\n",
    "for i in range(0,ncls):\n",
    "    df_tmp = df_tmp.select()\n",
    "\n",
    "\n",
    "df_tmp.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed64593-235f-45e8-9615-b35c26442755",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#47. How to reverse the rows of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7785de5e-8e2f-4f4d-a87a-8ebb79512e57",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+-----+-----+-----+\n",
       "col_1|col_2|col_3|col_4|\n",
       "+-----+-----+-----+-----+\n",
       "    1|    2|    3|    4|\n",
       "    2|    3|    4|    5|\n",
       "    3|    4|    5|    6|\n",
       "    4|    5|    6|    7|\n",
       "+-----+-----+-----+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+-----+-----+-----+\n|col_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n|    1|    2|    3|    4|\n|    2|    3|    4|    5|\n|    3|    4|    5|    6|\n|    4|    5|    6|    7|\n+-----+-----+-----+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(3, 4, 5, 6),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4531aa2-62db-4369-9b18-c12c8c06b45d",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_1</th><th>col_2</th><th>col_3</th><th>col_4</th></tr></thead><tbody><tr><td>4</td><td>5</td><td>6</td><td>7</td></tr><tr><td>3</td><td>4</td><td>5</td><td>6</td></tr><tr><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>1</td><td>2</td><td>3</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         5,
         6,
         7
        ],
        [
         3,
         4,
         5,
         6
        ],
        [
         2,
         3,
         4,
         5
        ],
        [
         1,
         2,
         3,
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col_2",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col_3",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "col_4",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "df_tmp = df.withColumn('row',row_number().over(w))\n",
    "df_result = df_tmp.orderBy('row',ascending=False).drop('row')\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0d3af4-8efb-41bb-a86c-2eb878a735e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#48. How to create one-hot encodings of a categorical variable (dummy variables)?\n",
    "- Get one-hot encodings for column Categories in the dataframe df and append it as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853dc9e4-b232-4a9f-b820-839f9d85ce7f",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+\n",
       "Categories|Value|\n",
       "+----------+-----+\n",
       "         A|   10|\n",
       "         A|   20|\n",
       "         B|   30|\n",
       "+----------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+\n|Categories|Value|\n+----------+-----+\n|         A|   10|\n|         A|   20|\n|         B|   30|\n+----------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "#data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10)]\n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506c33e1-1c49-492a-9b6a-15a3ed7c507d",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+------------------+-----------------+\n",
       "Categories|Value|Categories_Indexed|Categories_onehot|\n",
       "+----------+-----+------------------+-----------------+\n",
       "A         |10   |0.0               |(1,[0],[1.0])    |\n",
       "A         |20   |0.0               |(1,[0],[1.0])    |\n",
       "B         |30   |1.0               |(1,[],[])        |\n",
       "+----------+-----+------------------+-----------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+------------------+-----------------+\n|Categories|Value|Categories_Indexed|Categories_onehot|\n+----------+-----+------------------+-----------------+\n|A         |10   |0.0               |(1,[0],[1.0])    |\n|A         |20   |0.0               |(1,[0],[1.0])    |\n|B         |30   |1.0               |(1,[],[])        |\n+----------+-----+------------------+-----------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### one-hot encodings of a categorical variable\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "#from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "# StringIndexer Initialization\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Transform the DataFrame using the fitted StringIndexer model\n",
    "indexed_df = indexerModel.transform(df)\n",
    "#indexed_df.show()\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Categories_Indexed\", outputCol=\"Categories_onehot\")\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "#encoded_df = encoded_df.drop(\"Categories_Indexed\")\n",
    "encoded_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e7b193-82ad-4f19-b23a-7e8bafd39a50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#49. How to Pivot the dataframe (converting rows into columns) ?\n",
    "- convert region column categories to Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0c0c75-04b0-4e70-a9d3-23f1a2f9bd77",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+-------+------+-------+\n",
       "year|quarter|region|revenue|\n",
       "+----+-------+------+-------+\n",
       "2021|      1|    US|   5000|\n",
       "2021|      1|    EU|   4000|\n",
       "2021|      2|    US|   5500|\n",
       "2021|      2|    EU|   4500|\n",
       "2021|      3|    US|   6000|\n",
       "2021|      3|    EU|   5000|\n",
       "2021|      4|    US|   7000|\n",
       "2021|      4|    EU|   6000|\n",
       "+----+-------+------+-------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+-------+------+-------+\n|year|quarter|region|revenue|\n+----+-------+------+-------+\n|2021|      1|    US|   5000|\n|2021|      1|    EU|   4000|\n|2021|      2|    US|   5500|\n|2021|      2|    EU|   4500|\n|2021|      3|    US|   6000|\n|2021|      3|    EU|   5000|\n|2021|      4|    US|   7000|\n|2021|      4|    EU|   6000|\n+----+-------+------+-------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036f1847-49d2-4ad0-ab8f-6c60fb314e77",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>year</th><th>quarter</th><th>EU</th><th>US</th></tr></thead><tbody><tr><td>2021</td><td>2</td><td>4500</td><td>5500</td></tr><tr><td>2021</td><td>1</td><td>4000</td><td>5000</td></tr><tr><td>2021</td><td>3</td><td>5000</td><td>6000</td></tr><tr><td>2021</td><td>4</td><td>6000</td><td>7000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2021,
         2,
         4500,
         5500
        ],
        [
         2021,
         1,
         4000,
         5000
        ],
        [
         2021,
         3,
         5000,
         6000
        ],
        [
         2021,
         4,
         6000,
         7000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "EU",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "US",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "df_result = df.groupBy('year','quarter').pivot('region').sum('revenue')\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133d82b5-d8fe-4228-909a-89b9803267a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#50. How to UnPivot the dataframe (converting columns into rows) ?\n",
    "- UnPivot EU, US columns and create region, revenue Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c919ea6b-fb03-4466-bf8e-a34d30aef71c",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+-------+----+----+\n",
       "year|quarter|  EU|  US|\n",
       "+----+-------+----+----+\n",
       "2021|      2|4500|5500|\n",
       "2021|      1|4000|5000|\n",
       "2021|      3|5000|6000|\n",
       "2021|      4|6000|7000|\n",
       "+----+-------+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+-------+----+----+\n|year|quarter|  EU|  US|\n+----+-------+----+----+\n|2021|      2|4500|5500|\n|2021|      1|4000|5000|\n|2021|      3|5000|6000|\n|2021|      4|6000|7000|\n+----+-------+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(2021, 2, 4500, 5500),\n",
    "(2021, 1, 4000, 5000),\n",
    "(2021, 3, 5000, 6000),\n",
    "(2021, 4, 6000, 7000)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad7b997-192e-4d53-9b75-9190bcd77635",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-607835847313252&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### UnPivot the dataframe</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> df_result = df.unpivot(ids=[&#34;year&#34;, &#34;quarter&#34;],\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">      3</span>                           values<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;EU&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;US&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                           variableColumnName<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;country&#34;</span><span class=\"ansi-blue-fg\">,</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>                           valueColumnName=&#34;value&#34;)\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1664</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1665</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1666</span><span class=\"ansi-red-fg\">             raise AttributeError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1667</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1668</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;unpivot&#39;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-607835847313252&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### UnPivot the dataframe</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> df_result = df.unpivot(ids=[&#34;year&#34;, &#34;quarter&#34;],\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      3</span>                           values<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;EU&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;US&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                           variableColumnName<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;country&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>                           valueColumnName=&#34;value&#34;)\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1664</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1665</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1666</span><span class=\"ansi-red-fg\">             raise AttributeError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1667</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n<span class=\"ansi-green-intense-fg ansi-bold\">   1668</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;unpivot&#39;</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;unpivot&#39;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### UnPivot the dataframe\n",
    "df_result = df.unpivot(ids=[\"year\", \"quarter\"],\n",
    "                          values=[\"EU\", \"US\"],\n",
    "                          variableColumnName=\"country\",\n",
    "                          valueColumnName=\"value\")\n",
    "\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6896c0fc-3a66-4bcd-a8ec-17f951361cba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#51. How to impute missing values with Zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e901339-5980-41f3-b5e8-3cb8b417c407",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+\n",
       "   a|   b|\n",
       "+----+----+\n",
       "   1|null|\n",
       "null|   2|\n",
       "   3|   4|\n",
       "   5|null|\n",
       "+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+\n|   a|   b|\n+----+----+\n|   1|null|\n|null|   2|\n|   3|   4|\n|   5|null|\n+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppose df is your DataFrame\n",
    "df = spark.createDataFrame([(1, None), (None, 2), (3, 4), (5, None)], [\"a\", \"b\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2dfb89-c0f2-46c5-849a-ebefa5a174c1",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>a</th><th>b</th></tr></thead><tbody><tr><td>1</td><td>0</td></tr><tr><td>0</td><td>2</td></tr><tr><td>3</td><td>4</td></tr><tr><td>5</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         0
        ],
        [
         0,
         2
        ],
        [
         3,
         4
        ],
        [
         5,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "a",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "b",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = df.select(*[when(col(c).isNull(),lit(0)).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de86b03-6cfb-4a8f-85f0-4734ac1b09f4",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+---+\n",
       "  a|  b|\n",
       "+---+---+\n",
       "  1|  0|\n",
       "  0|  2|\n",
       "  3|  4|\n",
       "  5|  0|\n",
       "+---+---+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+---+\n|  a|  b|\n+---+---+\n|  1|  0|\n|  0|  2|\n|  3|  4|\n|  5|  0|\n+---+---+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_imputed = df.fillna(0)\n",
    "\n",
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37080776-2f15-4ed1-b085-e0aabfa5d38f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#52. How to identify continuous variables in a dataframe and create a list of those column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9341c2cd-04c1-4a5d-b869-5bc257e37da1",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-607835847313258&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> spark<span class=\"ansi-blue-fg\">.</span>sparkContext<span class=\"ansi-blue-fg\">.</span>addFile<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>SparkFiles<span class=\"ansi-blue-fg\">.</span>get<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Churn_Modelling_m.csv&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">#df = spark.read.csv(&#34;C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling_m.csv&#34;, header=True, inferSchema=True)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    762</span>             path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>path<span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    763</span>         <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> list<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 764</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    765</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    766</span>             <span class=\"ansi-green-fg\">def</span> func<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: dbfs:/local_disk0/spark-719d73cc-2fd8-4de5-83a6-9bf822f9cef8/userFiles-27d14bd7-c017-4361-b8c7-7cedbcbaca9c/Churn_Modelling_m.csv</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-607835847313258&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> spark<span class=\"ansi-blue-fg\">.</span>sparkContext<span class=\"ansi-blue-fg\">.</span>addFile<span class=\"ansi-blue-fg\">(</span>url<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>SparkFiles<span class=\"ansi-blue-fg\">.</span>get<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Churn_Modelling_m.csv&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">#df = spark.read.csv(&#34;C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling_m.csv&#34;, header=True, inferSchema=True)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    762</span>             path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>path<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    763</span>         <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> list<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 764</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    765</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    766</span>             <span class=\"ansi-green-fg\">def</span> func<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: dbfs:/local_disk0/spark-719d73cc-2fd8-4de5-83a6-9bf822f9cef8/userFiles-27d14bd7-c017-4361-b8c7-7cedbcbaca9c/Churn_Modelling_m.csv</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: dbfs:/local_disk0/spark-719d73cc-2fd8-4de5-83a6-9bf822f9cef8/userFiles-27d14bd7-c017-4361-b8c7-7cedbcbaca9c/Churn_Modelling_m.csv",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling_m.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling_m.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "#df = spark.read.csv(\"C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling_m.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabff210-f4f2-4db9-8183-be9fade3b128",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, NumericType\n",
    "from pyspark.sql.functions import approxCountDistinct\n",
    "\n",
    "def detect_continuous_variables(df, distinct_threshold):\n",
    "    \"\"\"\n",
    "    Identify continuous variables in a PySpark DataFrame.\n",
    "    :param df: The input PySpark DataFrame\n",
    "    :param distinct_threshold: Threshold to qualify as continuous variables - Count of distinct values > distinct_threshold\n",
    "    :return: A List containing names of continuous variables\n",
    "    \"\"\"\n",
    "    continuous_columns = []\n",
    "    for column in df.columns:\n",
    "        dtype = df.schema[column].dataType\n",
    "        if isinstance(dtype, (IntegerType, NumericType)):\n",
    "            distinct_count = df.select(approxCountDistinct(column)).collect()[0][0]\n",
    "        if distinct_count > distinct_threshold:\n",
    "            continuous_columns.append(column)\n",
    "        return continuous_columns\n",
    "\n",
    "continuous_variables = detect_continuous_variables(df, 10)\n",
    "print(continuous_variables)\n",
    "#['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab905be-096e-40cc-b96c-a353e764aae9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#53. How to calculate Mode of a PySpark DataFrame column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89228ae7-310b-4ee4-8713-226e0a5e9004",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+----+----+\n",
       "col1|col2|col3|\n",
       "+----+----+----+\n",
       "   1|   2|   3|\n",
       "   2|   2|   3|\n",
       "   2|   2|   4|\n",
       "   1|   2|   3|\n",
       "   1|   1|   3|\n",
       "+----+----+----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   1|   2|   3|\n|   2|   2|   3|\n|   2|   2|   4|\n|   1|   2|   3|\n|   1|   1|   3|\n+----+----+----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1, 2, 3), (2, 2, 3), (2, 2, 4), (1, 2, 3), (1, 1, 3)]\n",
    "columns = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d3cad5-d490-408f-b020-12bbb5877894",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----+-----+\n",
       "col2|count|\n",
       "+----+-----+\n",
       "   2|    4|\n",
       "+----+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----+-----+\n|col2|count|\n+----+-----+\n|   2|    4|\n+----+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_grouped = df.groupBy('col2').count()\n",
    "mode_df = df_grouped.orderBy(col('count').desc()).limit(1)\n",
    "\n",
    "mode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6684e9a-9929-4ffa-a488-09a13ea11421",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#54. How to find installed location of Apache Spark and PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f96d92-c522-4a58-a57e-9f2080899b9f",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-3151542764839094&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### find installed location of Apache Spark and PySpark</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> findspark\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> findspark<span class=\"ansi-blue-fg\">.</span>init<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> print<span class=\"ansi-blue-fg\">(</span>findspark<span class=\"ansi-blue-fg\">.</span>find<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    161</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 162</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    163</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    164</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;findspark&#39;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3151542764839094&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">### find installed location of Apache Spark and PySpark</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> findspark\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> findspark<span class=\"ansi-blue-fg\">.</span>init<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> print<span class=\"ansi-blue-fg\">(</span>findspark<span class=\"ansi-blue-fg\">.</span>find<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    161</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 162</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    164</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;findspark&#39;</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;findspark&#39;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### find installed location of Apache Spark and PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "print(os.path.dirname(pyspark.__file__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3fccf2a-88ac-4f80-8916-79b6648dc621",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#55. How to convert a column to lower case using UDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817d7e5e-69e2-48dd-a881-82215909538e",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------------+-------------+\n",
       "        Name|         City|\n",
       "+------------+-------------+\n",
       "    John Doe|     NEW YORK|\n",
       "    Jane Doe|  LOS ANGELES|\n",
       "Mike Johnson|      CHICAGO|\n",
       "  Sara Smith|SAN FRANCISCO|\n",
       "+------------+-------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------------+-------------+\n|        Name|         City|\n+------------+-------------+\n|    John Doe|     NEW YORK|\n|    Jane Doe|  LOS ANGELES|\n|Mike Johnson|      CHICAGO|\n|  Sara Smith|SAN FRANCISCO|\n+------------+-------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame to test\n",
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['Name', 'City'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5544859-4388-4c30-89c9-c279cf7ba358",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th></tr></thead><tbody><tr><td>john doe</td><td>NEW YORK</td></tr><tr><td>jane doe</td><td>LOS ANGELES</td></tr><tr><td>mike johnson</td><td>CHICAGO</td></tr><tr><td>sara smith</td><td>SAN FRANCISCO</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "john doe",
         "NEW YORK"
        ],
        [
         "jane doe",
         "LOS ANGELES"
        ],
        [
         "mike johnson",
         "CHICAGO"
        ],
        [
         "sara smith",
         "SAN FRANCISCO"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th></tr></thead><tbody><tr><td>john doe</td><td>new york</td></tr><tr><td>jane doe</td><td>los angeles</td></tr><tr><td>mike johnson</td><td>chicago</td></tr><tr><td>sara smith</td><td>san francisco</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "john doe",
         "new york"
        ],
        [
         "jane doe",
         "los angeles"
        ],
        [
         "mike johnson",
         "chicago"
        ],
        [
         "sara smith",
         "san francisco"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def lower_case(col_name,data_frame) -> DataFrame:\n",
    "    df_tmp = data_frame.withColumn(col_name,lower(col(col_name)))\n",
    "    return df_tmp\n",
    "\n",
    "df_result = lower_case('Name',df)\n",
    "df_result.display()\n",
    "\n",
    "\n",
    "def lower_case(data_frame) -> DataFrame:\n",
    "    df_tmp = data_frame.select(*[lower(col(c)).alias(c) for c in df.columns])\n",
    "    return df_tmp\n",
    "\n",
    "df_result = lower_case(df)\n",
    "df_result.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d4ca7a-9faa-4635-97c9-19dde8679ece",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>City</th><th>City_lower</th></tr></thead><tbody><tr><td>John Doe</td><td>NEW YORK</td><td>new york</td></tr><tr><td>Jane Doe</td><td>LOS ANGELES</td><td>los angeles</td></tr><tr><td>Mike Johnson</td><td>CHICAGO</td><td>chicago</td></tr><tr><td>Sara Smith</td><td>SAN FRANCISCO</td><td>san francisco</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John Doe",
         "NEW YORK",
         "new york"
        ],
        [
         "Jane Doe",
         "LOS ANGELES",
         "los angeles"
        ],
        [
         "Mike Johnson",
         "CHICAGO",
         "chicago"
        ],
        [
         "Sara Smith",
         "SAN FRANCISCO",
         "san francisco"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City_lower",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### convert a column to lower case using UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define your UDF function\n",
    "def to_lower(s):\n",
    "    if s is not None:\n",
    "        return s.lower()\n",
    "\n",
    "# Convert your Python function to a Spark UDF\n",
    "udf_to_lower = udf(to_lower, StringType())\n",
    "\n",
    "# Apply your UDF to the DataFrame\n",
    "df = df.withColumn('City_lower', udf_to_lower(df['City']))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6e3d25-ab57-499a-abdb-f99b7b75fab3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#56. How to convert PySpark data frame to pandas dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7cb55e-dd50-4362-8562-f3c24a9817b5",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------------+-------------+\n",
       "        Name|         City|\n",
       "+------------+-------------+\n",
       "    John Doe|     NEW YORK|\n",
       "    Jane Doe|  LOS ANGELES|\n",
       "Mike Johnson|      CHICAGO|\n",
       "  Sara Smith|SAN FRANCISCO|\n",
       "+------------+-------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------------+-------------+\n|        Name|         City|\n+------------+-------------+\n|    John Doe|     NEW YORK|\n|    Jane Doe|  LOS ANGELES|\n|Mike Johnson|      CHICAGO|\n|  Sara Smith|SAN FRANCISCO|\n+------------+-------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame to test\n",
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "pysparkDF = spark.createDataFrame(data, ['Name', 'City'])\n",
    "\n",
    "pysparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c20c572d-eabc-4c9a-80d6-c3c219149ddd",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">           Name           City\n",
       "0      John Doe       NEW YORK\n",
       "1      Jane Doe    LOS ANGELES\n",
       "2  Mike Johnson        CHICAGO\n",
       "3    Sara Smith  SAN FRANCISCO\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">           Name           City\n0      John Doe       NEW YORK\n1      Jane Doe    LOS ANGELES\n2  Mike Johnson        CHICAGO\n3    Sara Smith  SAN FRANCISCO\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result = pysparkDF.toPandas()\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c546924-ec44-464d-bce9-1e18665dec49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#57. How to View PySpark Cluster Details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8683c8-7089-4141-bf27-74d3d5799486",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">http://172.24.129.196:40001\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">http://172.24.129.196:40001\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Cluster Details\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d724b40-f243-4fd9-aaa8-2b86bddee1c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#58. How to View PySpark Cluster Configuration Details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c21c12-ac68-47f1-bea6-6ef31b85a1f5",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">http://172.24.129.196:40001\n",
       "spark.databricks.preemption.enabled : true\n",
       "spark.databricks.clusterUsageTags.driverContainerId : ae4932b93ce24c0da2d5cad7243bc496\n",
       "fs.azure.account.oauth2.client.secret.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.clusterFirstOnDemand : 1\n",
       "spark.driver.tempDirectory : /local_disk0/tmp\n",
       "spark.databricks.queryWatchdog.maxQueryTasks : 200000\n",
       "spark.databricks.managedCatalog.clientClassName : com.databricks.managedcatalog.ManagedCatalogClientImpl\n",
       "spark.databricks.clusterUsageTags.sparkImageLabel : release__9.1.x-snapshot-scala2.12__databricks-universe__9.1.61__73212e5__c999d52__jenkins__0f87a2f__format-3\n",
       "spark.hadoop.fs.fcfs-s3.impl.disable.cache : true\n",
       "spark.sql.streaming.checkpointFileManagerClass : com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\n",
       "spark.databricks.service.dbutils.repl.backend : com.databricks.dbconnect.ReplDBUtils\n",
       "spark.streaming.driver.writeAheadLog.allowBatching : true\n",
       "spark.hadoop.hive.server2.transport.mode : http\n",
       "spark.databricks.clusterUsageTags.driverInstanceId : 1b8c43f2ed3f482ba4ac1aa290277907\n",
       "fs.azure.account.oauth.provider.type.datafounddeveuwdlsiss.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
       "spark.app.id : app-20240510174946-0000\n",
       "spark.hadoop.fs.cpfs-adl.impl.disable.cache : true\n",
       "spark.databricks.clusterUsageTags.hailEnabled : false\n",
       "spark.hadoop.fs.mcfs-s3.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.databricks.clusterUsageTags.containerType : LXC\n",
       "spark.eventLog.enabled : false\n",
       "spark.databricks.clusterUsageTags.isIMv2Enabled : false\n",
       "spark.databricks.cluster.profile : serverless\n",
       "spark.repl.class.outputDir : /local_disk0/tmp/repl/spark-4618732084335514242-f8147b84-3151-4f4f-b891-a1eea072ef71\n",
       "spark.hadoop.hive.hmshandler.retry.interval : 2000\n",
       "spark.executor.tempDirectory : /local_disk0/tmp\n",
       "spark.databricks.clusterUsageTags.clusterLastActivityTime : 1715359672603\n",
       "spark.databricks.secret.sparkConf.keys.toRedact : ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5xYXN0ZGV2ZXV3ZGxzZHNsMDEuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2lzcy5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNpc3MuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLnFhc3RkZXZldXdkbHNkc2wwMS5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNlbnIuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2Vuci5kZnMuY29yZS53aW5kb3dzLm5ldA==\n",
       "spark.databricks.clusterUsageTags.clusterCreator : ThirdPartyApp\n",
       "spark.hadoop.mapred.output.committer.class : com.databricks.backend.daemon.data.client.DirectOutputCommitter\n",
       "spark.databricks.clusterUsageTags.clusterTargetWorkers : 2\n",
       "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version : 2\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 : 0\n",
       "spark.sql.allowMultipleContexts : false\n",
       "spark.databricks.eventLog.enabled : true\n",
       "spark.hadoop.hive.server2.thrift.http.port : 10000\n",
       "spark.home : /databricks/spark\n",
       "spark.hadoop.hive.server2.idle.operation.timeout : 7200000\n",
       "fs.azure.account.oauth2.client.endpoint.qastdeveuwdlsdsl01.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\n",
       "spark.repl.class.uri : spark://172.24.129.196:35411/classes\n",
       "spark.task.reaper.enabled : true\n",
       "spark.storage.memoryFraction : 0.5\n",
       "spark.databricks.clusterUsageTags.autoTerminationMinutes : 60\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline : false\n",
       "spark.hadoop.fs.fcfs-s3.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
       "spark.databricks.delta.multiClusterWrites.enabled : true\n",
       "spark.databricks.clusterUsageTags.driverInstancePrivateIp : 172.24.129.132\n",
       "spark.worker.cleanup.enabled : false\n",
       "spark.sql.legacy.createHiveTableByDefault : false\n",
       "spark.databricks.driver.preferredMavenCentralMirrorUrl : https://maven-central.storage-download.googleapis.com/maven2/\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File : 0\n",
       "spark.hadoop.fs.fcfs-s3a.impl.disable.cache : true\n",
       "spark.ui.port : 40001\n",
       "spark.databricks.workspace.matplotlibInline.enabled : true\n",
       "spark.databricks.clusterUsageTags.enableCredentialPassthrough : false\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign : false\n",
       "spark.databricks.clusterUsageTags.clusterNodeType : Standard_E4ds_v4\n",
       "spark.databricks.clusterUsageTags.enableJdbcAutoStart : true\n",
       "spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough : false\n",
       "spark.hadoop.fs.fcfs-s3n.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
       "spark.hadoop.fs.wasb.impl.disable.cache : true\n",
       "spark.databricks.wsfsPublicPreview : true\n",
       "spark.cleaner.referenceTracking.blocking : false\n",
       "fs.azure.account.oauth2.client.id.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.isSingleUserCluster : false\n",
       "spark.databricks.clusterUsageTags.clusterState : Pending\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes : false\n",
       "spark.databricks.tahoe.logStore.azure.class : com.databricks.tahoe.store.AzureLogStore\n",
       "spark.hadoop.fs.azure.skip.metrics : true\n",
       "spark.hadoop.fs.s3.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.hadoop.hive.hmshandler.retry.attempts : 10\n",
       "spark.scheduler.mode : FAIR\n",
       "spark.sql.sources.default : delta\n",
       "spark.hadoop.fs.mcfs-gs.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.hadoop.fs.cpfs-s3n.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.hadoop.fs.cpfs-adl.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.hadoop.fs.fcfs-s3n.impl.disable.cache : true\n",
       "spark.hadoop.fs.cpfs-abfss.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "fs.azure.account.oauth2.client.secret.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\n",
       "datanucleus.autoCreateSchema : true\n",
       "fs.azure.account.oauth2.client.id.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.clusterNumCustomTags : 1\n",
       "spark.databricks.clusterUsageTags.driverContainerPrivateIp : 172.24.129.196\n",
       "spark.databricks.passthrough.oauth.refresher.impl : com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\n",
       "spark.sql.hive.metastore.sharedPrefixes : org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\n",
       "spark.sql.hive.metastore.version : 2.3.7\n",
       "spark.databricks.io.directoryCommit.enableLogicalDelete : false\n",
       "spark.task.reaper.killTimeout : 60s\n",
       "spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider\n",
       "spark.hadoop.parquet.block.size.row.check.min : 10\n",
       "spark.hadoop.hive.server2.use.SSL : true\n",
       "spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType : vnet-injection\n",
       "spark.hadoop.fs.mcfs-s3a.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.hadoop.databricks.dbfs.client.version : v2\n",
       "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb : 0\n",
       "spark.databricks.repl.allowedLanguages : sql,python,r\n",
       "spark.hadoop.hive.server2.keystore.path : /databricks/keys/jetty-ssl-driver-keystore.jks\n",
       "spark.databricks.clusterUsageTags.clusterAllTags : [{&#34;key&#34;:&#34;ResourceClass&#34;,&#34;value&#34;:&#34;Serverless&#34;},{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;[REDACTED]&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;1202-161340-zrtmi161&#34;},{&#34;key&#34;:&#34;ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;UDP_Solution_BusinessOwner&#34;,&#34;value&#34;:&#34;ola.petersson@volvo.com&#34;},{&#34;key&#34;:&#34;Technical_Contact&#34;,&#34;value&#34;:&#34;maciej.poborca@consultant.volvo.com&#34;},{&#34;key&#34;:&#34;Business_Owner&#34;,&#34;value&#34;:&#34;bartlomiej.wierzbicki@volvo.com&#34;},{&#34;key&#34;:&#34;UDP_Platform_Environment&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;Assignment_Code&#34;,&#34;value&#34;:&#34;VY064I&#34;},{&#34;key&#34;:&#34;UDP_Solution_Name&#34;,&#34;value&#34;:&#34;qast&#34;},{&#34;key&#34;:&#34;InfoSeC&#34;,&#34;value&#34;:&#34;Confidential&#34;},{&#34;key&#34;:&#34;UDP_Solution_Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;UDP_Solution_ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;Environment_Type&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;PDS&#34;,&#34;value&#34;:&#34;High&#34;},{&#34;key&#34;:&#34;UDP_Solution_TechnicalContact&#34;,&#34;value&#34;:&#34;nikhil.rathore@volvo.com&#34;},{&#34;key&#34;:&#34;Mapped_ID&#34;,&#34;value&#34;:&#34;Automatic&#34;},{&#34;key&#34;:&#34;UDP_Solution_AssignmentCode&#34;,&#34;value&#34;:&#34;VY09ZT&#34;},{&#34;key&#34;:&#34;Service_Level&#34;,&#34;value&#34;:&#34;CEP_Basic&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8122829732360297&#34;}]\n",
       "spark.databricks.credential.redactor : com.databricks.logging.secrets.CredentialRedactorProxyImpl\n",
       "spark.sql.legacy.parquet.datetimeRebaseModeInRead : LEGACY\n",
       "spark.databricks.acl.provider : com.databricks.sql.acl.ReflectionBackedAclProvider\n",
       "spark.app.startTime : 1715363383441\n",
       "spark.databricks.mlflow.autologging.enabled : true\n",
       "spark.extraListeners : com.databricks.backend.daemon.driver.DBCEventLoggingListener\n",
       "spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled : false\n",
       "spark.sql.parquet.cacheMetadata : true\n",
       "spark.databricks.clusterUsageTags.clusterLogDestination : dbfs:/mnt/cluster-logs\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss : 0\n",
       "spark.r.sql.derby.temp.dir : /tmp/RtmpsKdyDl\n",
       "spark.hadoop.fs.adl.impl : com.databricks.adl.AdlFileSystem\n",
       "spark.hadoop.fs.cpfs-abfss.impl.disable.cache : true\n",
       "spark.databricks.clusterUsageTags.azureSubscriptionId : 04abf2eb-58f5-4264-8858-3793629a7259\n",
       "spark.databricks.clusterUsageTags.enableLocalDiskEncryption : false\n",
       "spark.databricks.tahoe.logStore.class : com.databricks.tahoe.store.DelegatingLogStore\n",
       "fs.azure.account.oauth2.client.id.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\n",
       "libraryDownload.sleepIntervalSeconds : 5\n",
       "spark.sql.hive.convertMetastoreParquet : true\n",
       "spark.databricks.service.dbutils.server.backend : com.databricks.dbconnect.SparkServerDBUtils\n",
       "spark.executor.id : driver\n",
       "spark.databricks.managedCatalog.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider\n",
       "spark.databricks.repl.enableClassFileCleanup : true\n",
       "spark.databricks.clusterUsageTags.clusterAvailability : SPOT_WITH_FALLBACK_AZURE\n",
       "spark.sql.catalogImplementation : hive\n",
       "spark.sql.legacy.parquet.datetimeRebaseModeInWrite : LEGACY\n",
       "spark.databricks.clusterUsageTags.region : westeurope\n",
       "spark.databricks.clusterUsageTags.instanceWorkerEnvId : workerenv-8122829732360297\n",
       "spark.hadoop.fs.s3a.multipart.size : 10485760\n",
       "spark.databricks.clusterUsageTags.effectiveSparkVersion : 9.1.x-scala2.12\n",
       "spark.metrics.conf : /databricks/spark/conf/metrics.properties\n",
       "spark.databricks.workspaceUrl : adb-8122829732360297.17.azuredatabricks.net\n",
       "spark.akka.frameSize : 256\n",
       "spark.hadoop.fs.s3a.fast.upload : true\n",
       "spark.hadoop.fs.wasbs.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\n",
       "spark.sql.streaming.stopTimeout : 15s\n",
       "spark.hadoop.hive.server2.keystore.password : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting : false\n",
       "spark.executor.extraClassPath : /databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--ch.qos.reload4j--reload4j--ch.qos.reload4j__reload4j__1.2.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacecommerceanalytics--com.amazonaws__aws-java-sdk-marketplacecommerceanalytics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacemeteringservice--com.amazonaws__aws-java-sdk-marketplacemeteringservice__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.29.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.\n",
       "*** WARNING: skipped 62833 bytes of output ***\n",
       "\n",
       "\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape : false\n",
       "spark.databricks.overrideDefaultCommitProtocol : org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\n",
       "spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass : com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\n",
       "spark.databricks.clusterUsageTags.clusterNoDriverDaemon : false\n",
       "libraryDownload.timeoutSeconds : 180\n",
       "spark.hadoop.parquet.memory.pool.ratio : 0.5\n",
       "spark.databricks.clusterUsageTags.clusterOwnerUserId : 8668945429017751\n",
       "spark.hadoop.javax.jdo.option.ConnectionUserName : hiveuser\n",
       "spark.databricks.sparkContextId : 4618732084335514242\n",
       "spark.databricks.clusterUsageTags.clusterScalingType : fixed_size\n",
       "spark.databricks.passthrough.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\n",
       "spark.executor.memory : 21588m\n",
       "fs.azure.account.oauth.provider.type.qastdeveuwdlsdsl01.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
       "spark.databricks.tahoe.logStore.gcp.class : com.databricks.tahoe.store.GCPLogStore\n",
       "spark.serializer.objectStreamReset : 100\n",
       "spark.databricks.clusterUsageTags.sparkMasterUrlType : None\n",
       "spark.hadoop.javax.jdo.option.ConnectionDriverName : com.microsoft.sqlserver.jdbc.SQLServerDriver\n",
       "spark.sql.sources.commitProtocolClass : com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\n",
       "spark.hadoop.fs.abfss.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs : 0\n",
       "spark.hadoop.fs.fcfs-s3a.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
       "spark.databricks.clusterUsageTags.attribute_tag_budget : \n",
       "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType : azure_disk_volume_type: PREMIUM_LRS\n",
       "\n",
       "spark.databricks.clusterUsageTags.clusterWorkers : 2\n",
       "spark.databricks.clusterUsageTags.clusterPythonVersion : 3\n",
       "spark.databricks.clusterUsageTags.enableDfAcls : false\n",
       "spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders : true\n",
       "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount : 0\n",
       "spark.shuffle.service.enabled : true\n",
       "spark.hadoop.fs.file.impl : com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\n",
       "spark.hadoop.fs.mcfs-s3n.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.hadoop.fs.cpfs-s3.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer : \n",
       "fs.azure.account.oauth.provider.type.datafounddeveuwdlsenr.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
       "spark.hadoop.fs.s3a.multipart.threshold : 104857600\n",
       "spark.databricks.workerNodeTypeId : Standard_E4ds_v4\n",
       "spark.rpc.message.maxSize : 256\n",
       "spark.databricks.clusterUsageTags.workerEnvironmentId : workerenv-8122829732360297\n",
       "spark.databricks.clusterUsageTags.attribute_tag_dust_suite : \n",
       "spark.databricks.driverNfs.enabled : true\n",
       "spark.databricks.clusterUsageTags.clusterMetastoreAccessType : RDS_DIRECT\n",
       "spark.databricks.passthrough.glue.executorServiceFactoryClassName : com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\n",
       "spark.hadoop.javax.jdo.option.ConnectionPassword : P5fm4wpj6fm4u4ebrag23o2lqwk2qyahju4tcg34x!\n",
       "spark.hadoop.fs.abfs.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\n",
       "spark.master : spark://172.24.129.196:7077\n",
       "spark.databricks.clusterUsageTags.enableElasticDisk : true\n",
       "spark.databricks.acl.scim.client : com.databricks.spark.sql.acl.client.DriverToWebappScimClient\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick : false\n",
       "spark.hadoop.fs.adl.impl.disable.cache : true\n",
       "spark.hadoop.parquet.block.size.row.check.max : 10\n",
       "spark.hadoop.fs.s3a.connection.maximum : 200\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 : 0\n",
       "spark.hadoop.fs.s3a.fast.upload.active.blocks : 32\n",
       "spark.shuffle.reduceLocality.enabled : false\n",
       "spark.hadoop.spark.sql.sources.outputCommitterClass : com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\n",
       "spark.hadoop.fs.AbstractFileSystem.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\n",
       "spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled : false\n",
       "spark.sql.parquet.compression.codec : snappy\n",
       "spark.databricks.cloudProvider : Azure\n",
       "spark.driver.maxResultSize : 0\n",
       "fs.azure.account.oauth2.client.secret.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\n",
       "spark.executor.extraJavaOptions : -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\n",
       "spark.databricks.cloudfetch.hasRegionSupport : true\n",
       "spark.databricks.clusterUsageTags.ngrokNpipEnabled : true\n",
       "spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories : false\n",
       "spark.hadoop.fs.wasb.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\n",
       "spark.hadoop.fs.mcfs-abfss.impl.disable.cache : true\n",
       "spark.databricks.passthrough.glue.credentialsProviderFactoryClassName : com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\n",
       "spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice : -1.0\n",
       "spark.sql.warehouse.dir : /user/hive/warehouse\n",
       "spark.databricks.passthrough.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\n",
       "spark.databricks.session.share : false\n",
       "spark.databricks.clusterUsageTags.userId : 8668945429017751\n",
       "spark.databricks.isShieldWorkspace : false\n",
       "spark.databricks.telemetry.prometheus.samplingRate : 100\n",
       "spark.driver.host : 172.24.129.196\n",
       "spark.databricks.clusterUsageTags.clusterSku : STANDARD_SKU\n",
       "spark.hadoop.fs.gs.impl.disable.cache : true\n",
       "spark.databricks.clusterUsageTags.managedResourceGroup : databricks-rg-udp-qast-dev-euw-dbw-lxni2stdfcvd6\n",
       "spark.databricks.privateLinkEnabled : false\n",
       "spark.databricks.clusterUsageTags.clusterUnityCatalogMode : CUSTOM\n",
       "spark.databricks.io.cache.enabled : true\n",
       "spark.databricks.managedCatalog.gcs.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider\n",
       "fs.azure.account.auth.type.datafounddeveuwdlsiss.dfs.core.windows.net : OAuth\n",
       "spark.databricks.automl.serviceEnabled : true\n",
       "spark.hadoop.parquet.page.size.check.estimate : false\n",
       "spark.hadoop.spark.driverproxy.customHeadersToProperties : X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name\n",
       "spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class : com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\n",
       "spark.databricks.clusterUsageTags.attribute_tag_service : \n",
       "spark.databricks.delta.preview.enabled : true\n",
       "spark.databricks.clusterSource : API\n",
       "spark.databricks.metrics.filesystem_io_metrics : true\n",
       "spark.databricks.clusterUsageTags.dataPlaneRegion : westeurope\n",
       "spark.databricks.cloudfetch.requesterClassName : com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\n",
       "spark.databricks.delta.logStore.crossCloud.fatal : true\n",
       "spark.files.fetchFailure.unRegisterOutputOnHost : true\n",
       "spark.databricks.clusterUsageTags.enableSqlAclsOnly : false\n",
       "spark.databricks.clusterUsageTags.clusterNumSshKeys : 0\n",
       "spark.databricks.clusterUsageTags.clusterSizeType : VM_CONTAINER\n",
       "spark.hadoop.databricks.fs.perfMetrics.enable : true\n",
       "spark.databricks.clusterUsageTags.clusterPinned : true\n",
       "spark.hadoop.fs.gs.outputstream.upload.chunk.size : 16777216\n",
       "spark.speculation.quantile : 0.9\n",
       "spark.databricks.clusterUsageTags.privateLinkEnabled : false\n",
       "spark.shuffle.manager : SORT\n",
       "spark.files.overwrite : true\n",
       "spark.databricks.clusterUsageTags.userProvidedSparkVersion : 9.1.x-scala2.12\n",
       "spark.databricks.driverNodeTypeId : Standard_E4ds_v4\n",
       "datanucleus.fixedDatastore : false\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes : false\n",
       "spark.r.numRBackendThreads : 1\n",
       "spark.hadoop.fs.wasbs.impl.disable.cache : true\n",
       "spark.hadoop.fs.abfss.impl.disable.cache : true\n",
       "spark.databricks.workspace.multipleResults.enabled : true\n",
       "spark.shuffle.service.port : 4048\n",
       "spark.databricks.clusterUsageTags.clusterOwnerOrgId : 8122829732360297\n",
       "spark.databricks.acl.client : com.databricks.spark.sql.acl.client.SparkSqlAclClient\n",
       "spark.streaming.driver.writeAheadLog.closeFileAfterWrite : true\n",
       "spark.hadoop.hive.warehouse.subdir.inherit.perms : false\n",
       "fs.azure.account.auth.type.qastdeveuwdlsdsl01.dfs.core.windows.net : OAuth\n",
       "spark.hadoop.fs.mcfs-abfss.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.databricks.clusterUsageTags.runtimeEngine : STANDARD\n",
       "spark.databricks.clusterUsageTags.isServicePrincipalCluster : false\n",
       "spark.hadoop.fs.s3n.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.databricks.clusterUsageTags.orgId : 8122829732360297\n",
       "spark.databricks.enablePublicDbfsFuse : false\n",
       "spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 : 1\n",
       "spark.driver.port : 35411\n",
       "spark.databricks.io.cache.maxDiskUsage : 50g\n",
       "spark.databricks.passthrough.adls.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\n",
       "spark.app.name : Databricks Shell\n",
       "spark.driver.allowMultipleContexts : false\n",
       "spark.rdd.compress : true\n",
       "spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException : false\n",
       "spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env : \n",
       "spark.databricks.eventLog.dir : eventlogs\n",
       "spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled : false\n",
       "spark.databricks.driverNfs.pathSuffix : .ephemeral_nfs\n",
       "spark.sql.hive.metastore.jars : builtin\n",
       "spark.speculation : false\n",
       "spark.hadoop.hive.server2.session.check.interval : 60000\n",
       "spark.sql.hive.convertCTAS : true\n",
       "spark.hadoop.spark.sql.parquet.output.committer.class : org.apache.spark.sql.parquet.DirectParquetOutputCommitter\n",
       "spark.hadoop.fs.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n",
       "spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled : true\n",
       "spark.databricks.tahoe.logStore.aws.class : com.databricks.tahoe.store.MultiClusterLogStore\n",
       "spark.hadoop.fs.s3a.fast.upload.default : true\n",
       "spark.hadoop.fs.mlflowdbfs.impl : com.databricks.mlflowdbfs.MlflowdbfsFileSystem\n",
       "spark.databricks.eventLog.listenerClassName : com.databricks.backend.daemon.driver.DBCEventLoggingListener\n",
       "spark.hadoop.fs.abfs.impl.disable.cache : true\n",
       "spark.databricks.io.cache.maxMetaDataCache : 10g\n",
       "spark.speculation.multiplier : 3\n",
       "spark.storage.blockManagerTimeoutIntervalMs : 300000\n",
       "spark.databricks.clusterUsageTags.sparkVersion : 9.1.x-scala2.12\n",
       "spark.databricks.clusterUsageTags.clusterGeneration : 323\n",
       "spark.sparkr.use.daemon : false\n",
       "spark.scheduler.listenerbus.eventqueue.capacity : 20000\n",
       "spark.databricks.clusterUsageTags.clusterResourceClass : Serverless\n",
       "spark.databricks.clusterUsageTags.clusterStateMessage : Starting Spark\n",
       "spark.hadoop.parquet.page.write-checksum.enabled : true\n",
       "spark.hadoop.databricks.s3commit.client.sslTrustAll : false\n",
       "spark.hadoop.fs.s3a.threads.max : 136\n",
       "spark.hadoop.javax.jdo.option.ConnectionURL : jdbc:sqlserver://datafound-dev-euw-sqlsrv-01.database.windows.net:1433;database=datafounddeveuwsqldbhivems;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\n",
       "spark.databricks.clusterUsageTags.clusterId : 1202-161340-zrtmi161\n",
       "spark.r.backendConnectionTimeout : 604800\n",
       "spark.ui.prometheus.enabled : true\n",
       "fs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsiss.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs : 0\n",
       "spark.hadoop.hive.server2.idle.session.timeout : 900000\n",
       "spark.databricks.redactor : com.databricks.spark.util.DatabricksSparkLogRedactorProxy\n",
       "spark.hadoop.fs.s3a.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.databricks.clusterUsageTags.clusterName : HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL\n",
       "spark.databricks.clusterUsageTags.driverNodeType : Standard_E4ds_v4\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes : 0\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace : 0\n",
       "spark.hadoop.parquet.page.verify-checksum.enabled : true\n",
       "spark.logConf : true\n",
       "spark.databricks.clusterUsageTags.enableJobsAutostart : true\n",
       "spark.hadoop.hive.server2.enable.doAs : false\n",
       "spark.shuffle.memoryFraction : 0.2\n",
       "spark.hadoop.fs.cpfs-s3a.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.databricks.secret.envVar.keys.toRedact : \n",
       "fs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsenr.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\n",
       "fs.azure.account.auth.type.datafounddeveuwdlsenr.dfs.core.windows.net : OAuth\n",
       "spark.databricks.clusterUsageTags.cloudProvider : Azure\n",
       "spark.files.useFetchCache : false\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">http://172.24.129.196:40001\nspark.databricks.preemption.enabled : true\nspark.databricks.clusterUsageTags.driverContainerId : ae4932b93ce24c0da2d5cad7243bc496\nfs.azure.account.oauth2.client.secret.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\nspark.databricks.clusterUsageTags.clusterFirstOnDemand : 1\nspark.driver.tempDirectory : /local_disk0/tmp\nspark.databricks.queryWatchdog.maxQueryTasks : 200000\nspark.databricks.managedCatalog.clientClassName : com.databricks.managedcatalog.ManagedCatalogClientImpl\nspark.databricks.clusterUsageTags.sparkImageLabel : release__9.1.x-snapshot-scala2.12__databricks-universe__9.1.61__73212e5__c999d52__jenkins__0f87a2f__format-3\nspark.hadoop.fs.fcfs-s3.impl.disable.cache : true\nspark.sql.streaming.checkpointFileManagerClass : com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\nspark.databricks.service.dbutils.repl.backend : com.databricks.dbconnect.ReplDBUtils\nspark.streaming.driver.writeAheadLog.allowBatching : true\nspark.hadoop.hive.server2.transport.mode : http\nspark.databricks.clusterUsageTags.driverInstanceId : 1b8c43f2ed3f482ba4ac1aa290277907\nfs.azure.account.oauth.provider.type.datafounddeveuwdlsiss.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.app.id : app-20240510174946-0000\nspark.hadoop.fs.cpfs-adl.impl.disable.cache : true\nspark.databricks.clusterUsageTags.hailEnabled : false\nspark.hadoop.fs.mcfs-s3.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.databricks.clusterUsageTags.containerType : LXC\nspark.eventLog.enabled : false\nspark.databricks.clusterUsageTags.isIMv2Enabled : false\nspark.databricks.cluster.profile : serverless\nspark.repl.class.outputDir : /local_disk0/tmp/repl/spark-4618732084335514242-f8147b84-3151-4f4f-b891-a1eea072ef71\nspark.hadoop.hive.hmshandler.retry.interval : 2000\nspark.executor.tempDirectory : /local_disk0/tmp\nspark.databricks.clusterUsageTags.clusterLastActivityTime : 1715359672603\nspark.databricks.secret.sparkConf.keys.toRedact : ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5xYXN0ZGV2ZXV3ZGxzZHNsMDEuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2lzcy5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNpc3MuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLnFhc3RkZXZldXdkbHNkc2wwMS5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNlbnIuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2Vuci5kZnMuY29yZS53aW5kb3dzLm5ldA==\nspark.databricks.clusterUsageTags.clusterCreator : ThirdPartyApp\nspark.hadoop.mapred.output.committer.class : com.databricks.backend.daemon.data.client.DirectOutputCommitter\nspark.databricks.clusterUsageTags.clusterTargetWorkers : 2\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version : 2\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 : 0\nspark.sql.allowMultipleContexts : false\nspark.databricks.eventLog.enabled : true\nspark.hadoop.hive.server2.thrift.http.port : 10000\nspark.home : /databricks/spark\nspark.hadoop.hive.server2.idle.operation.timeout : 7200000\nfs.azure.account.oauth2.client.endpoint.qastdeveuwdlsdsl01.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\nspark.repl.class.uri : spark://172.24.129.196:35411/classes\nspark.task.reaper.enabled : true\nspark.storage.memoryFraction : 0.5\nspark.databricks.clusterUsageTags.autoTerminationMinutes : 60\nspark.databricks.clusterUsageTags.sparkEnvVarContainsNewline : false\nspark.hadoop.fs.fcfs-s3.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.delta.multiClusterWrites.enabled : true\nspark.databricks.clusterUsageTags.driverInstancePrivateIp : 172.24.129.132\nspark.worker.cleanup.enabled : false\nspark.sql.legacy.createHiveTableByDefault : false\nspark.databricks.driver.preferredMavenCentralMirrorUrl : https://maven-central.storage-download.googleapis.com/maven2/\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File : 0\nspark.hadoop.fs.fcfs-s3a.impl.disable.cache : true\nspark.ui.port : 40001\nspark.databricks.workspace.matplotlibInline.enabled : true\nspark.databricks.clusterUsageTags.enableCredentialPassthrough : false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign : false\nspark.databricks.clusterUsageTags.clusterNodeType : Standard_E4ds_v4\nspark.databricks.clusterUsageTags.enableJdbcAutoStart : true\nspark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough : false\nspark.hadoop.fs.fcfs-s3n.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.hadoop.fs.wasb.impl.disable.cache : true\nspark.databricks.wsfsPublicPreview : true\nspark.cleaner.referenceTracking.blocking : false\nfs.azure.account.oauth2.client.id.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\nspark.databricks.clusterUsageTags.isSingleUserCluster : false\nspark.databricks.clusterUsageTags.clusterState : Pending\nspark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes : false\nspark.databricks.tahoe.logStore.azure.class : com.databricks.tahoe.store.AzureLogStore\nspark.hadoop.fs.azure.skip.metrics : true\nspark.hadoop.fs.s3.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.hive.hmshandler.retry.attempts : 10\nspark.scheduler.mode : FAIR\nspark.sql.sources.default : delta\nspark.hadoop.fs.mcfs-gs.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.hadoop.fs.cpfs-s3n.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.hadoop.fs.cpfs-adl.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.hadoop.fs.fcfs-s3n.impl.disable.cache : true\nspark.hadoop.fs.cpfs-abfss.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nfs.azure.account.oauth2.client.secret.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\ndatanucleus.autoCreateSchema : true\nfs.azure.account.oauth2.client.id.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\nspark.databricks.clusterUsageTags.clusterNumCustomTags : 1\nspark.databricks.clusterUsageTags.driverContainerPrivateIp : 172.24.129.196\nspark.databricks.passthrough.oauth.refresher.impl : com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\nspark.sql.hive.metastore.sharedPrefixes : org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version : 2.3.7\nspark.databricks.io.directoryCommit.enableLogicalDelete : false\nspark.task.reaper.killTimeout : 60s\nspark.databricks.managedCatalog.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider\nspark.hadoop.parquet.block.size.row.check.min : 10\nspark.hadoop.hive.server2.use.SSL : true\nspark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType : vnet-injection\nspark.hadoop.fs.mcfs-s3a.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.hadoop.databricks.dbfs.client.version : v2\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb : 0\nspark.databricks.repl.allowedLanguages : sql,python,r\nspark.hadoop.hive.server2.keystore.path : /databricks/keys/jetty-ssl-driver-keystore.jks\nspark.databricks.clusterUsageTags.clusterAllTags : [{&#34;key&#34;:&#34;ResourceClass&#34;,&#34;value&#34;:&#34;Serverless&#34;},{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;[REDACTED]&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;1202-161340-zrtmi161&#34;},{&#34;key&#34;:&#34;ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;UDP_Solution_BusinessOwner&#34;,&#34;value&#34;:&#34;ola.petersson@volvo.com&#34;},{&#34;key&#34;:&#34;Technical_Contact&#34;,&#34;value&#34;:&#34;maciej.poborca@consultant.volvo.com&#34;},{&#34;key&#34;:&#34;Business_Owner&#34;,&#34;value&#34;:&#34;bartlomiej.wierzbicki@volvo.com&#34;},{&#34;key&#34;:&#34;UDP_Platform_Environment&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;Assignment_Code&#34;,&#34;value&#34;:&#34;VY064I&#34;},{&#34;key&#34;:&#34;UDP_Solution_Name&#34;,&#34;value&#34;:&#34;qast&#34;},{&#34;key&#34;:&#34;InfoSeC&#34;,&#34;value&#34;:&#34;Confidential&#34;},{&#34;key&#34;:&#34;UDP_Solution_Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;UDP_Solution_ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;Environment_Type&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;PDS&#34;,&#34;value&#34;:&#34;High&#34;},{&#34;key&#34;:&#34;UDP_Solution_TechnicalContact&#34;,&#34;value&#34;:&#34;nikhil.rathore@volvo.com&#34;},{&#34;key&#34;:&#34;Mapped_ID&#34;,&#34;value&#34;:&#34;Automatic&#34;},{&#34;key&#34;:&#34;UDP_Solution_AssignmentCode&#34;,&#34;value&#34;:&#34;VY09ZT&#34;},{&#34;key&#34;:&#34;Service_Level&#34;,&#34;value&#34;:&#34;CEP_Basic&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8122829732360297&#34;}]\nspark.databricks.credential.redactor : com.databricks.logging.secrets.CredentialRedactorProxyImpl\nspark.sql.legacy.parquet.datetimeRebaseModeInRead : LEGACY\nspark.databricks.acl.provider : com.databricks.sql.acl.ReflectionBackedAclProvider\nspark.app.startTime : 1715363383441\nspark.databricks.mlflow.autologging.enabled : true\nspark.extraListeners : com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled : false\nspark.sql.parquet.cacheMetadata : true\nspark.databricks.clusterUsageTags.clusterLogDestination : dbfs:/mnt/cluster-logs\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss : 0\nspark.r.sql.derby.temp.dir : /tmp/RtmpsKdyDl\nspark.hadoop.fs.adl.impl : com.databricks.adl.AdlFileSystem\nspark.hadoop.fs.cpfs-abfss.impl.disable.cache : true\nspark.databricks.clusterUsageTags.azureSubscriptionId : 04abf2eb-58f5-4264-8858-3793629a7259\nspark.databricks.clusterUsageTags.enableLocalDiskEncryption : false\nspark.databricks.tahoe.logStore.class : com.databricks.tahoe.store.DelegatingLogStore\nfs.azure.account.oauth2.client.id.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\nlibraryDownload.sleepIntervalSeconds : 5\nspark.sql.hive.convertMetastoreParquet : true\nspark.databricks.service.dbutils.server.backend : com.databricks.dbconnect.SparkServerDBUtils\nspark.executor.id : driver\nspark.databricks.managedCatalog.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider\nspark.databricks.repl.enableClassFileCleanup : true\nspark.databricks.clusterUsageTags.clusterAvailability : SPOT_WITH_FALLBACK_AZURE\nspark.sql.catalogImplementation : hive\nspark.sql.legacy.parquet.datetimeRebaseModeInWrite : LEGACY\nspark.databricks.clusterUsageTags.region : westeurope\nspark.databricks.clusterUsageTags.instanceWorkerEnvId : workerenv-8122829732360297\nspark.hadoop.fs.s3a.multipart.size : 10485760\nspark.databricks.clusterUsageTags.effectiveSparkVersion : 9.1.x-scala2.12\nspark.metrics.conf : /databricks/spark/conf/metrics.properties\nspark.databricks.workspaceUrl : adb-8122829732360297.17.azuredatabricks.net\nspark.akka.frameSize : 256\nspark.hadoop.fs.s3a.fast.upload : true\nspark.hadoop.fs.wasbs.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.sql.streaming.stopTimeout : 15s\nspark.hadoop.hive.server2.keystore.password : [REDACTED]\nspark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting : false\nspark.executor.extraClassPath : /databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--ch.qos.reload4j--reload4j--ch.qos.reload4j__reload4j__1.2.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacecommerceanalytics--com.amazonaws__aws-java-sdk-marketplacecommerceanalytics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacemeteringservice--com.amazonaws__aws-java-sdk-marketplacemeteringservice__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.29.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.\n*** WARNING: skipped 62833 bytes of output ***\n\n\nspark.databricks.clusterUsageTags.sparkEnvVarContainsEscape : false\nspark.databricks.overrideDefaultCommitProtocol : org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\nspark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass : com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\nspark.databricks.clusterUsageTags.clusterNoDriverDaemon : false\nlibraryDownload.timeoutSeconds : 180\nspark.hadoop.parquet.memory.pool.ratio : 0.5\nspark.databricks.clusterUsageTags.clusterOwnerUserId : 8668945429017751\nspark.hadoop.javax.jdo.option.ConnectionUserName : hiveuser\nspark.databricks.sparkContextId : 4618732084335514242\nspark.databricks.clusterUsageTags.clusterScalingType : fixed_size\nspark.databricks.passthrough.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\nspark.executor.memory : 21588m\nfs.azure.account.oauth.provider.type.qastdeveuwdlsdsl01.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.databricks.tahoe.logStore.gcp.class : com.databricks.tahoe.store.GCPLogStore\nspark.serializer.objectStreamReset : 100\nspark.databricks.clusterUsageTags.sparkMasterUrlType : None\nspark.hadoop.javax.jdo.option.ConnectionDriverName : com.microsoft.sqlserver.jdbc.SQLServerDriver\nspark.sql.sources.commitProtocolClass : com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\nspark.hadoop.fs.abfss.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs : 0\nspark.hadoop.fs.fcfs-s3a.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.attribute_tag_budget : \nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeType : azure_disk_volume_type: PREMIUM_LRS\n\nspark.databricks.clusterUsageTags.clusterWorkers : 2\nspark.databricks.clusterUsageTags.clusterPythonVersion : 3\nspark.databricks.clusterUsageTags.enableDfAcls : false\nspark.databricks.cloudfetch.requestDownloadUrlsWithHeaders : true\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount : 0\nspark.shuffle.service.enabled : true\nspark.hadoop.fs.file.impl : com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\nspark.hadoop.fs.mcfs-s3n.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.hadoop.fs.cpfs-s3.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.attribute_tag_dust_maintainer : \nfs.azure.account.oauth.provider.type.datafounddeveuwdlsenr.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.hadoop.fs.s3a.multipart.threshold : 104857600\nspark.databricks.workerNodeTypeId : Standard_E4ds_v4\nspark.rpc.message.maxSize : 256\nspark.databricks.clusterUsageTags.workerEnvironmentId : workerenv-8122829732360297\nspark.databricks.clusterUsageTags.attribute_tag_dust_suite : \nspark.databricks.driverNfs.enabled : true\nspark.databricks.clusterUsageTags.clusterMetastoreAccessType : RDS_DIRECT\nspark.databricks.passthrough.glue.executorServiceFactoryClassName : com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\nspark.hadoop.javax.jdo.option.ConnectionPassword : P5fm4wpj6fm4u4ebrag23o2lqwk2qyahju4tcg34x!\nspark.hadoop.fs.abfs.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\nspark.master : spark://172.24.129.196:7077\nspark.databricks.clusterUsageTags.enableElasticDisk : true\nspark.databricks.acl.scim.client : com.databricks.spark.sql.acl.client.DriverToWebappScimClient\nspark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick : false\nspark.hadoop.fs.adl.impl.disable.cache : true\nspark.hadoop.parquet.block.size.row.check.max : 10\nspark.hadoop.fs.s3a.connection.maximum : 200\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 : 0\nspark.hadoop.fs.s3a.fast.upload.active.blocks : 32\nspark.shuffle.reduceLocality.enabled : false\nspark.hadoop.spark.sql.sources.outputCommitterClass : com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\nspark.hadoop.fs.AbstractFileSystem.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nspark.hadoop.hive.server2.thrift.http.cookie.auth.enabled : false\nspark.sql.parquet.compression.codec : snappy\nspark.databricks.cloudProvider : Azure\nspark.driver.maxResultSize : 0\nfs.azure.account.oauth2.client.secret.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\nspark.executor.extraJavaOptions : -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\nspark.databricks.cloudfetch.hasRegionSupport : true\nspark.databricks.clusterUsageTags.ngrokNpipEnabled : true\nspark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories : false\nspark.hadoop.fs.wasb.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.hadoop.fs.mcfs-abfss.impl.disable.cache : true\nspark.databricks.passthrough.glue.credentialsProviderFactoryClassName : com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\nspark.databricks.clusterUsageTags.clusterSpotBidMaxPrice : -1.0\nspark.sql.warehouse.dir : /user/hive/warehouse\nspark.databricks.passthrough.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\nspark.databricks.session.share : false\nspark.databricks.clusterUsageTags.userId : 8668945429017751\nspark.databricks.isShieldWorkspace : false\nspark.databricks.telemetry.prometheus.samplingRate : 100\nspark.driver.host : 172.24.129.196\nspark.databricks.clusterUsageTags.clusterSku : STANDARD_SKU\nspark.hadoop.fs.gs.impl.disable.cache : true\nspark.databricks.clusterUsageTags.managedResourceGroup : databricks-rg-udp-qast-dev-euw-dbw-lxni2stdfcvd6\nspark.databricks.privateLinkEnabled : false\nspark.databricks.clusterUsageTags.clusterUnityCatalogMode : CUSTOM\nspark.databricks.io.cache.enabled : true\nspark.databricks.managedCatalog.gcs.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider\nfs.azure.account.auth.type.datafounddeveuwdlsiss.dfs.core.windows.net : OAuth\nspark.databricks.automl.serviceEnabled : true\nspark.hadoop.parquet.page.size.check.estimate : false\nspark.hadoop.spark.driverproxy.customHeadersToProperties : X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name\nspark.databricks.passthrough.s3a.threadPoolExecutor.factory.class : com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\nspark.databricks.clusterUsageTags.attribute_tag_service : \nspark.databricks.delta.preview.enabled : true\nspark.databricks.clusterSource : API\nspark.databricks.metrics.filesystem_io_metrics : true\nspark.databricks.clusterUsageTags.dataPlaneRegion : westeurope\nspark.databricks.cloudfetch.requesterClassName : com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\nspark.databricks.delta.logStore.crossCloud.fatal : true\nspark.files.fetchFailure.unRegisterOutputOnHost : true\nspark.databricks.clusterUsageTags.enableSqlAclsOnly : false\nspark.databricks.clusterUsageTags.clusterNumSshKeys : 0\nspark.databricks.clusterUsageTags.clusterSizeType : VM_CONTAINER\nspark.hadoop.databricks.fs.perfMetrics.enable : true\nspark.databricks.clusterUsageTags.clusterPinned : true\nspark.hadoop.fs.gs.outputstream.upload.chunk.size : 16777216\nspark.speculation.quantile : 0.9\nspark.databricks.clusterUsageTags.privateLinkEnabled : false\nspark.shuffle.manager : SORT\nspark.files.overwrite : true\nspark.databricks.clusterUsageTags.userProvidedSparkVersion : 9.1.x-scala2.12\nspark.databricks.driverNodeTypeId : Standard_E4ds_v4\ndatanucleus.fixedDatastore : false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes : false\nspark.r.numRBackendThreads : 1\nspark.hadoop.fs.wasbs.impl.disable.cache : true\nspark.hadoop.fs.abfss.impl.disable.cache : true\nspark.databricks.workspace.multipleResults.enabled : true\nspark.shuffle.service.port : 4048\nspark.databricks.clusterUsageTags.clusterOwnerOrgId : 8122829732360297\nspark.databricks.acl.client : com.databricks.spark.sql.acl.client.SparkSqlAclClient\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite : true\nspark.hadoop.hive.warehouse.subdir.inherit.perms : false\nfs.azure.account.auth.type.qastdeveuwdlsdsl01.dfs.core.windows.net : OAuth\nspark.hadoop.fs.mcfs-abfss.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.databricks.clusterUsageTags.runtimeEngine : STANDARD\nspark.databricks.clusterUsageTags.isServicePrincipalCluster : false\nspark.hadoop.fs.s3n.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.databricks.clusterUsageTags.orgId : 8122829732360297\nspark.databricks.enablePublicDbfsFuse : false\nspark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 : 1\nspark.driver.port : 35411\nspark.databricks.io.cache.maxDiskUsage : 50g\nspark.databricks.passthrough.adls.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\nspark.app.name : Databricks Shell\nspark.driver.allowMultipleContexts : false\nspark.rdd.compress : true\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException : false\nspark.databricks.clusterUsageTags.attribute_tag_dust_execution_env : \nspark.databricks.eventLog.dir : eventlogs\nspark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled : false\nspark.databricks.driverNfs.pathSuffix : .ephemeral_nfs\nspark.sql.hive.metastore.jars : builtin\nspark.speculation : false\nspark.hadoop.hive.server2.session.check.interval : 60000\nspark.sql.hive.convertCTAS : true\nspark.hadoop.spark.sql.parquet.output.committer.class : org.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.hadoop.fs.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\nspark.databricks.clusterUsageTags.clusterLogDeliveryEnabled : true\nspark.databricks.tahoe.logStore.aws.class : com.databricks.tahoe.store.MultiClusterLogStore\nspark.hadoop.fs.s3a.fast.upload.default : true\nspark.hadoop.fs.mlflowdbfs.impl : com.databricks.mlflowdbfs.MlflowdbfsFileSystem\nspark.databricks.eventLog.listenerClassName : com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.fs.abfs.impl.disable.cache : true\nspark.databricks.io.cache.maxMetaDataCache : 10g\nspark.speculation.multiplier : 3\nspark.storage.blockManagerTimeoutIntervalMs : 300000\nspark.databricks.clusterUsageTags.sparkVersion : 9.1.x-scala2.12\nspark.databricks.clusterUsageTags.clusterGeneration : 323\nspark.sparkr.use.daemon : false\nspark.scheduler.listenerbus.eventqueue.capacity : 20000\nspark.databricks.clusterUsageTags.clusterResourceClass : Serverless\nspark.databricks.clusterUsageTags.clusterStateMessage : Starting Spark\nspark.hadoop.parquet.page.write-checksum.enabled : true\nspark.hadoop.databricks.s3commit.client.sslTrustAll : false\nspark.hadoop.fs.s3a.threads.max : 136\nspark.hadoop.javax.jdo.option.ConnectionURL : jdbc:sqlserver://datafound-dev-euw-sqlsrv-01.database.windows.net:1433;database=datafounddeveuwsqldbhivems;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\nspark.databricks.clusterUsageTags.clusterId : 1202-161340-zrtmi161\nspark.r.backendConnectionTimeout : 604800\nspark.ui.prometheus.enabled : true\nfs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsiss.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs : 0\nspark.hadoop.hive.server2.idle.session.timeout : 900000\nspark.databricks.redactor : com.databricks.spark.util.DatabricksSparkLogRedactorProxy\nspark.hadoop.fs.s3a.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.databricks.clusterUsageTags.clusterName : HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL\nspark.databricks.clusterUsageTags.driverNodeType : Standard_E4ds_v4\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes : 0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace : 0\nspark.hadoop.parquet.page.verify-checksum.enabled : true\nspark.logConf : true\nspark.databricks.clusterUsageTags.enableJobsAutostart : true\nspark.hadoop.hive.server2.enable.doAs : false\nspark.shuffle.memoryFraction : 0.2\nspark.hadoop.fs.cpfs-s3a.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.secret.envVar.keys.toRedact : \nfs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsenr.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\nfs.azure.account.auth.type.datafounddeveuwdlsenr.dfs.core.windows.net : OAuth\nspark.databricks.clusterUsageTags.cloudProvider : Azure\nspark.files.useFetchCache : false\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Cluster Details\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "# Print all configurations\n",
    "for k,v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b05cdd-ac51-4341-afcc-b6b37d1625c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#59. How to restrict the PySpark to use the number of cores in the system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc1c60d-793e-47f2-8581-99b2df9ea07a",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">http://172.24.129.196:40001\n",
       "spark.databricks.preemption.enabled : true\n",
       "spark.databricks.clusterUsageTags.driverContainerId : ae4932b93ce24c0da2d5cad7243bc496\n",
       "fs.azure.account.oauth2.client.secret.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.clusterFirstOnDemand : 1\n",
       "spark.driver.tempDirectory : /local_disk0/tmp\n",
       "spark.databricks.queryWatchdog.maxQueryTasks : 200000\n",
       "spark.databricks.managedCatalog.clientClassName : com.databricks.managedcatalog.ManagedCatalogClientImpl\n",
       "spark.databricks.clusterUsageTags.sparkImageLabel : release__9.1.x-snapshot-scala2.12__databricks-universe__9.1.61__73212e5__c999d52__jenkins__0f87a2f__format-3\n",
       "spark.hadoop.fs.fcfs-s3.impl.disable.cache : true\n",
       "spark.sql.streaming.checkpointFileManagerClass : com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\n",
       "spark.databricks.service.dbutils.repl.backend : com.databricks.dbconnect.ReplDBUtils\n",
       "spark.streaming.driver.writeAheadLog.allowBatching : true\n",
       "spark.hadoop.hive.server2.transport.mode : http\n",
       "spark.databricks.clusterUsageTags.driverInstanceId : 1b8c43f2ed3f482ba4ac1aa290277907\n",
       "fs.azure.account.oauth.provider.type.datafounddeveuwdlsiss.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
       "spark.app.id : app-20240510174946-0000\n",
       "spark.hadoop.fs.cpfs-adl.impl.disable.cache : true\n",
       "spark.databricks.clusterUsageTags.hailEnabled : false\n",
       "spark.hadoop.fs.mcfs-s3.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.databricks.clusterUsageTags.containerType : LXC\n",
       "spark.eventLog.enabled : false\n",
       "spark.databricks.clusterUsageTags.isIMv2Enabled : false\n",
       "spark.databricks.cluster.profile : serverless\n",
       "spark.repl.class.outputDir : /local_disk0/tmp/repl/spark-4618732084335514242-f8147b84-3151-4f4f-b891-a1eea072ef71\n",
       "spark.hadoop.hive.hmshandler.retry.interval : 2000\n",
       "spark.executor.tempDirectory : /local_disk0/tmp\n",
       "spark.databricks.clusterUsageTags.clusterLastActivityTime : 1715359672603\n",
       "spark.databricks.secret.sparkConf.keys.toRedact : ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5xYXN0ZGV2ZXV3ZGxzZHNsMDEuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2lzcy5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNpc3MuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLnFhc3RkZXZldXdkbHNkc2wwMS5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNlbnIuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2Vuci5kZnMuY29yZS53aW5kb3dzLm5ldA==\n",
       "spark.databricks.clusterUsageTags.clusterCreator : ThirdPartyApp\n",
       "spark.hadoop.mapred.output.committer.class : com.databricks.backend.daemon.data.client.DirectOutputCommitter\n",
       "spark.databricks.clusterUsageTags.clusterTargetWorkers : 2\n",
       "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version : 2\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 : 0\n",
       "spark.sql.allowMultipleContexts : false\n",
       "spark.databricks.eventLog.enabled : true\n",
       "spark.hadoop.hive.server2.thrift.http.port : 10000\n",
       "spark.home : /databricks/spark\n",
       "spark.hadoop.hive.server2.idle.operation.timeout : 7200000\n",
       "fs.azure.account.oauth2.client.endpoint.qastdeveuwdlsdsl01.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\n",
       "spark.repl.class.uri : spark://172.24.129.196:35411/classes\n",
       "spark.task.reaper.enabled : true\n",
       "spark.storage.memoryFraction : 0.5\n",
       "spark.databricks.clusterUsageTags.autoTerminationMinutes : 60\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline : false\n",
       "spark.hadoop.fs.fcfs-s3.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
       "spark.databricks.delta.multiClusterWrites.enabled : true\n",
       "spark.databricks.clusterUsageTags.driverInstancePrivateIp : 172.24.129.132\n",
       "spark.worker.cleanup.enabled : false\n",
       "spark.sql.legacy.createHiveTableByDefault : false\n",
       "spark.databricks.driver.preferredMavenCentralMirrorUrl : https://maven-central.storage-download.googleapis.com/maven2/\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File : 0\n",
       "spark.hadoop.fs.fcfs-s3a.impl.disable.cache : true\n",
       "spark.ui.port : 40001\n",
       "spark.databricks.workspace.matplotlibInline.enabled : true\n",
       "spark.databricks.clusterUsageTags.enableCredentialPassthrough : false\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign : false\n",
       "spark.databricks.clusterUsageTags.clusterNodeType : Standard_E4ds_v4\n",
       "spark.databricks.clusterUsageTags.enableJdbcAutoStart : true\n",
       "spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough : false\n",
       "spark.hadoop.fs.fcfs-s3n.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
       "spark.hadoop.fs.wasb.impl.disable.cache : true\n",
       "spark.databricks.wsfsPublicPreview : true\n",
       "spark.cleaner.referenceTracking.blocking : false\n",
       "fs.azure.account.oauth2.client.id.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.isSingleUserCluster : false\n",
       "spark.databricks.clusterUsageTags.clusterState : Pending\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes : false\n",
       "spark.databricks.tahoe.logStore.azure.class : com.databricks.tahoe.store.AzureLogStore\n",
       "spark.hadoop.fs.azure.skip.metrics : true\n",
       "spark.hadoop.fs.s3.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.hadoop.hive.hmshandler.retry.attempts : 10\n",
       "spark.scheduler.mode : FAIR\n",
       "spark.sql.sources.default : delta\n",
       "spark.hadoop.fs.mcfs-gs.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.hadoop.fs.cpfs-s3n.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.hadoop.fs.cpfs-adl.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.hadoop.fs.fcfs-s3n.impl.disable.cache : true\n",
       "spark.hadoop.fs.cpfs-abfss.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "fs.azure.account.oauth2.client.secret.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\n",
       "datanucleus.autoCreateSchema : true\n",
       "fs.azure.account.oauth2.client.id.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.clusterNumCustomTags : 1\n",
       "spark.databricks.clusterUsageTags.driverContainerPrivateIp : 172.24.129.196\n",
       "spark.databricks.passthrough.oauth.refresher.impl : com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\n",
       "spark.sql.hive.metastore.sharedPrefixes : org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\n",
       "spark.sql.hive.metastore.version : 2.3.7\n",
       "spark.databricks.io.directoryCommit.enableLogicalDelete : false\n",
       "spark.task.reaper.killTimeout : 60s\n",
       "spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider\n",
       "spark.hadoop.parquet.block.size.row.check.min : 10\n",
       "spark.hadoop.hive.server2.use.SSL : true\n",
       "spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType : vnet-injection\n",
       "spark.hadoop.fs.mcfs-s3a.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.hadoop.databricks.dbfs.client.version : v2\n",
       "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb : 0\n",
       "spark.databricks.repl.allowedLanguages : sql,python,r\n",
       "spark.hadoop.hive.server2.keystore.path : /databricks/keys/jetty-ssl-driver-keystore.jks\n",
       "spark.databricks.clusterUsageTags.clusterAllTags : [{&#34;key&#34;:&#34;ResourceClass&#34;,&#34;value&#34;:&#34;Serverless&#34;},{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;[REDACTED]&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;1202-161340-zrtmi161&#34;},{&#34;key&#34;:&#34;ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;UDP_Solution_BusinessOwner&#34;,&#34;value&#34;:&#34;ola.petersson@volvo.com&#34;},{&#34;key&#34;:&#34;Technical_Contact&#34;,&#34;value&#34;:&#34;maciej.poborca@consultant.volvo.com&#34;},{&#34;key&#34;:&#34;Business_Owner&#34;,&#34;value&#34;:&#34;bartlomiej.wierzbicki@volvo.com&#34;},{&#34;key&#34;:&#34;UDP_Platform_Environment&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;Assignment_Code&#34;,&#34;value&#34;:&#34;VY064I&#34;},{&#34;key&#34;:&#34;UDP_Solution_Name&#34;,&#34;value&#34;:&#34;qast&#34;},{&#34;key&#34;:&#34;InfoSeC&#34;,&#34;value&#34;:&#34;Confidential&#34;},{&#34;key&#34;:&#34;UDP_Solution_Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;UDP_Solution_ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;Environment_Type&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;PDS&#34;,&#34;value&#34;:&#34;High&#34;},{&#34;key&#34;:&#34;UDP_Solution_TechnicalContact&#34;,&#34;value&#34;:&#34;nikhil.rathore@volvo.com&#34;},{&#34;key&#34;:&#34;Mapped_ID&#34;,&#34;value&#34;:&#34;Automatic&#34;},{&#34;key&#34;:&#34;UDP_Solution_AssignmentCode&#34;,&#34;value&#34;:&#34;VY09ZT&#34;},{&#34;key&#34;:&#34;Service_Level&#34;,&#34;value&#34;:&#34;CEP_Basic&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8122829732360297&#34;}]\n",
       "spark.databricks.credential.redactor : com.databricks.logging.secrets.CredentialRedactorProxyImpl\n",
       "spark.sql.legacy.parquet.datetimeRebaseModeInRead : LEGACY\n",
       "spark.databricks.acl.provider : com.databricks.sql.acl.ReflectionBackedAclProvider\n",
       "spark.app.startTime : 1715363383441\n",
       "spark.databricks.mlflow.autologging.enabled : true\n",
       "spark.extraListeners : com.databricks.backend.daemon.driver.DBCEventLoggingListener\n",
       "spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled : false\n",
       "spark.sql.parquet.cacheMetadata : true\n",
       "spark.databricks.clusterUsageTags.clusterLogDestination : dbfs:/mnt/cluster-logs\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss : 0\n",
       "spark.r.sql.derby.temp.dir : /tmp/RtmpsKdyDl\n",
       "spark.hadoop.fs.adl.impl : com.databricks.adl.AdlFileSystem\n",
       "spark.hadoop.fs.cpfs-abfss.impl.disable.cache : true\n",
       "spark.databricks.clusterUsageTags.azureSubscriptionId : 04abf2eb-58f5-4264-8858-3793629a7259\n",
       "spark.databricks.clusterUsageTags.enableLocalDiskEncryption : false\n",
       "spark.databricks.tahoe.logStore.class : com.databricks.tahoe.store.DelegatingLogStore\n",
       "fs.azure.account.oauth2.client.id.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\n",
       "libraryDownload.sleepIntervalSeconds : 5\n",
       "spark.sql.hive.convertMetastoreParquet : true\n",
       "spark.databricks.service.dbutils.server.backend : com.databricks.dbconnect.SparkServerDBUtils\n",
       "spark.executor.id : driver\n",
       "spark.databricks.managedCatalog.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider\n",
       "spark.databricks.repl.enableClassFileCleanup : true\n",
       "spark.databricks.clusterUsageTags.clusterAvailability : SPOT_WITH_FALLBACK_AZURE\n",
       "spark.sql.catalogImplementation : hive\n",
       "spark.sql.legacy.parquet.datetimeRebaseModeInWrite : LEGACY\n",
       "spark.databricks.clusterUsageTags.region : westeurope\n",
       "spark.databricks.clusterUsageTags.instanceWorkerEnvId : workerenv-8122829732360297\n",
       "spark.hadoop.fs.s3a.multipart.size : 10485760\n",
       "spark.databricks.clusterUsageTags.effectiveSparkVersion : 9.1.x-scala2.12\n",
       "spark.metrics.conf : /databricks/spark/conf/metrics.properties\n",
       "spark.databricks.workspaceUrl : adb-8122829732360297.17.azuredatabricks.net\n",
       "spark.akka.frameSize : 256\n",
       "spark.hadoop.fs.s3a.fast.upload : true\n",
       "spark.hadoop.fs.wasbs.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\n",
       "spark.sql.streaming.stopTimeout : 15s\n",
       "spark.hadoop.hive.server2.keystore.password : [REDACTED]\n",
       "spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting : false\n",
       "spark.executor.extraClassPath : /databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--ch.qos.reload4j--reload4j--ch.qos.reload4j__reload4j__1.2.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacecommerceanalytics--com.amazonaws__aws-java-sdk-marketplacecommerceanalytics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacemeteringservice--com.amazonaws__aws-java-sdk-marketplacemeteringservice__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.29.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.\n",
       "*** WARNING: skipped 62833 bytes of output ***\n",
       "\n",
       "\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape : false\n",
       "spark.databricks.overrideDefaultCommitProtocol : org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\n",
       "spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass : com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\n",
       "spark.databricks.clusterUsageTags.clusterNoDriverDaemon : false\n",
       "libraryDownload.timeoutSeconds : 180\n",
       "spark.hadoop.parquet.memory.pool.ratio : 0.5\n",
       "spark.databricks.clusterUsageTags.clusterOwnerUserId : 8668945429017751\n",
       "spark.hadoop.javax.jdo.option.ConnectionUserName : hiveuser\n",
       "spark.databricks.sparkContextId : 4618732084335514242\n",
       "spark.databricks.clusterUsageTags.clusterScalingType : fixed_size\n",
       "spark.databricks.passthrough.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\n",
       "spark.executor.memory : 21588m\n",
       "fs.azure.account.oauth.provider.type.qastdeveuwdlsdsl01.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
       "spark.databricks.tahoe.logStore.gcp.class : com.databricks.tahoe.store.GCPLogStore\n",
       "spark.serializer.objectStreamReset : 100\n",
       "spark.databricks.clusterUsageTags.sparkMasterUrlType : None\n",
       "spark.hadoop.javax.jdo.option.ConnectionDriverName : com.microsoft.sqlserver.jdbc.SQLServerDriver\n",
       "spark.sql.sources.commitProtocolClass : com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\n",
       "spark.hadoop.fs.abfss.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs : 0\n",
       "spark.hadoop.fs.fcfs-s3a.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
       "spark.databricks.clusterUsageTags.attribute_tag_budget : \n",
       "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType : azure_disk_volume_type: PREMIUM_LRS\n",
       "\n",
       "spark.databricks.clusterUsageTags.clusterWorkers : 2\n",
       "spark.databricks.clusterUsageTags.clusterPythonVersion : 3\n",
       "spark.databricks.clusterUsageTags.enableDfAcls : false\n",
       "spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders : true\n",
       "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount : 0\n",
       "spark.shuffle.service.enabled : true\n",
       "spark.hadoop.fs.file.impl : com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\n",
       "spark.hadoop.fs.mcfs-s3n.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.hadoop.fs.cpfs-s3.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer : \n",
       "fs.azure.account.oauth.provider.type.datafounddeveuwdlsenr.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
       "spark.hadoop.fs.s3a.multipart.threshold : 104857600\n",
       "spark.databricks.workerNodeTypeId : Standard_E4ds_v4\n",
       "spark.rpc.message.maxSize : 256\n",
       "spark.databricks.clusterUsageTags.workerEnvironmentId : workerenv-8122829732360297\n",
       "spark.databricks.clusterUsageTags.attribute_tag_dust_suite : \n",
       "spark.databricks.driverNfs.enabled : true\n",
       "spark.databricks.clusterUsageTags.clusterMetastoreAccessType : RDS_DIRECT\n",
       "spark.databricks.passthrough.glue.executorServiceFactoryClassName : com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\n",
       "spark.hadoop.javax.jdo.option.ConnectionPassword : P5fm4wpj6fm4u4ebrag23o2lqwk2qyahju4tcg34x!\n",
       "spark.hadoop.fs.abfs.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\n",
       "spark.master : spark://172.24.129.196:7077\n",
       "spark.databricks.clusterUsageTags.enableElasticDisk : true\n",
       "spark.databricks.acl.scim.client : com.databricks.spark.sql.acl.client.DriverToWebappScimClient\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick : false\n",
       "spark.hadoop.fs.adl.impl.disable.cache : true\n",
       "spark.hadoop.parquet.block.size.row.check.max : 10\n",
       "spark.hadoop.fs.s3a.connection.maximum : 200\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 : 0\n",
       "spark.hadoop.fs.s3a.fast.upload.active.blocks : 32\n",
       "spark.shuffle.reduceLocality.enabled : false\n",
       "spark.hadoop.spark.sql.sources.outputCommitterClass : com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\n",
       "spark.hadoop.fs.AbstractFileSystem.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\n",
       "spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled : false\n",
       "spark.sql.parquet.compression.codec : snappy\n",
       "spark.databricks.cloudProvider : Azure\n",
       "spark.driver.maxResultSize : 0\n",
       "fs.azure.account.oauth2.client.secret.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\n",
       "spark.executor.extraJavaOptions : -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\n",
       "spark.databricks.cloudfetch.hasRegionSupport : true\n",
       "spark.databricks.clusterUsageTags.ngrokNpipEnabled : true\n",
       "spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories : false\n",
       "spark.hadoop.fs.wasb.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\n",
       "spark.hadoop.fs.mcfs-abfss.impl.disable.cache : true\n",
       "spark.databricks.passthrough.glue.credentialsProviderFactoryClassName : com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\n",
       "spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice : -1.0\n",
       "spark.sql.warehouse.dir : /user/hive/warehouse\n",
       "spark.databricks.passthrough.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\n",
       "spark.databricks.session.share : false\n",
       "spark.databricks.clusterUsageTags.userId : 8668945429017751\n",
       "spark.databricks.isShieldWorkspace : false\n",
       "spark.databricks.telemetry.prometheus.samplingRate : 100\n",
       "spark.driver.host : 172.24.129.196\n",
       "spark.databricks.clusterUsageTags.clusterSku : STANDARD_SKU\n",
       "spark.hadoop.fs.gs.impl.disable.cache : true\n",
       "spark.databricks.clusterUsageTags.managedResourceGroup : databricks-rg-udp-qast-dev-euw-dbw-lxni2stdfcvd6\n",
       "spark.databricks.privateLinkEnabled : false\n",
       "spark.databricks.clusterUsageTags.clusterUnityCatalogMode : CUSTOM\n",
       "spark.databricks.io.cache.enabled : true\n",
       "spark.databricks.managedCatalog.gcs.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider\n",
       "fs.azure.account.auth.type.datafounddeveuwdlsiss.dfs.core.windows.net : OAuth\n",
       "spark.databricks.automl.serviceEnabled : true\n",
       "spark.hadoop.parquet.page.size.check.estimate : false\n",
       "spark.hadoop.spark.driverproxy.customHeadersToProperties : X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name\n",
       "spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class : com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\n",
       "spark.databricks.clusterUsageTags.attribute_tag_service : \n",
       "spark.databricks.delta.preview.enabled : true\n",
       "spark.databricks.clusterSource : API\n",
       "spark.databricks.metrics.filesystem_io_metrics : true\n",
       "spark.databricks.clusterUsageTags.dataPlaneRegion : westeurope\n",
       "spark.databricks.cloudfetch.requesterClassName : com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\n",
       "spark.databricks.delta.logStore.crossCloud.fatal : true\n",
       "spark.files.fetchFailure.unRegisterOutputOnHost : true\n",
       "spark.databricks.clusterUsageTags.enableSqlAclsOnly : false\n",
       "spark.databricks.clusterUsageTags.clusterNumSshKeys : 0\n",
       "spark.databricks.clusterUsageTags.clusterSizeType : VM_CONTAINER\n",
       "spark.hadoop.databricks.fs.perfMetrics.enable : true\n",
       "spark.databricks.clusterUsageTags.clusterPinned : true\n",
       "spark.hadoop.fs.gs.outputstream.upload.chunk.size : 16777216\n",
       "spark.speculation.quantile : 0.9\n",
       "spark.databricks.clusterUsageTags.privateLinkEnabled : false\n",
       "spark.shuffle.manager : SORT\n",
       "spark.files.overwrite : true\n",
       "spark.databricks.clusterUsageTags.userProvidedSparkVersion : 9.1.x-scala2.12\n",
       "spark.databricks.driverNodeTypeId : Standard_E4ds_v4\n",
       "datanucleus.fixedDatastore : false\n",
       "spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes : false\n",
       "spark.r.numRBackendThreads : 1\n",
       "spark.hadoop.fs.wasbs.impl.disable.cache : true\n",
       "spark.hadoop.fs.abfss.impl.disable.cache : true\n",
       "spark.databricks.workspace.multipleResults.enabled : true\n",
       "spark.shuffle.service.port : 4048\n",
       "spark.databricks.clusterUsageTags.clusterOwnerOrgId : 8122829732360297\n",
       "spark.databricks.acl.client : com.databricks.spark.sql.acl.client.SparkSqlAclClient\n",
       "spark.streaming.driver.writeAheadLog.closeFileAfterWrite : true\n",
       "spark.hadoop.hive.warehouse.subdir.inherit.perms : false\n",
       "fs.azure.account.auth.type.qastdeveuwdlsdsl01.dfs.core.windows.net : OAuth\n",
       "spark.hadoop.fs.mcfs-abfss.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\n",
       "spark.databricks.clusterUsageTags.runtimeEngine : STANDARD\n",
       "spark.databricks.clusterUsageTags.isServicePrincipalCluster : false\n",
       "spark.hadoop.fs.s3n.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.databricks.clusterUsageTags.orgId : 8122829732360297\n",
       "spark.databricks.enablePublicDbfsFuse : false\n",
       "spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 : 1\n",
       "spark.driver.port : 35411\n",
       "spark.databricks.io.cache.maxDiskUsage : 50g\n",
       "spark.databricks.passthrough.adls.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\n",
       "spark.app.name : Databricks Shell\n",
       "spark.driver.allowMultipleContexts : false\n",
       "spark.rdd.compress : true\n",
       "spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException : false\n",
       "spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env : \n",
       "spark.databricks.eventLog.dir : eventlogs\n",
       "spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled : false\n",
       "spark.databricks.driverNfs.pathSuffix : .ephemeral_nfs\n",
       "spark.sql.hive.metastore.jars : builtin\n",
       "spark.speculation : false\n",
       "spark.hadoop.hive.server2.session.check.interval : 60000\n",
       "spark.sql.hive.convertCTAS : true\n",
       "spark.hadoop.spark.sql.parquet.output.committer.class : org.apache.spark.sql.parquet.DirectParquetOutputCommitter\n",
       "spark.hadoop.fs.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n",
       "spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled : true\n",
       "spark.databricks.tahoe.logStore.aws.class : com.databricks.tahoe.store.MultiClusterLogStore\n",
       "spark.hadoop.fs.s3a.fast.upload.default : true\n",
       "spark.hadoop.fs.mlflowdbfs.impl : com.databricks.mlflowdbfs.MlflowdbfsFileSystem\n",
       "spark.databricks.eventLog.listenerClassName : com.databricks.backend.daemon.driver.DBCEventLoggingListener\n",
       "spark.hadoop.fs.abfs.impl.disable.cache : true\n",
       "spark.databricks.io.cache.maxMetaDataCache : 10g\n",
       "spark.speculation.multiplier : 3\n",
       "spark.storage.blockManagerTimeoutIntervalMs : 300000\n",
       "spark.databricks.clusterUsageTags.sparkVersion : 9.1.x-scala2.12\n",
       "spark.databricks.clusterUsageTags.clusterGeneration : 323\n",
       "spark.sparkr.use.daemon : false\n",
       "spark.scheduler.listenerbus.eventqueue.capacity : 20000\n",
       "spark.databricks.clusterUsageTags.clusterResourceClass : Serverless\n",
       "spark.databricks.clusterUsageTags.clusterStateMessage : Starting Spark\n",
       "spark.hadoop.parquet.page.write-checksum.enabled : true\n",
       "spark.hadoop.databricks.s3commit.client.sslTrustAll : false\n",
       "spark.hadoop.fs.s3a.threads.max : 136\n",
       "spark.hadoop.javax.jdo.option.ConnectionURL : jdbc:sqlserver://datafound-dev-euw-sqlsrv-01.database.windows.net:1433;database=datafounddeveuwsqldbhivems;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\n",
       "spark.databricks.clusterUsageTags.clusterId : 1202-161340-zrtmi161\n",
       "spark.r.backendConnectionTimeout : 604800\n",
       "spark.ui.prometheus.enabled : true\n",
       "fs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsiss.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs : 0\n",
       "spark.hadoop.hive.server2.idle.session.timeout : 900000\n",
       "spark.databricks.redactor : com.databricks.spark.util.DatabricksSparkLogRedactorProxy\n",
       "spark.hadoop.fs.s3a.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\n",
       "spark.databricks.clusterUsageTags.clusterName : HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL\n",
       "spark.databricks.clusterUsageTags.driverNodeType : Standard_E4ds_v4\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes : 0\n",
       "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace : 0\n",
       "spark.hadoop.parquet.page.verify-checksum.enabled : true\n",
       "spark.logConf : true\n",
       "spark.databricks.clusterUsageTags.enableJobsAutostart : true\n",
       "spark.hadoop.hive.server2.enable.doAs : false\n",
       "spark.shuffle.memoryFraction : 0.2\n",
       "spark.hadoop.fs.cpfs-s3a.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
       "spark.databricks.secret.envVar.keys.toRedact : \n",
       "fs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsenr.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\n",
       "fs.azure.account.auth.type.datafounddeveuwdlsenr.dfs.core.windows.net : OAuth\n",
       "spark.databricks.clusterUsageTags.cloudProvider : Azure\n",
       "spark.files.useFetchCache : false\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">http://172.24.129.196:40001\nspark.databricks.preemption.enabled : true\nspark.databricks.clusterUsageTags.driverContainerId : ae4932b93ce24c0da2d5cad7243bc496\nfs.azure.account.oauth2.client.secret.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\nspark.databricks.clusterUsageTags.clusterFirstOnDemand : 1\nspark.driver.tempDirectory : /local_disk0/tmp\nspark.databricks.queryWatchdog.maxQueryTasks : 200000\nspark.databricks.managedCatalog.clientClassName : com.databricks.managedcatalog.ManagedCatalogClientImpl\nspark.databricks.clusterUsageTags.sparkImageLabel : release__9.1.x-snapshot-scala2.12__databricks-universe__9.1.61__73212e5__c999d52__jenkins__0f87a2f__format-3\nspark.hadoop.fs.fcfs-s3.impl.disable.cache : true\nspark.sql.streaming.checkpointFileManagerClass : com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\nspark.databricks.service.dbutils.repl.backend : com.databricks.dbconnect.ReplDBUtils\nspark.streaming.driver.writeAheadLog.allowBatching : true\nspark.hadoop.hive.server2.transport.mode : http\nspark.databricks.clusterUsageTags.driverInstanceId : 1b8c43f2ed3f482ba4ac1aa290277907\nfs.azure.account.oauth.provider.type.datafounddeveuwdlsiss.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.app.id : app-20240510174946-0000\nspark.hadoop.fs.cpfs-adl.impl.disable.cache : true\nspark.databricks.clusterUsageTags.hailEnabled : false\nspark.hadoop.fs.mcfs-s3.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.databricks.clusterUsageTags.containerType : LXC\nspark.eventLog.enabled : false\nspark.databricks.clusterUsageTags.isIMv2Enabled : false\nspark.databricks.cluster.profile : serverless\nspark.repl.class.outputDir : /local_disk0/tmp/repl/spark-4618732084335514242-f8147b84-3151-4f4f-b891-a1eea072ef71\nspark.hadoop.hive.hmshandler.retry.interval : 2000\nspark.executor.tempDirectory : /local_disk0/tmp\nspark.databricks.clusterUsageTags.clusterLastActivityTime : 1715359672603\nspark.databricks.secret.sparkConf.keys.toRedact : ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5xYXN0ZGV2ZXV3ZGxzZHNsMDEuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2lzcy5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNpc3MuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLnFhc3RkZXZldXdkbHNkc2wwMS5kZnMuY29yZS53aW5kb3dzLm5ldA==,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LnNlY3JldC5kYXRhZm91bmRkZXZldXdkbHNlbnIuZGZzLmNvcmUud2luZG93cy5uZXQ=,ZnMuYXp1cmUuYWNjb3VudC5vYXV0aDIuY2xpZW50LmlkLmRhdGFmb3VuZGRldmV1d2Rsc2Vuci5kZnMuY29yZS53aW5kb3dzLm5ldA==\nspark.databricks.clusterUsageTags.clusterCreator : ThirdPartyApp\nspark.hadoop.mapred.output.committer.class : com.databricks.backend.daemon.data.client.DirectOutputCommitter\nspark.databricks.clusterUsageTags.clusterTargetWorkers : 2\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version : 2\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 : 0\nspark.sql.allowMultipleContexts : false\nspark.databricks.eventLog.enabled : true\nspark.hadoop.hive.server2.thrift.http.port : 10000\nspark.home : /databricks/spark\nspark.hadoop.hive.server2.idle.operation.timeout : 7200000\nfs.azure.account.oauth2.client.endpoint.qastdeveuwdlsdsl01.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\nspark.repl.class.uri : spark://172.24.129.196:35411/classes\nspark.task.reaper.enabled : true\nspark.storage.memoryFraction : 0.5\nspark.databricks.clusterUsageTags.autoTerminationMinutes : 60\nspark.databricks.clusterUsageTags.sparkEnvVarContainsNewline : false\nspark.hadoop.fs.fcfs-s3.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.delta.multiClusterWrites.enabled : true\nspark.databricks.clusterUsageTags.driverInstancePrivateIp : 172.24.129.132\nspark.worker.cleanup.enabled : false\nspark.sql.legacy.createHiveTableByDefault : false\nspark.databricks.driver.preferredMavenCentralMirrorUrl : https://maven-central.storage-download.googleapis.com/maven2/\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File : 0\nspark.hadoop.fs.fcfs-s3a.impl.disable.cache : true\nspark.ui.port : 40001\nspark.databricks.workspace.matplotlibInline.enabled : true\nspark.databricks.clusterUsageTags.enableCredentialPassthrough : false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign : false\nspark.databricks.clusterUsageTags.clusterNodeType : Standard_E4ds_v4\nspark.databricks.clusterUsageTags.enableJdbcAutoStart : true\nspark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough : false\nspark.hadoop.fs.fcfs-s3n.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.hadoop.fs.wasb.impl.disable.cache : true\nspark.databricks.wsfsPublicPreview : true\nspark.cleaner.referenceTracking.blocking : false\nfs.azure.account.oauth2.client.id.qastdeveuwdlsdsl01.dfs.core.windows.net : [REDACTED]\nspark.databricks.clusterUsageTags.isSingleUserCluster : false\nspark.databricks.clusterUsageTags.clusterState : Pending\nspark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes : false\nspark.databricks.tahoe.logStore.azure.class : com.databricks.tahoe.store.AzureLogStore\nspark.hadoop.fs.azure.skip.metrics : true\nspark.hadoop.fs.s3.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.hadoop.hive.hmshandler.retry.attempts : 10\nspark.scheduler.mode : FAIR\nspark.sql.sources.default : delta\nspark.hadoop.fs.mcfs-gs.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.hadoop.fs.cpfs-s3n.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.hadoop.fs.cpfs-adl.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.hadoop.fs.fcfs-s3n.impl.disable.cache : true\nspark.hadoop.fs.cpfs-abfss.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nfs.azure.account.oauth2.client.secret.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\ndatanucleus.autoCreateSchema : true\nfs.azure.account.oauth2.client.id.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\nspark.databricks.clusterUsageTags.clusterNumCustomTags : 1\nspark.databricks.clusterUsageTags.driverContainerPrivateIp : 172.24.129.196\nspark.databricks.passthrough.oauth.refresher.impl : com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\nspark.sql.hive.metastore.sharedPrefixes : org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version : 2.3.7\nspark.databricks.io.directoryCommit.enableLogicalDelete : false\nspark.task.reaper.killTimeout : 60s\nspark.databricks.managedCatalog.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider\nspark.hadoop.parquet.block.size.row.check.min : 10\nspark.hadoop.hive.server2.use.SSL : true\nspark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType : vnet-injection\nspark.hadoop.fs.mcfs-s3a.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.hadoop.databricks.dbfs.client.version : v2\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb : 0\nspark.databricks.repl.allowedLanguages : sql,python,r\nspark.hadoop.hive.server2.keystore.path : /databricks/keys/jetty-ssl-driver-keystore.jks\nspark.databricks.clusterUsageTags.clusterAllTags : [{&#34;key&#34;:&#34;ResourceClass&#34;,&#34;value&#34;:&#34;Serverless&#34;},{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;[REDACTED]&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;1202-161340-zrtmi161&#34;},{&#34;key&#34;:&#34;ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;UDP_Solution_BusinessOwner&#34;,&#34;value&#34;:&#34;ola.petersson@volvo.com&#34;},{&#34;key&#34;:&#34;Technical_Contact&#34;,&#34;value&#34;:&#34;maciej.poborca@consultant.volvo.com&#34;},{&#34;key&#34;:&#34;Business_Owner&#34;,&#34;value&#34;:&#34;bartlomiej.wierzbicki@volvo.com&#34;},{&#34;key&#34;:&#34;UDP_Platform_Environment&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;Assignment_Code&#34;,&#34;value&#34;:&#34;VY064I&#34;},{&#34;key&#34;:&#34;UDP_Solution_Name&#34;,&#34;value&#34;:&#34;qast&#34;},{&#34;key&#34;:&#34;InfoSeC&#34;,&#34;value&#34;:&#34;Confidential&#34;},{&#34;key&#34;:&#34;UDP_Solution_Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;Description&#34;,&#34;value&#34;:&#34;SCTASK100497225 - RITM100273453&#34;},{&#34;key&#34;:&#34;UDP_Solution_ID&#34;,&#34;value&#34;:&#34;109460&#34;},{&#34;key&#34;:&#34;Environment_Type&#34;,&#34;value&#34;:&#34;Dev&#34;},{&#34;key&#34;:&#34;PDS&#34;,&#34;value&#34;:&#34;High&#34;},{&#34;key&#34;:&#34;UDP_Solution_TechnicalContact&#34;,&#34;value&#34;:&#34;nikhil.rathore@volvo.com&#34;},{&#34;key&#34;:&#34;Mapped_ID&#34;,&#34;value&#34;:&#34;Automatic&#34;},{&#34;key&#34;:&#34;UDP_Solution_AssignmentCode&#34;,&#34;value&#34;:&#34;VY09ZT&#34;},{&#34;key&#34;:&#34;Service_Level&#34;,&#34;value&#34;:&#34;CEP_Basic&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8122829732360297&#34;}]\nspark.databricks.credential.redactor : com.databricks.logging.secrets.CredentialRedactorProxyImpl\nspark.sql.legacy.parquet.datetimeRebaseModeInRead : LEGACY\nspark.databricks.acl.provider : com.databricks.sql.acl.ReflectionBackedAclProvider\nspark.app.startTime : 1715363383441\nspark.databricks.mlflow.autologging.enabled : true\nspark.extraListeners : com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled : false\nspark.sql.parquet.cacheMetadata : true\nspark.databricks.clusterUsageTags.clusterLogDestination : dbfs:/mnt/cluster-logs\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss : 0\nspark.r.sql.derby.temp.dir : /tmp/RtmpsKdyDl\nspark.hadoop.fs.adl.impl : com.databricks.adl.AdlFileSystem\nspark.hadoop.fs.cpfs-abfss.impl.disable.cache : true\nspark.databricks.clusterUsageTags.azureSubscriptionId : 04abf2eb-58f5-4264-8858-3793629a7259\nspark.databricks.clusterUsageTags.enableLocalDiskEncryption : false\nspark.databricks.tahoe.logStore.class : com.databricks.tahoe.store.DelegatingLogStore\nfs.azure.account.oauth2.client.id.datafounddeveuwdlsiss.dfs.core.windows.net : [REDACTED]\nlibraryDownload.sleepIntervalSeconds : 5\nspark.sql.hive.convertMetastoreParquet : true\nspark.databricks.service.dbutils.server.backend : com.databricks.dbconnect.SparkServerDBUtils\nspark.executor.id : driver\nspark.databricks.managedCatalog.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider\nspark.databricks.repl.enableClassFileCleanup : true\nspark.databricks.clusterUsageTags.clusterAvailability : SPOT_WITH_FALLBACK_AZURE\nspark.sql.catalogImplementation : hive\nspark.sql.legacy.parquet.datetimeRebaseModeInWrite : LEGACY\nspark.databricks.clusterUsageTags.region : westeurope\nspark.databricks.clusterUsageTags.instanceWorkerEnvId : workerenv-8122829732360297\nspark.hadoop.fs.s3a.multipart.size : 10485760\nspark.databricks.clusterUsageTags.effectiveSparkVersion : 9.1.x-scala2.12\nspark.metrics.conf : /databricks/spark/conf/metrics.properties\nspark.databricks.workspaceUrl : adb-8122829732360297.17.azuredatabricks.net\nspark.akka.frameSize : 256\nspark.hadoop.fs.s3a.fast.upload : true\nspark.hadoop.fs.wasbs.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.sql.streaming.stopTimeout : 15s\nspark.hadoop.hive.server2.keystore.password : [REDACTED]\nspark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting : false\nspark.executor.extraClassPath : /databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.1-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_1--common--kvstore--kvstore-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-common--network-common-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--network-shuffle--network-shuffle-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--sketch--sketch-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--tags--tags-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--common--unsafe--unsafe-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--core-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_1--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_1--core--proto-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--graphx--graphx-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--launcher--launcher-hive-2.3__hadoop-2.7_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--ch.qos.reload4j--reload4j--ch.qos.reload4j__reload4j__1.2.19.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacecommerceanalytics--com.amazonaws__aws-java-sdk-marketplacecommerceanalytics__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-marketplacemeteringservice--com.amazonaws__aws-java-sdk-marketplacemeteringservice__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.678.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill-java--com.twitter__chill-java__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--chill_2.12--com.twitter__chill_2.12__0.9.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.29.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8-1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-core--org.apache.arrow__arrow-memory-core__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-memory-netty--org.apache.arrow__arrow-memory-netty__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__2.0.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-text--org.apache.commons__commons-text__1.10.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.4.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-cli--org.apache.hive__hive-cli__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-client--org.apache.hive__hive-llap-client__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive--hive-storage-api--org.apache.hive__hive-storage-api__2.7.2.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hive.shims--hive-shims-scheduler--org.apache.hive.shims__hive-shims-scheduler__2.3.7.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:/databricks/jars/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.\n*** WARNING: skipped 62833 bytes of output ***\n\n\nspark.databricks.clusterUsageTags.sparkEnvVarContainsEscape : false\nspark.databricks.overrideDefaultCommitProtocol : org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\nspark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass : com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\nspark.databricks.clusterUsageTags.clusterNoDriverDaemon : false\nlibraryDownload.timeoutSeconds : 180\nspark.hadoop.parquet.memory.pool.ratio : 0.5\nspark.databricks.clusterUsageTags.clusterOwnerUserId : 8668945429017751\nspark.hadoop.javax.jdo.option.ConnectionUserName : hiveuser\nspark.databricks.sparkContextId : 4618732084335514242\nspark.databricks.clusterUsageTags.clusterScalingType : fixed_size\nspark.databricks.passthrough.adls.gen2.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\nspark.executor.memory : 21588m\nfs.azure.account.oauth.provider.type.qastdeveuwdlsdsl01.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.databricks.tahoe.logStore.gcp.class : com.databricks.tahoe.store.GCPLogStore\nspark.serializer.objectStreamReset : 100\nspark.databricks.clusterUsageTags.sparkMasterUrlType : None\nspark.hadoop.javax.jdo.option.ConnectionDriverName : com.microsoft.sqlserver.jdbc.SQLServerDriver\nspark.sql.sources.commitProtocolClass : com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\nspark.hadoop.fs.abfss.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs : 0\nspark.hadoop.fs.fcfs-s3a.impl : com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.attribute_tag_budget : \nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeType : azure_disk_volume_type: PREMIUM_LRS\n\nspark.databricks.clusterUsageTags.clusterWorkers : 2\nspark.databricks.clusterUsageTags.clusterPythonVersion : 3\nspark.databricks.clusterUsageTags.enableDfAcls : false\nspark.databricks.cloudfetch.requestDownloadUrlsWithHeaders : true\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount : 0\nspark.shuffle.service.enabled : true\nspark.hadoop.fs.file.impl : com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\nspark.hadoop.fs.mcfs-s3n.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.hadoop.fs.cpfs-s3.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.attribute_tag_dust_maintainer : \nfs.azure.account.oauth.provider.type.datafounddeveuwdlsenr.dfs.core.windows.net : org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.hadoop.fs.s3a.multipart.threshold : 104857600\nspark.databricks.workerNodeTypeId : Standard_E4ds_v4\nspark.rpc.message.maxSize : 256\nspark.databricks.clusterUsageTags.workerEnvironmentId : workerenv-8122829732360297\nspark.databricks.clusterUsageTags.attribute_tag_dust_suite : \nspark.databricks.driverNfs.enabled : true\nspark.databricks.clusterUsageTags.clusterMetastoreAccessType : RDS_DIRECT\nspark.databricks.passthrough.glue.executorServiceFactoryClassName : com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\nspark.hadoop.javax.jdo.option.ConnectionPassword : P5fm4wpj6fm4u4ebrag23o2lqwk2qyahju4tcg34x!\nspark.hadoop.fs.abfs.impl : shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem\nspark.master : spark://172.24.129.196:7077\nspark.databricks.clusterUsageTags.enableElasticDisk : true\nspark.databricks.acl.scim.client : com.databricks.spark.sql.acl.client.DriverToWebappScimClient\nspark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick : false\nspark.hadoop.fs.adl.impl.disable.cache : true\nspark.hadoop.parquet.block.size.row.check.max : 10\nspark.hadoop.fs.s3a.connection.maximum : 200\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 : 0\nspark.hadoop.fs.s3a.fast.upload.active.blocks : 32\nspark.shuffle.reduceLocality.enabled : false\nspark.hadoop.spark.sql.sources.outputCommitterClass : com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\nspark.hadoop.fs.AbstractFileSystem.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nspark.hadoop.hive.server2.thrift.http.cookie.auth.enabled : false\nspark.sql.parquet.compression.codec : snappy\nspark.databricks.cloudProvider : Azure\nspark.driver.maxResultSize : 0\nfs.azure.account.oauth2.client.secret.datafounddeveuwdlsenr.dfs.core.windows.net : [REDACTED]\nspark.executor.extraJavaOptions : -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\nspark.databricks.cloudfetch.hasRegionSupport : true\nspark.databricks.clusterUsageTags.ngrokNpipEnabled : true\nspark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories : false\nspark.hadoop.fs.wasb.impl : shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.hadoop.fs.mcfs-abfss.impl.disable.cache : true\nspark.databricks.passthrough.glue.credentialsProviderFactoryClassName : com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\nspark.databricks.clusterUsageTags.clusterSpotBidMaxPrice : -1.0\nspark.sql.warehouse.dir : /user/hive/warehouse\nspark.databricks.passthrough.s3a.tokenProviderClassName : com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\nspark.databricks.session.share : false\nspark.databricks.clusterUsageTags.userId : 8668945429017751\nspark.databricks.isShieldWorkspace : false\nspark.databricks.telemetry.prometheus.samplingRate : 100\nspark.driver.host : 172.24.129.196\nspark.databricks.clusterUsageTags.clusterSku : STANDARD_SKU\nspark.hadoop.fs.gs.impl.disable.cache : true\nspark.databricks.clusterUsageTags.managedResourceGroup : databricks-rg-udp-qast-dev-euw-dbw-lxni2stdfcvd6\nspark.databricks.privateLinkEnabled : false\nspark.databricks.clusterUsageTags.clusterUnityCatalogMode : CUSTOM\nspark.databricks.io.cache.enabled : true\nspark.databricks.managedCatalog.gcs.tokenProviderClassName : com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider\nfs.azure.account.auth.type.datafounddeveuwdlsiss.dfs.core.windows.net : OAuth\nspark.databricks.automl.serviceEnabled : true\nspark.hadoop.parquet.page.size.check.estimate : false\nspark.hadoop.spark.driverproxy.customHeadersToProperties : X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name\nspark.databricks.passthrough.s3a.threadPoolExecutor.factory.class : com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\nspark.databricks.clusterUsageTags.attribute_tag_service : \nspark.databricks.delta.preview.enabled : true\nspark.databricks.clusterSource : API\nspark.databricks.metrics.filesystem_io_metrics : true\nspark.databricks.clusterUsageTags.dataPlaneRegion : westeurope\nspark.databricks.cloudfetch.requesterClassName : com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\nspark.databricks.delta.logStore.crossCloud.fatal : true\nspark.files.fetchFailure.unRegisterOutputOnHost : true\nspark.databricks.clusterUsageTags.enableSqlAclsOnly : false\nspark.databricks.clusterUsageTags.clusterNumSshKeys : 0\nspark.databricks.clusterUsageTags.clusterSizeType : VM_CONTAINER\nspark.hadoop.databricks.fs.perfMetrics.enable : true\nspark.databricks.clusterUsageTags.clusterPinned : true\nspark.hadoop.fs.gs.outputstream.upload.chunk.size : 16777216\nspark.speculation.quantile : 0.9\nspark.databricks.clusterUsageTags.privateLinkEnabled : false\nspark.shuffle.manager : SORT\nspark.files.overwrite : true\nspark.databricks.clusterUsageTags.userProvidedSparkVersion : 9.1.x-scala2.12\nspark.databricks.driverNodeTypeId : Standard_E4ds_v4\ndatanucleus.fixedDatastore : false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes : false\nspark.r.numRBackendThreads : 1\nspark.hadoop.fs.wasbs.impl.disable.cache : true\nspark.hadoop.fs.abfss.impl.disable.cache : true\nspark.databricks.workspace.multipleResults.enabled : true\nspark.shuffle.service.port : 4048\nspark.databricks.clusterUsageTags.clusterOwnerOrgId : 8122829732360297\nspark.databricks.acl.client : com.databricks.spark.sql.acl.client.SparkSqlAclClient\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite : true\nspark.hadoop.hive.warehouse.subdir.inherit.perms : false\nfs.azure.account.auth.type.qastdeveuwdlsdsl01.dfs.core.windows.net : OAuth\nspark.hadoop.fs.mcfs-abfss.impl : com.databricks.sql.acl.fs.ManagedCatalogFileSystem\nspark.databricks.clusterUsageTags.runtimeEngine : STANDARD\nspark.databricks.clusterUsageTags.isServicePrincipalCluster : false\nspark.hadoop.fs.s3n.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.databricks.clusterUsageTags.orgId : 8122829732360297\nspark.databricks.enablePublicDbfsFuse : false\nspark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 : 1\nspark.driver.port : 35411\nspark.databricks.io.cache.maxDiskUsage : 50g\nspark.databricks.passthrough.adls.tokenProviderClassName : com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\nspark.app.name : Databricks Shell\nspark.driver.allowMultipleContexts : false\nspark.rdd.compress : true\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException : false\nspark.databricks.clusterUsageTags.attribute_tag_dust_execution_env : \nspark.databricks.eventLog.dir : eventlogs\nspark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled : false\nspark.databricks.driverNfs.pathSuffix : .ephemeral_nfs\nspark.sql.hive.metastore.jars : builtin\nspark.speculation : false\nspark.hadoop.hive.server2.session.check.interval : 60000\nspark.sql.hive.convertCTAS : true\nspark.hadoop.spark.sql.parquet.output.committer.class : org.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.hadoop.fs.gs.impl : shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\nspark.databricks.clusterUsageTags.clusterLogDeliveryEnabled : true\nspark.databricks.tahoe.logStore.aws.class : com.databricks.tahoe.store.MultiClusterLogStore\nspark.hadoop.fs.s3a.fast.upload.default : true\nspark.hadoop.fs.mlflowdbfs.impl : com.databricks.mlflowdbfs.MlflowdbfsFileSystem\nspark.databricks.eventLog.listenerClassName : com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.fs.abfs.impl.disable.cache : true\nspark.databricks.io.cache.maxMetaDataCache : 10g\nspark.speculation.multiplier : 3\nspark.storage.blockManagerTimeoutIntervalMs : 300000\nspark.databricks.clusterUsageTags.sparkVersion : 9.1.x-scala2.12\nspark.databricks.clusterUsageTags.clusterGeneration : 323\nspark.sparkr.use.daemon : false\nspark.scheduler.listenerbus.eventqueue.capacity : 20000\nspark.databricks.clusterUsageTags.clusterResourceClass : Serverless\nspark.databricks.clusterUsageTags.clusterStateMessage : Starting Spark\nspark.hadoop.parquet.page.write-checksum.enabled : true\nspark.hadoop.databricks.s3commit.client.sslTrustAll : false\nspark.hadoop.fs.s3a.threads.max : 136\nspark.hadoop.javax.jdo.option.ConnectionURL : jdbc:sqlserver://datafound-dev-euw-sqlsrv-01.database.windows.net:1433;database=datafounddeveuwsqldbhivems;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\nspark.databricks.clusterUsageTags.clusterId : 1202-161340-zrtmi161\nspark.r.backendConnectionTimeout : 604800\nspark.ui.prometheus.enabled : true\nfs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsiss.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs : 0\nspark.hadoop.hive.server2.idle.session.timeout : 900000\nspark.databricks.redactor : com.databricks.spark.util.DatabricksSparkLogRedactorProxy\nspark.hadoop.fs.s3a.impl : shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.databricks.clusterUsageTags.clusterName : HC-UDP-QAST-ADF-ENGINE-9.1-LTS-ETL\nspark.databricks.clusterUsageTags.driverNodeType : Standard_E4ds_v4\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes : 0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace : 0\nspark.hadoop.parquet.page.verify-checksum.enabled : true\nspark.logConf : true\nspark.databricks.clusterUsageTags.enableJobsAutostart : true\nspark.hadoop.hive.server2.enable.doAs : false\nspark.shuffle.memoryFraction : 0.2\nspark.hadoop.fs.cpfs-s3a.impl : com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.secret.envVar.keys.toRedact : \nfs.azure.account.oauth2.client.endpoint.datafounddeveuwdlsenr.dfs.core.windows.net : https://login.microsoftonline.com/f25493ae-1c98-41d7-8a33-0be75f5fe603/oauth2/token\nfs.azure.account.auth.type.datafounddeveuwdlsenr.dfs.core.windows.net : OAuth\nspark.databricks.clusterUsageTags.cloudProvider : Azure\nspark.files.useFetchCache : false\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-3151542764839107&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> conf <span class=\"ansi-blue-fg\">=</span> SparkConf<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> conf<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.executor.cores&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;2&#34;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\"># set the number of cores you want here</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 12</span><span class=\"ansi-red-fg\"> </span>sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">(</span>conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    143</span>                 &#34; is not allowed as it is a security risk.&#34;)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    144</span> \n",
       "<span class=\"ansi-green-fg\">--&gt; 145</span><span class=\"ansi-red-fg\">         </span>SparkContext<span class=\"ansi-blue-fg\">.</span>_ensure_initialized<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> gateway<span class=\"ansi-blue-fg\">=</span>gateway<span class=\"ansi-blue-fg\">,</span> conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    146</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    147</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">_ensure_initialized</span><span class=\"ansi-blue-fg\">(cls, instance, gateway, conf)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    348</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    349</span>                     <span class=\"ansi-red-fg\"># Raise error if there is already a running Spark context</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 350</span><span class=\"ansi-red-fg\">                     raise ValueError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    351</span>                         <span class=\"ansi-blue-fg\">&#34;Cannot run multiple SparkContexts at once; &#34;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    352</span>                         <span class=\"ansi-blue-fg\">&#34;existing SparkContext(app=%s, master=%s)&#34;</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=spark://172.24.129.196:7077) created by __init__ at /databricks/python_shell/scripts/PythonShellImpl.py:1223 </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3151542764839107&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> conf <span class=\"ansi-blue-fg\">=</span> SparkConf<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> conf<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.executor.cores&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;2&#34;</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\"># set the number of cores you want here</span>\n<span class=\"ansi-green-fg\">---&gt; 12</span><span class=\"ansi-red-fg\"> </span>sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">(</span>conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    143</span>                 &#34; is not allowed as it is a security risk.&#34;)\n<span class=\"ansi-green-intense-fg ansi-bold\">    144</span> \n<span class=\"ansi-green-fg\">--&gt; 145</span><span class=\"ansi-red-fg\">         </span>SparkContext<span class=\"ansi-blue-fg\">.</span>_ensure_initialized<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> gateway<span class=\"ansi-blue-fg\">=</span>gateway<span class=\"ansi-blue-fg\">,</span> conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    146</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    147</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">_ensure_initialized</span><span class=\"ansi-blue-fg\">(cls, instance, gateway, conf)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    348</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    349</span>                     <span class=\"ansi-red-fg\"># Raise error if there is already a running Spark context</span>\n<span class=\"ansi-green-fg\">--&gt; 350</span><span class=\"ansi-red-fg\">                     raise ValueError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    351</span>                         <span class=\"ansi-blue-fg\">&#34;Cannot run multiple SparkContexts at once; &#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    352</span>                         <span class=\"ansi-blue-fg\">&#34;existing SparkContext(app=%s, master=%s)&#34;</span>\n\n<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=spark://172.24.129.196:7077) created by __init__ at /databricks/python_shell/scripts/PythonShellImpl.py:1223 </div>",
       "errorSummary": "<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=spark://172.24.129.196:7077) created by __init__ at /databricks/python_shell/scripts/PythonShellImpl.py:1223 ",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Cluster Details\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "# Print all configurations\n",
    "for k,v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} : {v}\")\n",
    "    \n",
    "### PySpark to use the number of cores\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.cores\", \"2\") # set the number of cores you want here\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "befb6a2a-38ad-4e84-9768-0216bda2fe19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#60. How to cache PySpark DataFrame or objects and delete cache?\n",
    "- In PySpark, caching or persisting data is done to speed up data retrieval during iterative and interactive computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4dfd61-60bb-421c-9524-e70274b39696",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[124]: DataFrame[Name: string, City: string, City_lower: string]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[124]: DataFrame[Name: string, City: string, City_lower: string]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Caching the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# un-cache or unpersist data using the unpersist() method.\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bd0104-8302-445e-843d-470a16f06128",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#61. How to Divide a PySpark DataFrame randomly in a given ratio (0.8, 0.2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b446baf-88d0-406c-bdea-82f30ee194db",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-3151542764839111&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Randomly split data (0.8, 0.2)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>train_data<span class=\"ansi-blue-fg\">,</span> test_data <span class=\"ansi-blue-fg\">=</span> data<span class=\"ansi-blue-fg\">.</span>randomSplit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.8</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0.2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> seed<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">42</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;list&#39; object has no attribute &#39;randomSplit&#39;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3151542764839111&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Randomly split data (0.8, 0.2)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>train_data<span class=\"ansi-blue-fg\">,</span> test_data <span class=\"ansi-blue-fg\">=</span> data<span class=\"ansi-blue-fg\">.</span>randomSplit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.8</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0.2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> seed<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">42</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;list&#39; object has no attribute &#39;randomSplit&#39;</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;list&#39; object has no attribute &#39;randomSplit&#39;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly split data (0.8, 0.2)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64cfc06-e99d-4df1-bc50-0741c64f8ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#62. How to build logistic regression in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9e1a79-819a-4b74-85c8-9a44090a44b2",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = spark.createDataFrame([\n",
    "(0, 1.0, -1.0),\n",
    "(1, 2.0, 1.0),\n",
    "(1, 3.0, -2.0),\n",
    "(0, 4.0, 1.0),\n",
    "(1, 5.0, -3.0),\n",
    "(0, 6.0, 2.0),\n",
    "(1, 7.0, -1.0),\n",
    "(0, 8.0, 3.0),\n",
    "(1, 9.0, -2.0),\n",
    "(0, 10.0, 2.0),\n",
    "(1, 11.0, -3.0),\n",
    "(0, 12.0, 1.0),\n",
    "(1, 13.0, -1.0),\n",
    "(0, 14.0, 2.0),\n",
    "(1, 15.0, -2.0),\n",
    "(0, 16.0, 3.0),\n",
    "(1, 17.0, -3.0),\n",
    "(0, 18.0, 1.0),\n",
    "(1, 19.0, -1.0),\n",
    "(0, 20.0, 2.0)\n",
    "], [\"label\", \"feat1\", \"feat2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c2d147d-48af-4830-bf3b-81900c88bf48",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Coefficients: [0.020277736834151697,-1.6129606404964458]\n",
       "Intercept: -0.22092825608663494\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Coefficients: [0.020277736834151697,-1.6129606404964458]\nIntercept: -0.22092825608663494\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### logistic regression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# convert the feature columns into a single vector column using VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=['feat1', 'feat2'], outputCol=\"features\")\n",
    "data = vecAssembler.transform(data)\n",
    "\n",
    "# fit the logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(data)\n",
    "\n",
    "# look at the coefficients and intercept of the logistic regression model\n",
    "print(f\"Coefficients: {str(lr_model.coefficients)}\")\n",
    "print(f\"Intercept: {str(lr_model.intercept)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e94e001-a0f1-48d4-99af-e21ac1d6c824",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#63. How to convert the categorical string data into numerical data or index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9411d1ba-7bdd-4415-a221-d0e18becda83",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+\n",
       "animal|\n",
       "+------+\n",
       "   cat|\n",
       "   dog|\n",
       " mouse|\n",
       "  fish|\n",
       "   dog|\n",
       "   cat|\n",
       " mouse|\n",
       "+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+\n|animal|\n+------+\n|   cat|\n|   dog|\n| mouse|\n|  fish|\n|   dog|\n|   cat|\n| mouse|\n+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [('cat',), ('dog',), ('mouse',), ('fish',), ('dog',), ('cat',), ('mouse',)]\n",
    "df = spark.createDataFrame(data, [\"animal\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bef264-b1f9-44f9-adb6-1e55c74fbdff",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>animal</th><th>animalIndex</th></tr></thead><tbody><tr><td>cat</td><td>0.0</td></tr><tr><td>dog</td><td>1.0</td></tr><tr><td>mouse</td><td>2.0</td></tr><tr><td>fish</td><td>3.0</td></tr><tr><td>dog</td><td>1.0</td></tr><tr><td>cat</td><td>0.0</td></tr><tr><td>mouse</td><td>2.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "cat",
         0.0
        ],
        [
         "dog",
         1.0
        ],
        [
         "mouse",
         2.0
        ],
        [
         "fish",
         3.0
        ],
        [
         "dog",
         1.0
        ],
        [
         "cat",
         0.0
        ],
        [
         "mouse",
         2.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "animal",
         "type": "\"string\""
        },
        {
         "metadata": "{\"ml_attr\":{\"vals\":[\"cat\",\"dog\",\"mouse\",\"fish\"],\"type\":\"nominal\",\"name\":\"animalIndex\"}}",
         "name": "animalIndex",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### convert the categorical string data into numerical data or index\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Initialize a StringIndexer\n",
    "indexer = StringIndexer(inputCol='animal', outputCol='animalIndex')\n",
    "\n",
    "# Fit the indexer to the DataFrame and transform the data\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d94a37-a128-4473-9c2e-43f0ba788851",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#64. How to calculate Correlation of two variables in a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85025a0a-c79d-4896-87db-507da2f5d47e",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+--------+--------+\n",
       "feature1|feature2|feature3|\n",
       "+--------+--------+--------+\n",
       "       5|      10|      25|\n",
       "       6|      15|      35|\n",
       "       7|      25|      30|\n",
       "       8|      20|      60|\n",
       "       9|      30|      70|\n",
       "+--------+--------+--------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------+--------+--------+\n|feature1|feature2|feature3|\n+--------+--------+--------+\n|       5|      10|      25|\n|       6|      15|      35|\n|       7|      25|      30|\n|       8|      20|      60|\n|       9|      30|      70|\n+--------+--------+--------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f811a21-311c-4cfd-96e7-2b6b56a4cf90",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Correlation between feature1 and feature2 : 0.9000000000000001\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Correlation between feature1 and feature2 : 0.9000000000000001\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Calculate correlation\n",
    "correlation = df.corr(\"feature1\", \"feature2\")\n",
    "\n",
    "print(\"Correlation between feature1 and feature2 :\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea05a0d4-8a20-4d46-885b-5ef9cb2fab6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#65. How to calculate Correlation Matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e0133b-0789-439e-a2f9-73d4c5fa7831",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+--------+--------+\n",
       "feature1|feature2|feature3|\n",
       "+--------+--------+--------+\n",
       "       5|      10|      25|\n",
       "       6|      15|      35|\n",
       "       7|      25|      30|\n",
       "       8|      20|      60|\n",
       "       9|      30|      70|\n",
       "+--------+--------+--------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------+--------+--------+\n|feature1|feature2|feature3|\n+--------+--------+--------+\n|       5|      10|      25|\n|       6|      15|      35|\n|       7|      25|      30|\n|       8|      20|      60|\n|       9|      30|      70|\n+--------+--------+--------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe87d37-3c69-4bd0-9395-6fce89e73fd6",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">DenseMatrix([[1.        , 0.9       , 0.91779992],\n",
       "             [0.9       , 1.        , 0.67837385],\n",
       "             [0.91779992, 0.67837385, 1.        ]])\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">DenseMatrix([[1.        , 0.9       , 0.91779992],\n             [0.9       , 1.        , 0.67837385],\n             [0.91779992, 0.67837385, 1.        ]])\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col0</th><th>col1</th><th>col2</th></tr></thead><tbody><tr><td>1.0</td><td>0.9</td><td>0.9177999</td></tr><tr><td>0.9</td><td>1.0</td><td>0.6783739</td></tr><tr><td>0.9177999</td><td>0.6783739</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1.0,
         0.9,
         0.9177999
        ],
        [
         0.9,
         1.0,
         0.6783739
        ],
        [
         0.9177999,
         0.6783739,
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col0",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "col1",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "col2",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Correlation Matrix\n",
    "# Calculate Correlation Using Using MLlib\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import *\n",
    "# Assemble feature vector\n",
    "# Define the feature and label columns & Assemble the feature vector\n",
    "vector_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "data_vector = vector_assembler.transform(df).select(\"features\")\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = Correlation.corr(data_vector, \"features\").head()[0]\n",
    "\n",
    "print(correlation_matrix)\n",
    "\n",
    "correlation_list = correlation_matrix.toArray().tolist()\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([StructField(f\"col{i}\", FloatType(), True) for i in range(len(correlation_list))])\n",
    "# Create the DataFrame\n",
    "correlation_df = spark.createDataFrame(correlation_list, schema)\n",
    "\n",
    "correlation_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d2f4e9-ea92-45c0-a3b9-327c383fc06b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#66. How to calculate VIF (Variance Inflation Factor ) for set of variables in a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c780aade-72a8-469f-b494-af6729ff6432",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+--------+--------+\n",
       "feature1|feature2|feature3|\n",
       "+--------+--------+--------+\n",
       "       5|      10|      25|\n",
       "       6|      15|      35|\n",
       "       7|      25|      30|\n",
       "       8|      20|      60|\n",
       "       9|      30|      70|\n",
       "+--------+--------+--------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------+--------+--------+\n|feature1|feature2|feature3|\n+--------+--------+--------+\n|       5|      10|      25|\n|       6|      15|      35|\n|       7|      25|      30|\n|       8|      20|      60|\n|       9|      30|      70|\n+--------+--------+--------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f8407f-6e17-485a-94a9-c4c3f283ee5a",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">VIF for feature1: 66.2109374999998\n",
       "VIF for feature2: 19.335937499999957\n",
       "VIF for feature3: 23.30468749999998\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">VIF for feature1: 66.2109374999998\nVIF for feature2: 19.335937499999957\nVIF for feature3: 23.30468749999998\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def calculate_vif(data, features):\n",
    "    vif_dict = {}\n",
    "\n",
    "    for feature in features:\n",
    "        non_feature_cols = [col for col in features if col != feature]\n",
    "        assembler = VectorAssembler(inputCols=non_feature_cols, outputCol=\"features\")\n",
    "        lr = LinearRegression(featuresCol='features', labelCol=feature)\n",
    "\n",
    "        model = lr.fit(assembler.transform(data))\n",
    "        vif = 1 / (1 - model.summary.r2)\n",
    "\n",
    "        vif_dict[feature] = vif\n",
    "\n",
    "    return vif_dict\n",
    "\n",
    "features = ['feature1', 'feature2', 'feature3']\n",
    "vif_values = calculate_vif(df, features)\n",
    "\n",
    "for feature, vif in vif_values.items():\n",
    "    print(f'VIF for {feature}: {vif}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a680723-f598-408b-bad7-2acd90aaed80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#67. How to perform Chi-Square test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4081f416-ada0-4c7e-95f3-438ed9c14d56",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+--------+--------+--------+-----+\n",
       " id|feature1|feature2|feature3|label|\n",
       "+---+--------+--------+--------+-----+\n",
       "  1|       0|       0|       1|    1|\n",
       "  2|       0|       1|       0|    0|\n",
       "  3|       1|       0|       0|    0|\n",
       "  4|       0|       0|       1|    1|\n",
       "  5|       0|       1|       1|    0|\n",
       "+---+--------+--------+--------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+--------+--------+--------+-----+\n| id|feature1|feature2|feature3|label|\n+---+--------+--------+--------+-----+\n|  1|       0|       0|       1|    1|\n|  2|       0|       1|       0|    0|\n|  3|       1|       0|       0|    0|\n|  4|       0|       0|       1|    1|\n|  5|       0|       1|       1|    0|\n+---+--------+--------+--------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [(1, 0, 0, 1, 1),\n",
    "(2, 0, 1, 0, 0),\n",
    "(3, 1, 0, 0, 0),\n",
    "(4, 0, 0, 1, 1),\n",
    "(5, 0, 1, 1, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"feature1\", \"feature2\", \"feature3\", \"label\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95cf453-353e-4110-878e-51c2987f6529",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">pValues: [0.36131042852617856,0.13603712811414348,0.1360371281141436]\n",
       "degreesOfFreedom: [1, 1, 1]\n",
       "statistics: [0.8333333333333335,2.2222222222222228,2.2222222222222223]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">pValues: [0.36131042852617856,0.13603712811414348,0.1360371281141436]\ndegreesOfFreedom: [1, 1, 1]\nstatistics: [0.8333333333333335,2.2222222222222228,2.2222222222222223]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a24cf7-6a62-439f-93c5-2a298bcb40c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#68. How to calculate the Standard Deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3fa8ff-8c15-4f2e-ad6a-a68ed02ba9c1",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+----------+------+\n",
       "Employee|Department|Salary|\n",
       "+--------+----------+------+\n",
       "   James|     Sales|  3000|\n",
       " Michael|     Sales|  4600|\n",
       "  Robert|     Sales|  4100|\n",
       "   Maria|   Finance|  3000|\n",
       "   James|     Sales|  3000|\n",
       "   Scott|   Finance|  3300|\n",
       "     Jen|   Finance|  3900|\n",
       "    Jeff| Marketing|  3000|\n",
       "   Kumar| Marketing|  2000|\n",
       "    Saif|     Sales|  4100|\n",
       "+--------+----------+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|   James|     Sales|  3000|\n| Michael|     Sales|  4600|\n|  Robert|     Sales|  4100|\n|   Maria|   Finance|  3000|\n|   James|     Sales|  3000|\n|   Scott|   Finance|  3300|\n|     Jen|   Finance|  3900|\n|    Jeff| Marketing|  3000|\n|   Kumar| Marketing|  2000|\n|    Saif|     Sales|  4100|\n+--------+----------+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "(\"Michael\", \"Sales\", 4600),\n",
    "(\"Robert\", \"Sales\", 4100),\n",
    "(\"Maria\", \"Finance\", 3000),\n",
    "(\"James\", \"Sales\", 3000),\n",
    "(\"Scott\", \"Finance\", 3300),\n",
    "(\"Jen\", \"Finance\", 3900),\n",
    "(\"Jeff\", \"Marketing\", 3000),\n",
    "(\"Kumar\", \"Marketing\", 2000),\n",
    "(\"Saif\", \"Sales\", 4100)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Employee\", \"Department\", \"Salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c79f3c-456f-4c40-b13e-680248c58b0c",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>stddev</th></tr></thead><tbody><tr><td>765.9416862050705</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         765.9416862050705
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "stddev",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Standard Deviation\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "df_result = df.select(stddev(\"Salary\").alias(\"stddev\"))\n",
    "\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1013a2c-20e2-4b66-af9e-c81cd72cab88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#69. How to calculate missing value percentage in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e79e39-04ce-4996-b43a-ac134463df7a",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+--------+--------+\n",
       "FirstName|LastName|    City|\n",
       "+---------+--------+--------+\n",
       "     John|     Doe|    null|\n",
       "     null|   Smith|New York|\n",
       "     Mike|   Smith|    null|\n",
       "     Anna|   Smith|  Boston|\n",
       "     null|    null|    null|\n",
       "+---------+--------+--------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---------+--------+--------+\n|FirstName|LastName|    City|\n+---------+--------+--------+\n|     John|     Doe|    null|\n|     null|   Smith|New York|\n|     Mike|   Smith|    null|\n|     Anna|   Smith|  Boston|\n|     null|    null|    null|\n+---------+--------+--------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a sample dataframe\n",
    "data = [(\"John\", \"Doe\", None),\n",
    "(None, \"Smith\", \"New York\"),\n",
    "(\"Mike\", \"Smith\", None),\n",
    "(\"Anna\", \"Smith\", \"Boston\"),\n",
    "(None, None, None)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"FirstName\", \"LastName\", \"City\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d590a18-d493-425b-adca-3b23eeaf6020",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Missing values in FirstName: 40.0%\n",
       "Missing values in LastName: 20.0%\n",
       "Missing values in City: 60.0%\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Missing values in FirstName: 40.0%\nMissing values in LastName: 20.0%\nMissing values in City: 60.0%\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the total number of rows in the dataframe\n",
    "total_rows = df.count()\n",
    "\n",
    "# For each column calculate the number of null values and then calculate the percentage\n",
    "for column in df.columns:\n",
    "    null_values = df.filter(df[column].isNull()).count()\n",
    "    missing_percentage = (null_values / total_rows) * 100\n",
    "    print(f\"Missing values in {column}: {missing_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce035cd-1be7-4eb9-9090-c876bd3b3c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#70. How to get the names of DataFrame objects that have been created in an environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105da72e-4656-4a94-9764-663dc32cd397",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">_\n",
       "df\n",
       "df_index\n",
       "df_min\n",
       "df_freq\n",
       "df_tmp\n",
       "df_result\n",
       "df_renamed\n",
       "df_binned\n",
       "df_A\n",
       "df_B\n",
       "df_src\n",
       "df_with_diag_zero\n",
       "indexed_df\n",
       "encoded_df\n",
       "df_imputed\n",
       "df_grouped\n",
       "mode_df\n",
       "pysparkDF\n",
       "_124\n",
       "indexed\n",
       "data_vector\n",
       "correlation_df\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">_\ndf\ndf_index\ndf_min\ndf_freq\ndf_tmp\ndf_result\ndf_renamed\ndf_binned\ndf_A\ndf_B\ndf_src\ndf_with_diag_zero\nindexed_df\nencoded_df\ndf_imputed\ndf_grouped\nmode_df\npysparkDF\n_124\nindexed\ndata_vector\ncorrelation_df\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## DataFrame objects that have been created\n",
    "from pyspark.sql import dataframe\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dataframe_names = [name for name, obj in globals().items() if isinstance(obj, DataFrame)]\n",
    "\n",
    "for name in dataframe_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a5497d-3a5a-4d47-9a40-46a28cd96d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#71. Reading and Writing Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b73352f-ba6f-44cd-9a7d-e07a433a4cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##1. CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9c27e8-acc8-4c82-b64b-fe11af02bc19",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-1177762865691178&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\">#infer schema</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> csv_file <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;path/to/your/csv/file.csv&#34;</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>df_csv <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>csv_file<span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> StructType<span class=\"ansi-blue-fg\">,</span> StructField<span class=\"ansi-blue-fg\">,</span> StringType<span class=\"ansi-blue-fg\">,</span> DoubleType<span class=\"ansi-blue-fg\">,</span> TimestampType\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    762</span>             path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>path<span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    763</span>         <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> list<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 764</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    765</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    766</span>             <span class=\"ansi-green-fg\">def</span> func<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Path must be absolute: path/to/your/csv/file.csv</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1177762865691178&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\">#infer schema</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> csv_file <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;path/to/your/csv/file.csv&#34;</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>df_csv <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>csv_file<span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> StructType<span class=\"ansi-blue-fg\">,</span> StructField<span class=\"ansi-blue-fg\">,</span> StringType<span class=\"ansi-blue-fg\">,</span> DoubleType<span class=\"ansi-blue-fg\">,</span> TimestampType\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">csv</span><span class=\"ansi-blue-fg\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    762</span>             path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>path<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    763</span>         <span class=\"ansi-green-fg\">if</span> type<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> list<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 764</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    765</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    766</span>             <span class=\"ansi-green-fg\">def</span> func<span class=\"ansi-blue-fg\">(</span>iterator<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Path must be absolute: path/to/your/csv/file.csv</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Path must be absolute: path/to/your/csv/file.csv",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###To read a CSV file using PySpark, you can use the read.csv() method:\n",
    "\n",
    "#infer schema\n",
    "csv_file = \"path/to/your/csv/file.csv\"\n",
    "df_csv = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "customschema = StructType([\n",
    "    StructField(\"A\", StringType(), True),\n",
    "    StructField(\"B\", DoubleType(), True),\n",
    "    StructField(\"C\", TimestampType(), True)\n",
    "])\n",
    "#header=\"true\": Indicates that the first row contains column names.\n",
    "#multiline=\"true\": Allows reading multiline records.\n",
    "#schema=customschema: Specifies the custom schema you defined.\n",
    "#load(destinationPath): Replace destinationPath with the path to your CSV file.\n",
    "\n",
    "df_1 = spark.read.format(\"csv\") \\\n",
    "    .options(header=\"true\", multiline=\"true\") \\\n",
    "    .schema(customschema) \\\n",
    "    .load(destinationPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc1bb0b-4c1b-4761-bf56-dae51010f942",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-1177762865691179&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">#Now that you have your data in a DataFrame, you can write it back to a CSV file using the write.csv() method:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> output_path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;path/to/output/csv/file.csv&#34;</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df_csv<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>output_path<span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;overwrite&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;header&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;false&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;sep&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;|&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/path/to/save/output.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;df_csv&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1177762865691179&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">#Now that you have your data in a DataFrame, you can write it back to a CSV file using the write.csv() method:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> output_path <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;path/to/output/csv/file.csv&#34;</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df_csv<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span>output_path<span class=\"ansi-blue-fg\">,</span> header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> mode<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;overwrite&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;header&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;false&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;sep&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;|&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/path/to/save/output.csv&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;df_csv&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;df_csv&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now that you have your data in a DataFrame, you can write it back to a CSV file using the write.csv() method:\n",
    "output_path = \"path/to/output/csv/file.csv\"\n",
    "df_csv.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "df.write.option(\"header\", \"false\").option(\"sep\", \"|\").csv(\"/path/to/save/output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc63cac3-f5b6-40c7-86a7-9d3c457c4c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##2. Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2470850f-0590-4bd4-8b0d-f47218a9da71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-1177762865691182&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">###To read a Parquet file using PySpark, you can use the read.parquet() method:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> parquet_file <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;path/to/your/parquet/file.parquet&#34;</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df_parquet <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>parquet_file<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\">#To write the data back to a Parquet file, use the write.parquet() method:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, *paths, **options)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    483</span>                        int96RebaseMode=int96RebaseMode)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    484</span> \n",
       "<span class=\"ansi-green-fg\">--&gt; 485</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    486</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    487</span>     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Path must be absolute: path/to/your/parquet/file.parquet</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1177762865691182&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">###To read a Parquet file using PySpark, you can use the read.parquet() method:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> parquet_file <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;path/to/your/parquet/file.parquet&#34;</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df_parquet <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>parquet_file<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\">#To write the data back to a Parquet file, use the write.parquet() method:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, *paths, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    483</span>                        int96RebaseMode=int96RebaseMode)\n<span class=\"ansi-green-intense-fg ansi-bold\">    484</span> \n<span class=\"ansi-green-fg\">--&gt; 485</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    487</span>     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Path must be absolute: path/to/your/parquet/file.parquet</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">IllegalArgumentException</span>: Path must be absolute: path/to/your/parquet/file.parquet",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###To read a Parquet file using PySpark, you can use the read.parquet() method:\n",
    "parquet_file = \"path/to/your/parquet/file.parquet\"\n",
    "df_parquet = spark.read.parquet(parquet_file)\n",
    "\n",
    "#To write the data back to a Parquet file, use the write.parquet() method:\n",
    "output_path = \"path/to/output/parquet/file.parquet\"\n",
    "df_parquet.write.parquet(output_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2b773b-28d3-48a5-900a-a5ef4431533c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#72. Temporary SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03331329-c63c-4285-a086-8b609aeb5d3b",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>city</th><th>name</th></tr></thead><tbody><tr><td>30</td><td>New York</td><td>Alice</td></tr><tr><td>35</td><td>Los Angeles</td><td>Charlie</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         30,
         "New York",
         "Alice"
        ],
        [
         35,
         "Los Angeles",
         "Charlie"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Temporary SQL table\n",
    "#We’ll create a sample DataFrame using a list of dictionaries and register the \n",
    "# DataFrame as a temporary SQL table to perform SQL operations\n",
    "\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Los Angeles\"}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "query = \"SELECT * FROM people WHERE age >= 30\"\n",
    "result_df = spark.sql(query)\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181b52bb-641c-411d-a87b-686f88c9aa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#73. Dropping Columns Using Regex Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791de89a-3a92-4411-9772-aa9f36d6d992",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+---+-------------+------+\n",
       " name|age|         city|gender|\n",
       "+-----+---+-------------+------+\n",
       "Alice| 30|     New York|     F|\n",
       "  Bob| 28|San Francisco|     M|\n",
       "Cathy| 29|  Los Angeles|     F|\n",
       "David| 32|      Chicago|     M|\n",
       "+-----+---+-------------+------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+---+-------------+------+\n| name|age|         city|gender|\n+-----+---+-------------+------+\n|Alice| 30|     New York|     F|\n|  Bob| 28|San Francisco|     M|\n|Cathy| 29|  Los Angeles|     F|\n|David| 32|      Chicago|     M|\n+-----+---+-------------+------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"Alice\", 30, \"New York\", \"F\"),\n",
    "        (\"Bob\", 28, \"San Francisco\", \"M\"),\n",
    "        (\"Cathy\", 29, \"Los Angeles\", \"F\"),\n",
    "        (\"David\", 32, \"Chicago\", \"M\")]\n",
    "\n",
    "columns = [\"name\", \"age\", \"city\", \"gender\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c5c557-3888-4f3d-b736-96e84ddae061",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+-------------+\n",
       " name|         city|\n",
       "+-----+-------------+\n",
       "Alice|     New York|\n",
       "  Bob|San Francisco|\n",
       "Cathy|  Los Angeles|\n",
       "David|      Chicago|\n",
       "+-----+-------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+-------------+\n| name|         city|\n+-----+-------------+\n|Alice|     New York|\n|  Bob|San Francisco|\n|Cathy|  Los Angeles|\n|David|      Chicago|\n+-----+-------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import re\n",
    "\n",
    "regex_pattern = \"gender|age\"\n",
    "df = df.select([col(c) for c in df.columns if not re.match(regex_pattern, c)])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcdfcec-8f51-4745-a072-bae8c8fef764",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#74. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f7a7d2-2f25-48ca-81de-69ba04a1e829",
     "showTitle": true,
     "title": "Inner Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103679&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103679&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d961cb6b-6550-4f5c-8545-c0536c8de3e8",
     "showTitle": true,
     "title": "Full Outer Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103680&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-red-fg\"># Includes all rows from both DataFrames.</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103680&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-red-fg\"># Includes all rows from both DataFrames.</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")\n",
    "### Full Outer Join:\n",
    "# Includes all rows from both DataFrames.\n",
    "# If there’s no match, the missing values are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c06fe8-7170-4a9f-a940-3b2c2d51a8b1",
     "showTitle": true,
     "title": "Left Outer Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103681&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-red-fg\"># Includes all rows from both DataFrames.</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103681&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-red-fg\"># Includes all rows from both DataFrames.</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")\n",
    "### Full Outer Join:\n",
    "# Includes all rows from both DataFrames.\n",
    "# If there’s no match, the missing values are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"outer\")\n",
    "### Left Outer Join:\n",
    "# Includes all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "# Non-matching rows from the right DataFrame are filled with null.\n",
    "\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ff2422-8202-4173-a33c-c46f10a439ed",
     "showTitle": true,
     "title": "Right Outer Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103682&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103682&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")\n",
    "\n",
    "### Full Outer Join:\n",
    "# Includes all rows from both DataFrames.\n",
    "# If there’s no match, the missing values are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"outer\")\n",
    "\n",
    "### Left Outer Join:\n",
    "# Includes all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "# Non-matching rows from the right DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left\")\n",
    "\n",
    "### Right Outer Join:\n",
    "# Similar to left outer join but includes all rows from the right DataFrame.\n",
    "# Non-matching rows from the left DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec06c261-63c8-4447-b5d1-fb821ff25b1a",
     "showTitle": true,
     "title": "Left Anti Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103683&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103683&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")\n",
    "\n",
    "### Full Outer Join:\n",
    "# Includes all rows from both DataFrames.\n",
    "# If there’s no match, the missing values are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"outer\")\n",
    "\n",
    "### Left Outer Join:\n",
    "# Includes all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "# Non-matching rows from the right DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left\")\n",
    "\n",
    "### Right Outer Join:\n",
    "# Similar to left outer join but includes all rows from the right DataFrame.\n",
    "# Non-matching rows from the left DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"right\")\n",
    "\n",
    "### Left Anti Join:\n",
    "# Returns rows from the left DataFrame that do not have a match in the right DataFrame.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff97b0af-bc14-4324-b46c-5af92a71848e",
     "showTitle": true,
     "title": "Left Semi Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103684&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103684&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")\n",
    "\n",
    "### Full Outer Join:\n",
    "# Includes all rows from both DataFrames.\n",
    "# If there’s no match, the missing values are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"outer\")\n",
    "\n",
    "### Left Outer Join:\n",
    "# Includes all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "# Non-matching rows from the right DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left\")\n",
    "\n",
    "### Right Outer Join:\n",
    "# Similar to left outer join but includes all rows from the right DataFrame.\n",
    "# Non-matching rows from the left DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"right\")\n",
    "\n",
    "### Left Anti Join:\n",
    "# Returns rows from the left DataFrame that do not have a match in the right DataFrame.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left_anti\")\n",
    "\n",
    "### Left Semi Join:\n",
    "# Returns rows from the left DataFrame that have at least one match in the right DataFrame.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left_semi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6f1bc8-86f5-4240-b156-b41891d05de9",
     "showTitle": true,
     "title": "Cross Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-4490529496103685&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4490529496103685&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Combines rows from both DataFrames where the join condition is met.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Only the matching rows are included in the result.</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>joined_df <span class=\"ansi-blue-fg\">=</span> empDF<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>deptDF<span class=\"ansi-blue-fg\">,</span> empDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;emp_dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> deptDF<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;dept_id&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\">### Full Outer Join:</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">NameError</span>: name &#39;empDF&#39; is not defined",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Inner Join:\n",
    "# Combines rows from both DataFrames where the join condition is met.\n",
    "# Only the matching rows are included in the result.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"inner\")\n",
    "\n",
    "### Full Outer Join:\n",
    "# Includes all rows from both DataFrames.\n",
    "# If there’s no match, the missing values are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"outer\")\n",
    "\n",
    "### Left Outer Join:\n",
    "# Includes all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "# Non-matching rows from the right DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left\")\n",
    "\n",
    "### Right Outer Join:\n",
    "# Similar to left outer join but includes all rows from the right DataFrame.\n",
    "# Non-matching rows from the left DataFrame are filled with null.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"right\")\n",
    "\n",
    "### Left Anti Join:\n",
    "# Returns rows from the left DataFrame that do not have a match in the right DataFrame.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left_anti\")\n",
    "\n",
    "### Left Semi Join:\n",
    "# Returns rows from the left DataFrame that have at least one match in the right DataFrame.\n",
    "joined_df = empDF.join(deptDF, empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"], \"left_semi\")\n",
    "\n",
    "### Cross Join:\n",
    "# Generates all possible combinations of rows from both DataFrames.\n",
    "# Use with caution as it can result in a large output.\n",
    "joined_df = empDF.crossJoin(deptDF.select(\"dept_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5635c16-68af-487f-9440-2b2b1a0a88a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#75. Remove extra spaces from the specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7f9764-545d-44d5-a7d1-b608d493780e",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+-------------+\n",
       " name|         city|\n",
       "+-----+-------------+\n",
       "Alice|     New York|\n",
       "  Bob|San Francisco|\n",
       "Cathy|  Los Angeles|\n",
       "David|      Chicago|\n",
       "+-----+-------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----+-------------+\n| name|         city|\n+-----+-------------+\n|Alice|     New York|\n|  Bob|San Francisco|\n|Cathy|  Los Angeles|\n|David|      Chicago|\n+-----+-------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_extra_spaces(df, column_name):\n",
    "    # Remove extra spaces from the specified column\n",
    "    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
    "    return df_transformed\n",
    "\n",
    "transformed_df = remove_extra_spaces(df, \"name\")\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97246ee2-a29e-4555-adb9-c520e38dfda9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#76. StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fc98c1-76cc-45bd-a89d-227e216e2e8a",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+\n",
       "Categories|Value|\n",
       "+----------+-----+\n",
       "         A|   10|\n",
       "         A|   20|\n",
       "         B|   30|\n",
       "         B|   20|\n",
       "         B|   30|\n",
       "         C|   40|\n",
       "         C|   10|\n",
       "         D|   10|\n",
       "+----------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+\n|Categories|Value|\n+----------+-----+\n|         A|   10|\n|         A|   20|\n|         B|   30|\n|         B|   20|\n|         B|   30|\n|         C|   40|\n|         C|   10|\n|         D|   10|\n+----------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The StringIndexer is a vital PySpark feature that helps convert categorical string columns in a DataFrame into numerical indices.\n",
    "\n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5a6452-d35a-4d21-804b-5a031c20ae90",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+------------------+\n",
       "Categories|Value|Categories_Indexed|\n",
       "+----------+-----+------------------+\n",
       "         A|   10|               1.0|\n",
       "         A|   20|               1.0|\n",
       "         B|   30|               0.0|\n",
       "         B|   20|               0.0|\n",
       "         B|   30|               0.0|\n",
       "         C|   40|               2.0|\n",
       "         C|   10|               2.0|\n",
       "         D|   10|               3.0|\n",
       "+----------+-----+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+------------------+\n|Categories|Value|Categories_Indexed|\n+----------+-----+------------------+\n|         A|   10|               1.0|\n|         A|   20|               1.0|\n|         B|   30|               0.0|\n|         B|   20|               0.0|\n|         B|   30|               0.0|\n|         C|   40|               2.0|\n|         C|   10|               2.0|\n|         D|   10|               3.0|\n+----------+-----+------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### StringIndexer\n",
    "# The StringIndexer is a vital PySpark feature that helps convert \n",
    "# categorical string columns in a DataFrame into numerical indices.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# StringIndexer Initialization\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Transform the DataFrame using the fitted StringIndexer model\n",
    "indexed_df = indexerModel.transform(df)\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbd0862-bfee-4562-9680-45b015dec413",
     "showTitle": true,
     "title": "Input 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+\n",
       "Categories|Value|\n",
       "+----------+-----+\n",
       "         A|   10|\n",
       "         A|   20|\n",
       "         B|   30|\n",
       "         B|   20|\n",
       "         B|   30|\n",
       "         C|   40|\n",
       "         C|   10|\n",
       "         D|   10|\n",
       "+----------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+\n|Categories|Value|\n+----------+-----+\n|         A|   10|\n|         A|   20|\n|         B|   30|\n|         B|   20|\n|         B|   30|\n|         C|   40|\n|         C|   10|\n|         D|   10|\n+----------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Handling unseen labels in test data\n",
    "# In real-world scenarios, your model may encounter unseen labels in the test data. By default, StringIndexer throws an\n",
    "# error when it comes across an unseen label. To handle such cases, you can set the handleInvalid1 parameter to 'skip',\n",
    "# 'keep', or 'error', depending on your requirements.\n",
    " \n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "train_df = spark.createDataFrame(data, columns)\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b37d1d5-b7cc-4d21-a24f-1672bca9c3db",
     "showTitle": true,
     "title": "Solution 2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+\n",
       "Categories|Value|\n",
       "+----------+-----+\n",
       "         A|   15|\n",
       "         A|   22|\n",
       "         B|   38|\n",
       "         B|   20|\n",
       "         C|   18|\n",
       "         E|   19|\n",
       "         F|   17|\n",
       "+----------+-----+\n",
       "\n",
       "+----------+-----+------------------+\n",
       "Categories|Value|Categories_Indexed|\n",
       "+----------+-----+------------------+\n",
       "         A|   15|               1.0|\n",
       "         A|   22|               1.0|\n",
       "         B|   38|               0.0|\n",
       "         B|   20|               0.0|\n",
       "         C|   18|               2.0|\n",
       "         E|   19|               4.0|\n",
       "         F|   17|               4.0|\n",
       "+----------+-----+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+\n|Categories|Value|\n+----------+-----+\n|         A|   15|\n|         A|   22|\n|         B|   38|\n|         B|   20|\n|         C|   18|\n|         E|   19|\n|         F|   17|\n+----------+-----+\n\n+----------+-----+------------------+\n|Categories|Value|Categories_Indexed|\n+----------+-----+------------------+\n|         A|   15|               1.0|\n|         A|   22|               1.0|\n|         B|   38|               0.0|\n|         B|   20|               0.0|\n|         C|   18|               2.0|\n|         E|   19|               4.0|\n|         F|   17|               4.0|\n+----------+-----+------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize the StringIndexer uaing handleInvalid=\"keep\" and fit on train_df where dataframe train_df is having four categories A, B, C, D\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\", handleInvalid=\"keep\")\n",
    "train_indexerModel = indexer.fit(train_df)\n",
    "\n",
    "#Create Test DataFrame\n",
    "data = [(\"A\", 15),(\"A\", 22),(\"B\", 38),(\"B\", 20),(\"C\", 18),(\"E\", 19),(\"F\", 17)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "test_df = spark.createDataFrame(data, columns)\n",
    "test_df.show()\n",
    "\n",
    "#ransform DataFrame test_df where in dataframe test_df is having tow new categories E, F and category D is missing\n",
    "test_indexed_df = train_indexerModel.transform(test_df)\n",
    "test_indexed_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8282341-2039-4ed3-be73-5f64536d4c23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#77. Reversing StringIndexer transformation with IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88707321-760c-4796-b1ec-a4d5863c62eb",
     "showTitle": true,
     "title": "Input"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+\n",
       "Categories|Value|\n",
       "+----------+-----+\n",
       "         A|   10|\n",
       "         A|   20|\n",
       "         B|   30|\n",
       "         B|   20|\n",
       "         B|   30|\n",
       "         C|   40|\n",
       "         C|   10|\n",
       "         D|   10|\n",
       "+----------+-----+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+\n|Categories|Value|\n+----------+-----+\n|         A|   10|\n|         A|   20|\n|         B|   30|\n|         B|   20|\n|         B|   30|\n|         C|   40|\n|         C|   10|\n|         D|   10|\n+----------+-----+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In some cases, you may need to reverse the transformation applied by StringIndexer to interpret your model’s predictions.\n",
    "\n",
    "# Example Data\n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41292f2-a6ca-463c-b361-73d6ff29eadd",
     "showTitle": true,
     "title": "Solution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+------------------+\n",
       "Categories|Value|Categories_Indexed|\n",
       "+----------+-----+------------------+\n",
       "         A|   10|               1.0|\n",
       "         A|   20|               1.0|\n",
       "         B|   30|               0.0|\n",
       "         B|   20|               0.0|\n",
       "         B|   30|               0.0|\n",
       "         C|   40|               2.0|\n",
       "         C|   10|               2.0|\n",
       "         D|   10|               3.0|\n",
       "+----------+-----+------------------+\n",
       "\n",
       "+----------+-----+------------------+-------------+\n",
       "Categories|Value|Categories_Indexed|Pred_Category|\n",
       "+----------+-----+------------------+-------------+\n",
       "         A|   10|               1.0|            A|\n",
       "         A|   20|               1.0|            A|\n",
       "         B|   30|               0.0|            B|\n",
       "         B|   20|               0.0|            B|\n",
       "         B|   30|               0.0|            B|\n",
       "         C|   40|               2.0|            C|\n",
       "         C|   10|               2.0|            C|\n",
       "         D|   10|               3.0|            D|\n",
       "+----------+-----+------------------+-------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+-----+------------------+\n|Categories|Value|Categories_Indexed|\n+----------+-----+------------------+\n|         A|   10|               1.0|\n|         A|   20|               1.0|\n|         B|   30|               0.0|\n|         B|   20|               0.0|\n|         B|   30|               0.0|\n|         C|   40|               2.0|\n|         C|   10|               2.0|\n|         D|   10|               3.0|\n+----------+-----+------------------+\n\n+----------+-----+------------------+-------------+\n|Categories|Value|Categories_Indexed|Pred_Category|\n+----------+-----+------------------+-------------+\n|         A|   10|               1.0|            A|\n|         A|   20|               1.0|            A|\n|         B|   30|               0.0|            B|\n|         B|   20|               0.0|            B|\n|         B|   30|               0.0|            B|\n|         C|   40|               2.0|            C|\n|         C|   10|               2.0|            C|\n|         D|   10|               3.0|            D|\n+----------+-----+------------------+-------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Reverse StringIndexer\n",
    "# Initialize the StringIndexer and Transform the \n",
    "# DataFrame using the fitted StringIndexer model\n",
    "# StringIndexer Initialization\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
    "indexerModel = indexer.fit(df)\n",
    "\n",
    "# Transform the DataFrame using the fitted StringIndexer model\n",
    "indexed_df = indexerModel.transform(df)\n",
    "indexed_df.show()\n",
    "\n",
    "#Import the IndexToString transformer\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "#Initialize the IndexToString\n",
    "index_to_string = IndexToString(inputCol=\"Categories_Indexed\", outputCol=\"Pred_Category\",\n",
    "                                labels=indexerModel.labels)\n",
    "\n",
    "# Transform the DataFrame\n",
    "result_df = index_to_string.transform(indexed_df)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6400bbeb-6a8e-4451-8be8-946c889e8a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read static DataFrame\n",
    "staticDf = spark.read.format(\"csv\").load(\"static_data.csv\")\n",
    "\n",
    "# Read streaming DataFrame\n",
    "streamingDf = spark.readStream.format(\"csv\").load(\"streaming_data.csv\")\n",
    "\n",
    "# Perform an inner equi-join with a static DataFrame\n",
    "joinedDf = streamingDf.join(staticDf, \"type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685f4605-878c-4adf-a9ed-c0e31058ad9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Streaming Data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVAppendExample\").getOrCreate()\n",
    "\n",
    "# Define your schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "newCsvPath = \"path/to/new_data.csv\"\n",
    "newCsvDf = spark.read.csv(newCsvPath, header=True, schema=schema)\n",
    "\n",
    "appendedDf = existingDf.union(newCsvDf)\n",
    "\n",
    "appendedDf.write.csv(\"path/to/output.csv\", mode=\"append\", header=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Examples",
   "widgets": {
    "database": {
     "currentValue": "customers_dev",
     "nuid": "00edf2cc-f473-4602-81b2-dc05b9970e95",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "customers_dev",
      "label": null,
      "name": "database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "state": {
     "currentValue": "IL",
     "nuid": "f3768d2e-345a-4fac-a838-c9c8c962e18f",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "CA",
      "label": null,
      "name": "state",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "CA",
        "IL",
        "MI",
        "NY",
        "OR",
        "VA"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

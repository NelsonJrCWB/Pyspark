{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e43af875-a56a-42b0-80f7-8d887a58970d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#customized dbutils\n",
    "\n",
    "- Provide common resources for standardizing codes and functions;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6cea496-eb3e-4510-99b2-ce54b9c2ef68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##standard_debug()\n",
    "\n",
    "- Function created for printing log information;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "572be219-0c32-45fd-851b-19ad3c610f20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Created: 2023.11.13 - Nelson Jr. ngoliveirajr@gmail.com\n",
    "\n",
    "#_dbg_option: if Y the log will be printed with the request\n",
    "#_dbg_validation: object to be printed in the log (string, list or dataframe)\n",
    "#_dbg_message: Message header to be displayed in the log\n",
    "def standard_debug(_dbg_option: bool,_dbg_validation,_dbg_message: str):\n",
    "    if _dbg_option==True:\n",
    "        print(_dbg_message)\n",
    "        if type(_dbg_validation)==str or type(_dbg_validation)==list:\n",
    "            print(_dbg_validation)\n",
    "        elif isinstance(_dbg_validation,DataFrame):\n",
    "                _dbg_validation.display()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "284ac513-970b-4f80-be0f-69cf1404d0e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##standard_dim_fact_table()\n",
    "\n",
    "- Customized Function for creating DIMENSION and FACT tables based on a transactional database;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3541afc6-904b-4c9e-9501-fb87423951ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Created: 2023.11.13 - Nelson Oliveira nelson.oliveira@volvo.com\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_src_df: the main dataframe to be used in the transformation\n",
    "#_col_keys: list of columns to be considered as key for the selection. Ex. [\"name\",\"id\",\"brand\"]\n",
    "#_TK_name: name for the technical key column\n",
    "#_incremental: if 'N' the dimension and target tables will be created every time. \n",
    "#             if 'Y' the dimension and target tables will be retained and just \n",
    "#                    the new combination will be added with the new TK\n",
    "#             @Any case if tables do not exist, it will be created for the first time\n",
    "#_TK_consistence: Replace the database with the latest information for the technical key.\n",
    "#_dim_database: the database name for the dimension table\n",
    "# _dim_tablename: name for the dimension table\n",
    "#_target_database: the database name for the target table\n",
    "#_target_tablename: name for the target table\n",
    "#_debug: if True the function will print and display in all steps\n",
    "#\n",
    "\n",
    "\n",
    "def standard_dim_fact_table(_src_df,_col_keys: list,_TK_name: str,_incremental: bool,_TK_consistence: bool,_dim_database: str,_dim_tablename: str,_fact_database: str, _fact_tablename: str,_debug: bool):\n",
    "    \n",
    "    \n",
    "    #Creates the variable appointed to the dimension table\n",
    "    _dim_name=set_database_name(_dim_database)+\".\"+_dim_tablename\n",
    "   \n",
    "\n",
    "    #debug**********************\n",
    "    standard_debug(_debug,_dim_name,\"1.Creates the variable appointed to the DIMENSION table:\")\n",
    "        \n",
    "    #Creates the variable appointed to the fact table\n",
    "    _fact_table=set_database_name(_fact_database)+\".\"+_fact_tablename\n",
    "\n",
    "    #debug**********************\n",
    "    standard_debug(_debug,_fact_table,\"2.Creates the variable appointed to the FACT table:\")\n",
    "\n",
    "    \n",
    "    if spark.catalog._jcatalog.tableExists(f\"{_dim_name}\"):\n",
    "        #Added this just for good understand about the logic.\n",
    "        _incremental=_incremental\n",
    "    else:\n",
    "        _incremental=False\n",
    "    \n",
    "    #debug**********************\n",
    "    standard_debug(_debug,_incremental,\"3.Check if the dimension table is present in the catalog. If table is not present then the append will not be considered:\")\n",
    "\n",
    "    #Creates the DF based on the src table\n",
    "    df=_src_df\n",
    "\n",
    "    #Creates the distinct combination for considering as the technical key (TK)\n",
    "    df_distinct_combination = df.select(*_col_keys).distinct()\n",
    "    print(type(df_distinct_combination))\n",
    "\n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_distinct_combination,\"4.Creates the distinct combination for considering as the technical key (TK):\")\n",
    "\n",
    "    #Creates the array column for the technical key calculation\n",
    "    df_with_array = df_distinct_combination.select(df_distinct_combination['*'],struct([f.col(c).alias(c) for c in df_distinct_combination.columns]).alias(\"array_col\"))\n",
    "    \n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_with_array,\"5.Creates the array column for the technical key calculation:\")\n",
    "\n",
    "    last_key=0\n",
    "    if(_incremental==True):\n",
    "\n",
    "        #debug**********************\n",
    "        standard_debug(_debug,\"\",\"5.0.>>Incremental selected.<<<\")\n",
    "\n",
    "        df_dimension_key=spark.sql(f\"select * from {_dim_name}\")\n",
    "\n",
    "        #Collect the column names to filter\n",
    "        df_key_cols=df_dimension_key.select(*_col_keys).limit(1)\n",
    "\n",
    "        #debug**********************\n",
    "        standard_debug(_debug,df_key_cols,\"5.1.Collect the column names to filter:\")\n",
    "\n",
    "        #Collect the last key in the dim table\n",
    "        last_key = df_dimension_key.agg(max(_TK_name)).collect()[0][0]\n",
    "\n",
    "        #debug**********************\n",
    "        standard_debug(_debug,last_key,\"5.2.Collect the last key in the dim table:\")\n",
    "\n",
    "        #Recreates the dimension table with the columns in array\n",
    "        df_dimension_array = df_dimension_key.select(f.col(_TK_name),*_col_keys,struct([f.col(c).alias(c) for c in df_key_cols.columns]).alias(\"array_col\"))\n",
    "\n",
    "        #debug**********************\n",
    "        standard_debug(_debug,df_dimension_array,\"5.3.Recreates the dimension table with the columns in array:\")\n",
    "\n",
    "        #Create a left anti join to check the new incoming figures to be added\n",
    "        #Keep the same df name to be considered in case of N for appending\n",
    "        df_with_array=df_with_array.join(df_dimension_array,on='array_col',how='leftanti')\n",
    "        \n",
    "        #debug**********************\n",
    "        standard_debug(_debug,df_with_array,\"5.4.Create a left anti join to check the new incoming figures to be added:\")\n",
    "\n",
    "    #Creates the technical key column considering the last key as start point\n",
    "    window = Window.orderBy(df_with_array['array_col'])\n",
    "    df_with_array_key = df_with_array.withColumn(_TK_name,row_number().over(window)+last_key)\n",
    "    \n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_with_array_key,\"6.Creates the technical key column considering the last key as start point:\")\n",
    "\n",
    "    #If append is Y the new data will be added in the dimension\n",
    "    if(_incremental==True):\n",
    "        df_dimension_array = df_dimension_array.unionByName(df_with_array_key, allowMissingColumns=True)\n",
    "    else:\n",
    "        df_dimension_array=df_with_array_key\n",
    "    \n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_dimension_array,\"7.If append is Y the new data will be added in the dimension:\")\n",
    "\n",
    "    #Creates the array column with the columns in scope for the source base\n",
    "    df_base_with_array = df.select(*[f.col(column) for column in df.columns if column not in _col_keys],struct([f.col(c).alias(c) for c in df_distinct_combination.columns]).alias(\"array_col\"))\n",
    "    \n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_base_with_array,\"8.Creates the array column with the columns in scope for the source base:\")\n",
    "\n",
    "    #Add the technical key by joining the array column as the key\n",
    "    df_join_TK= df_base_with_array.join(df_dimension_array,on='array_col',how='left')\n",
    "    df_join_TK=df_join_TK.drop(\"array_col\")\n",
    "    \n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_join_TK,\"9.Add the technical key by joining the array column as the key:\")\n",
    "\n",
    "    #***Needs to repair the logic for selecting the right columns\n",
    "    df_TK= df_join_TK.select(*[f.col(column) for column in df_join_TK.columns if column not in _col_keys])\n",
    "\n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_join_TK,\"10.***Needs to repair the logic for selecting the right columns:\")\n",
    "\n",
    "    if(_incremental==True):    \n",
    "        #If the table exists it will append the new incoming data  \n",
    "        if spark.catalog._jcatalog.tableExists(f\"{_fact_table}\"):\n",
    "            df_base=spark.sql(f\"select * from {_fact_table}\")\n",
    "\n",
    "            if _TK_consistence==True:\n",
    "                #In order to keep the consistancy, the data must to be checked\n",
    "                #Otherwise the data will be appended anyway.\n",
    "                df = [column for column in df_base.columns if column not in _TK_name]\n",
    "                \n",
    "                df_base=df_base.join(df_TK,on=df,how='leftanti')\n",
    "                #debug**********************\n",
    "                standard_debug(_debug,df_base,\"Check new database:\")\n",
    "\n",
    "            df_base_new = df_base.unionByName(df_TK, allowMissingColumns=True)\n",
    "\n",
    "            #Check if the distinct is needed as part of that (it can be too slow)\n",
    "            df_base_new = df_base_new.distinct()\n",
    "\n",
    "        #if not the table will be created\n",
    "        else:\n",
    "            df_base_new=df_TK\n",
    "    #if the append is not required it will create a new table\n",
    "    else:\n",
    "        df_base_new=df_TK\n",
    "\n",
    "    #debug**********************\n",
    "    standard_debug(_debug,\"\",\"11.If the table exists it will append the new data. Otherwise the table will be recreated:\")\n",
    "    \n",
    "    standard_debug(_debug,df_base_new,\"12.Final Fact table:\")\n",
    "    store_and_mount_table(df_base_new,_fact_tablename,_fact_database,file_location, True)\n",
    "\n",
    "    #Save the new dimension table\n",
    "    #Store database with the function\n",
    "    df_dimension_array=df_dimension_array.drop('array_col')\n",
    "\n",
    "    #debug**********************\n",
    "    standard_debug(_debug,df_dimension_array,\"13.Final Dimension table:\")\n",
    "\n",
    "    store_and_mount_table(df_dimension_array,_dim_tablename,_dim_database,file_location, True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "customized_dbutils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
